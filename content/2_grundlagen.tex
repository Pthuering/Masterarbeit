\section{Theoretische Grundlagen und Stand der Forschung} \label{chap:2}

% Ziel: 16-18 Seiten

% ============================================================================
% AGENT-TODO: FORMULATE TEXTS FROM BULLET POINTS
% ============================================================================
% Instructions for agent task:
% 1. Go through ALL subsections with bullet points (marked with %)
% 2. Formulate complete, scientific paragraphs from these bullet points
% 3. Use ONLY sources already mentioned in bullet points (marked with [SOURCE-MISSING])
% 4. DO NOT add new sources or citations on your own
% 5. Keep all [SOURCE-MISSING] tags in the formulated text where citations belong
% 6. Maintain cross-references (\ref{}) to other chapters
% 7. Keep scientific style
%
% Sections to formulate:
% - Kapitel-Einleitung (lines ~5-33)
% - 2.1 Abschnitt-Einleitung (lines ~36-65)
% - 2.1.2 NLP-Anwendungen im Verkehrssektor (lines ~115-203)
% - 2.1.3 Domänenspezifische Sprachmodelle (lines ~205-366)
% - 2.2.1 Fine-Tuning-Methoden (partial - LoRA, other PEFT methods)
% - 2.2.2 Prompt Engineering (complete)
% - 2.2.3 Datensätze (partial - needs completion)
% - 2.2.4 Overfitting & Catastrophic Forgetting (complete)
% - 2.3.1 Quantisierung (complete)
% - 2.3.2 Halluzination Prevention (complete)
% - 2.3.3 RAG & Knowledge Integration (complete)
%
% Do NOT formulate:
% - Already written paragraphs (e.g., 2.1.1 Transformer-Architektur)
% - Sections with only structural comments
% ============================================================================

Die in Kapitel~\ref{chap:1} vorgestellte Problemstellung der Leipziger Verkehrsbetriebe erfordert fundierte Natural Language Processing-Expertise, da die Transformation von fachsprachlichen Verkehrsanweisungen in allgemeinverständliche Fahrgastinformationen keine triviale Übersetzungsaufgabe darstellt. Vielmehr müssen Kontextabhängigkeit, semantische Präzision und kommunikative Verständlichkeit simultan gewährleistet werden, was moderne Sprachverarbeitungsmethoden unerlässlich macht.

Das vorliegende Kapitel legt die theoretischen Grundlagen für die automatisierte Texttransformation und gliedert sich in drei aufeinander aufbauende Themenkomplexe. Abschnitt~\ref{sec:2.1} behandelt zunächst die NLP-Fundamente, wobei die Transformer-Architektur als paradigmatischer Durchbruch im Bereich der Sprachverarbeitung eingeführt wird. Das Pretraining-Finetuning-Paradigma wird erörtert, bevor deutschsprachige Modelle von Mistral bis LeoLM betrachtet werden, welche die sprachliche Basis für die vorliegende Arbeit bilden.

Abschnitt~\ref{sec:2.2} widmet sich der Anpassungsmethodik vortrainierter Sprachmodelle an domänenspezifische Aufgaben. Parameter-effiziente Fine-Tuning-Verfahren wie LoRA werden vorgestellt, ergänzt durch das Unsloth-Optimierungsframework, das Training unter Ressourcenbeschränkungen ermöglicht. Datenstrategien bei limitierter Datenverfügbarkeit sowie Prompt Engineering als komplementärer Ansatz zur Verhaltenssteuerung von Sprachmodellen runden diesen Abschnitt ab.

Der dritte Themenkomplex in Abschnitt~\ref{sec:2.3} fokussiert auf Produktionsreife und praktische Einsatzfähigkeit. Quantisierung als Technik für das Deployment unter Hardware-Constraints wird erörtert, bevor Mechanismen zur Halluzination Prevention und zur Sicherstellung faktentreuer Ausgaben behandelt werden. Strategien zur Knowledge Integration ohne den Overhead vollständiger Retrieval-Augmented Generation-Systeme schließen diesen Abschnitt.

Der rote Faden dieses Kapitels führt von theoretischen Konzepten zu deren praktischer Anwendbarkeit unter realen Constraints. Während Kapitel~\ref{chap:3} den Anwendungskontext beschreibt, der technologische Entscheidungen determiniert, und Kapitel~\ref{chap:6} die konkrete Implementierung darstellt, fundiert das vorliegende Kapitel die entwickelte Lösung in der etablierten wissenschaftlichen Forschung und schafft das konzeptionelle Fundament für die nachfolgenden praktischen Ausführungen.

    \subsection{Natural Language Processing für Verkehrsinformationen}
    \label{sec:2.1}
    
    % Ziel: 5-6 Seiten
    
Die Auswahl geeigneter Sprachmodelle für die Verarbeitung von Verkehrsinformationen bewegt sich im Spannungsfeld zwischen universellen und domänenspezifischen Ansätzen. Während proprietäre Systeme wie GPT-4 oder Claude beeindruckende Generalfähigkeiten demonstrieren, erfordern Verkehrsdomänen spezifische Terminologie, rechtliche Präzision und lokale Kontextkenntnisse. Zudem machen Anforderungen der Datenschutz-Grundverordnung, Datensouveränität und Kosteneffizienz den Einsatz von Open-Source-Modellen für öffentliche Verkehrsbetriebe unerlässlich.

Die Komplexität von Natural Language Processing im Verkehrssektor wird häufig unterschätzt. Es genügt nicht, Sentiment-Analysen oder FAQ-Bots bereitzustellen; die Transformation fachsprachlicher Anweisungen erfordert gleichzeitig tiefes Kontextverständnis, präzise Stiladaption und absolute Faktentreue. Die Umformulierung einer verkürzten Mitteilung wie „Linie 10 fährt nicht" in eine vollständige Fahrgastinformation „Die Straßenbahn der Linie 10 verkehrt aufgrund technischer Störungen gegenwärtig nicht" erfordert implizites Wissen über Liniennummern, Fahrzeugtypen und gegebenenfalls Alternativrouten, das nicht unmittelbar in der Eingabe kodiert ist.

Die Evolution natürlichsprachlicher Generierungsansätze im Verkehrssektor spiegelt den allgemeinen Fortschritt im Natural Language Processing wider. Template-basierte Systeme waren starr, in ihrem Ausdrucksvermögen limitiert und wartungsintensiv. Regelbasierte Natural Language Generation bot zwar bessere Kontrolle über die Ausgabequalität, stieß jedoch bei der Skalierung auf erhebliche Probleme. Moderne Large Language Models vereinen Flexibilität mit Generalisierungsfähigkeit, stellen jedoch neue Herausforderungen hinsichtlich der Kontrollierbarkeit und Faktentreue dar.

Transformer-basierte Sprachmodelle unterscheiden sich fundamental von früheren Ansätzen und ermöglichen erst die geforderte Qualität automatisierter Texttransformation. Der Attention-Mechanismus erfasst semantische Abhängigkeiten über Satzgrenzen hinweg, während das Pretraining auf Milliarden von Tokens ein robustes allgemeines Sprachverständnis etabliert. Fine-Tuning erlaubt anschließend die Domänenanpassung, ohne dass ein vollständiges Training from-scratch erforderlich wäre, was bei begrenzten Ressourcen und Datenmengen entscheidend ist.

Die folgenden Unterabschnitte strukturieren die theoretischen Grundlagen systematisch: Abschnitt~\ref{sec:2.1.1} behandelt die technischen Fundamente der Transformer-Architektur, das Pretraining-Paradigma und Transfer Learning. Abschnitt~\ref{sec:2.1.2} gibt einen Überblick über verkehrsspezifische NLP-Anwendungen und den aktuellen Stand der Technik. Abschnitt~\ref{sec:2.1.3} fokussiert auf deutschsprachige und domänenspezifische Modelle, insbesondere die LeoLM-Familie, die als Basis für die vorliegende Arbeit dient.
    
        \subsubsection{Grundlagen der Sprachverarbeitung}
        \label{sec:2.1.1}
        
        % Bereits ausgearbeitet - siehe unten
        
        Die automatisierte Verarbeitung und Transformation von Verkehrsinformationen stellt hohe Anforderungen an Natural Language Processing-Systeme. Präzision, Kontextverständnis und die Fähigkeit zur semantischen Umformulierung sind dabei zentrale Anforderungen. Moderne Ansätze der Sprachverarbeitung basieren auf der Transformer-Architektur, die seit ihrer Einführung im Jahr 2017 die Entwicklung von Sprachmodellen maßgeblich geprägt hat. Das in dieser Arbeit verwendete Modell LeoLM-7B baut auf dieser Architektur auf und nutzt deren Vorteile für die Verarbeitung deutschsprachiger Texte.
        
        \paragraph{Transformer-Architektur}
        
        Die Transformer-Architektur wurde 2017 von Vaswani et al. mit dem wegweisenden Paper „Attention is All You Need" eingeführt \cite{vaswani2017attention}. Im Gegensatz zu vorherigen Ansätzen wie Recurrent Neural Networks (RNNs) oder Long Short-Term Memory (LSTM) verzichtet die Transformer-Architektur vollständig auf rekurrente Strukturen und basiert stattdessen auf dem Attention-Mechanismus. Dieser fundamentale Paradigmenwechsel ermöglicht die parallele Verarbeitung von Sequenzen und führt zu deutlich schnelleren Trainingszeiten sowie besserer Skalierbarkeit.
        
        Das Kernprinzip der Transformer-Architektur ist der Self-Attention-Mechanismus \cite{vaswani2017attention}. Dieser ermöglicht es jedem Token in einer Sequenz, auf alle anderen Tokens im Kontext zuzugreifen und deren Relevanz für die eigene Repräsentation zu bewerten. Durch den Einsatz von Multi-Head Attention werden mehrere parallele Attention-Mechanismen verwendet, die unterschiedliche Aspekte der Kontextbeziehungen erfassen können \cite{vaswani2017attention}. Diese Architektur ermöglicht es dem Modell, komplexe syntaktische und semantische Abhängigkeiten auch über große Distanzen im Text hinweg zu modellieren, ohne unter dem Vanishing-Gradient-Problem zu leiden, das RNNs bei langen Sequenzen beeinträchtigt.
        
        Transformer-basierte Modelle lassen sich in drei Hauptkategorien einteilen: Encoder-Only-Modelle wie BERT \cite{devlin2018bert}, die primär für bidirektionale Textklassifikation und Embeddings konzipiert sind, Decoder-Only-Modelle wie die GPT-Serie \cite{radford2018improving, radford2019language}, die für autoregressive Textgenerierung optimiert sind, sowie Encoder-Decoder-Architekturen wie der ursprüngliche Transformer \cite{vaswani2017attention} und T5 \cite{raffel2020exploring}, die vor allem für Übersetzungsaufgaben entwickelt wurden. Für die in dieser Arbeit behandelte Aufgabe der Textgenerierung und -transformation ist die Decoder-Only-Architektur besonders geeignet, da sie speziell für die sequenzielle Erzeugung von Text konzipiert wurde.
        
        \paragraph{Vortrainierte Sprachmodelle}
        
        Ein zentrales Konzept moderner NLP-Systeme ist das Pretraining von Sprachmodellen auf großen, unlabeled Textkorpora. Durch Self-Supervised Learning, bei dem das Modell darauf trainiert wird, das jeweils nächste Token in einer Sequenz vorherzusagen \cite{radford2018improving}, entwickeln diese Modelle ein umfassendes Sprachverständnis inklusive Syntax, Semantik und implizitem Weltwissen. Zu den einflussreichsten vortrainierten Modellen gehören BERT \cite{devlin2018bert} mit seinem bidirektionalen Masked Language Modeling-Ansatz, die GPT-Serie \cite{radford2018improving, radford2019language} für autoregressive Textgenerierung sowie T5 \cite{raffel2020exploring} mit seinem universellen Text-to-Text-Framework.
        
        Für die vorliegende Arbeit ist insbesondere die Mistral-7B-Architektur von Bedeutung, da sie die Grundlage für das verwendete LeoLM-Modell bildet. Mistral 7B \cite{jiang2023mistral} ist ein hocheffizienter Decoder-Only-Transformer mit 7 Milliarden Parametern, der mehrere innovative Architekturmerkmale aufweist. Grouped-Query Attention (GQA) reduziert die Größe des Key-Value-Cache und ermöglicht schnellere Inferenz \cite{jiang2023mistral}. Sliding Window Attention erlaubt die effiziente Verarbeitung langer Kontexte, während der Rolling Buffer Cache die Speichernutzung optimiert \cite{jiang2023mistral}. Mit 7 Milliarden Parametern stellt Mistral einen Sweet Spot zwischen Modellleistung und Ressourceneffizienz dar und übertrifft in Benchmarks viele deutlich größere Modelle \cite{jiang2023mistral}. Als Open-Source-Modell unter Apache 2.0 Lizenz ist es besonders für lokale Ausführung und Fine-Tuning geeignet.
        
        Ein wichtiger Unterschied besteht zwischen Base Models und Instruct Models. Base Models wie Mistral-7B \cite{jiang2023mistral} sind auf reines Language Modeling trainiert \cite{radford2019language} und setzen primär Texte fort, ohne notwendigerweise expliziten Anweisungen zu folgen. Sie sind als Ausgangspunkt für aufgabenspezifisches Fine-Tuning konzipiert. Instruct Models hingegen durchlaufen zusätzlich ein Instruction Tuning \cite{wei2021finetuned}, bei dem sie mittels Supervised Fine-Tuning auf Instruktionsdatensätzen trainiert werden, um gezielt Anweisungen zu befolgen. Optional kann dieser Prozess durch Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training} weiter verfeinert werden. Mistral bietet beide Varianten an: Mistral-7B als Base Model und Mistral-7B-Instruct als instruction-tuned Version \cite{jiang2023mistral}. Der detaillierte Vergleich und die Auswahlbegründung zwischen diesen Varianten erfolgt in Kapitel~\ref{chap:4}.
        
        Für deutschsprachige Anwendungen ist die LeoLM-Familie von besonderer Relevanz. Diese Modelle nutzen Mistral-7B als Basis und durchlaufen ein Continued Pretraining auf deutschen Textkorpora \cite{leolm2023}. Das in dieser Arbeit verwendete Modell leo-mistral-hessianai-7b ist eine Base-Variante, die speziell für die deutsche Sprache optimiert wurde \cite{leolm2023}. Diese Adaption kombiniert die architektonischen Vorteile und die Effizienz von Mistral mit erhöhter Sprachkompetenz im Deutschen und erzielt dadurch bessere Ergebnisse für deutschsprachige Anwendungen als rein englischsprachige Basismodelle. Die Tokenization erfolgt bei Mistral mittels Byte Pair Encoding (BPE) mit einem Vokabular von 32.000 Tokens \cite{jiang2023mistral}, wobei LeoLM einen an die deutsche Morphologie angepassten Tokenizer verwendet \cite{leolm2023}, der besser mit Komposita, Umlauten und anderen sprachspezifischen Besonderheiten umgehen kann.
        
        \paragraph{Transfer Learning}
        
        Das Konzept des Transfer Learning bildet die theoretische Grundlage für die Nutzung vortrainierter Modelle in spezifischen Anwendungsdomänen. Transfer Learning bezeichnet den Wissenstransfer von einer Source Domain, in der das Modell vortrainiert wurde, zu einer Target Domain, für die es angepasst werden soll \cite{pan2010survey}. Dieser Ansatz reduziert den Bedarf an aufgabenspezifischen Trainingsdaten und die erforderliche Trainingszeit erheblich.
        
        Das etablierte Pretrain-Finetune-Paradigma verläuft in zwei Phasen: Zunächst erfolgt das Pretraining auf großen, unlabeled Textkorpora mittels unsupervised Learning \cite{radford2018improving}, wodurch das Modell grundlegendes Sprachverständnis, syntaktische Strukturen, semantische Zusammenhänge und implizites Weltwissen erwirbt. In der zweiten Phase wird das Modell mittels Fine-Tuning auf eine spezifische Aufgabe angepasst \cite{howard2018universal}, wobei supervised Learning mit aufgabenspezifischen Daten zum Einsatz kommt. Im Kontext dieser Arbeit bedeutet dies die Spezialisierung auf die Transformation von LVB-Verkehrsanweisungen.
        
        Die Vorteile von Transfer Learning sind vielfältig: Howard und Ruder zeigten mit ULMFiT, dass durch Transfer Learning mit nur 100 gelabelten Beispielen eine vergleichbare Performance erreicht werden kann wie mit 10.000 Beispielen beim Training from-scratch \cite{howard2018universal}. Vortrainierte Gewichte dienen als optimaler Startpunkt und beschleunigen das Training erheblich. Zudem führt das bereits vorhandene Sprachverständnis zu besserer Generalisierung auf neue Daten \cite{pan2010survey}.
        
        Dennoch bestehen Herausforderungen: Catastrophic Forgetting bezeichnet den Verlust vortrainierter Fähigkeiten während des Fine-Tunings, dem in Abschnitt~\ref{sec:2.2.4} weiter nachgegangen wird. Der Domain Shift zwischen Pretraining-Daten und Zieldomäne \cite{pan2010survey} kann zu Leistungseinbußen führen, insbesondere wenn sich Vokabular oder Sprachstil deutlich unterscheiden. Bei kleinen Datensätzen besteht zudem die Gefahr des Overfittings, was ebenfalls in Abschnitt~\ref{sec:2.2.4} behandelt wird.
        
        Für die vorliegende Arbeit ist besonders relevant, dass LeoLM bereits auf umfangreichen deutschen Textkorpora vortrainiert wurde \cite{leolm2023}, was eine solide Basis für die weitere Spezialisierung bildet. Das Fine-Tuning auf den vergleichsweise kleinen Datensatz der LVB-Verkehrsanweisungen wird durch Transfer Learning erst praktikabel und ermöglicht die notwendige domänenspezifische Anpassung an die fachsprachlichen Anforderungen des Verkehrssektors.
        
        \subsubsection{NLP-Anwendungen im Verkehrssektor}
        \label{sec:2.1.2}
        
        % Ziel: 2-3 Seiten
        
        Der Personentransportsektor produziert massive Mengen an Textdaten, darunter Verkehrsberichte in Echtzeit und historischer Form, technische Wartungsprotokolle, Incident Reports zu Unfällen und Störungen, Fahrgastbeschwerden und Feedback sowie ungefilterte, heterogene Beiträge in sozialen Medien. Das Datenvolumen übersteigt die menschliche Verarbeitungskapazität bei weitem, während sich regelbasierte Ansätze als unzureichend erwiesen haben. Starre Pattern-Matching-Logik skaliert nicht mit der Variabilität natürlicher Sprache, und der Wartungsaufwand wächst exponentiell mit zunehmender Systemkomplexität.

Natural Language Processing entwickelt sich in diesem Kontext zur Enabler-Technologie, die den Wandel von reaktivem zu proaktivem Verkehrsmanagement ermöglicht. Die automatisierte Erkenntnisgewinnung aus heterogenen Datenquellen steigert die operative Effizienz erheblich. Während frühe NLP-Ansätze auf Schlüsselwortsuche und einfache Klassifizierung beschränkt waren, beherrschen moderne Systeme komplexes Kontextverständnis und erfassen semantische Zusammenhänge. Diese Evolution transformiert Natural Language Processing vom reinen Mustererkenner zum Bedeutungsversteher.

Laut [SOURCE-MISSING] lassen sich NLP-Anwendungen im Verkehrssektor in mehrere Hauptkategorien einteilen. Verkehrsvorhersage und -management umfasst Predictive Analytics aus Textdaten, Anomalie-Detektion in Verkehrsberichten sowie textbasierte Ressourcenallokation. Sentimentanalyse von Social-Media-Daten ermöglicht ein Echtzeit-Stimmungsbarometer der Fahrgäste, die Früherkennung von Service-Problemen und unterstützt Crisis Management bei Großstörungen. Natürliche Sprachschnittstellen finden Anwendung in Query-Interfaces für Fahrplandatenbanken, Conversational AI für den Kundenservice sowie in Voice-Assistenten für Fahrzeuge. Verkehrsampel-Steuerung mittels NLP für adaptive Systeme und textbasierte Koordination zwischen Systemen ist für die vorliegende Arbeit weniger relevant.

Für diese Arbeit zentral ist die automatisierte Berichtserstellung und Dokumentenanalyse. Jüngste Forschungsarbeiten dokumentieren erhebliche Fortschritte: Die Zusammenfassung von Unfallberichten [SOURCE-MISSING], die Klassifizierung der Unfall-Schwere aus Textbeschreibungen [SOURCE-MISSING] sowie die generelle Verbesserung textbasierter Verarbeitung durch leistungsstarke Sprachmodelle zeigen das Potenzial dieser Technologien.

Direkt relevant für die vorliegende Arbeit sind öffentliche Verkehrsinformationsdienste. Large Language Models erweisen sich als effektiv in der Analyse und Aufbereitung von Fahrgastanfragen [SOURCE-MISSING]. Die Transformation technischer Informationen in endnutzergerechte Formate unterstützt die zentrale Aufgabe dieser Arbeit: die verständliche Aufbereitung von Verkehrsanweisungen für Fahrgäste unter Wahrung konsistenter und präziser Kommunikation. Mehrsprachige Verarbeitung wird mit steigender Vielseitigkeit von Large Language Models zunehmend möglich [SOURCE-MISSING], stellt jedoch aktuell ein sekundäres Ziel dar, das in dieser Arbeit nicht tiefergehend behandelt wird, jedoch Potenzial für zukünftige Erweiterungen bietet.

Der Fokus dieser Arbeit liegt primär auf automatisierter Berichtserstellung und öffentlichen Informationsdiensten. Der konkrete Anwendungsfall der Transformation von LVB-Verkehrsanweisungen schlägt eine Brücke zwischen technischer Fachsprache und Allgemeinverständlichkeit. Diese Aufgabe ist verwandt mit Dokumentenanalyse, Textzusammenfassung und Stiladaption, unterscheidet sich jedoch von reiner Klassifizierung durch die essenzielle generative Komponente.

Empirische Erfolge in verwandten Domänen stützen die Hypothese, dass Large Language Models für die Transformation von Verkehrsanweisungen geeignet sind. State-of-the-Art-Performance wurde bei Zusammenfassung und Klassifizierung nachgewiesen, die effektive Bearbeitung von Fahrgastanfragen ist dokumentiert, und die automatisierte Verarbeitung von Unfallberichten übertrifft regelbasierte Systeme deutlich.

Dennoch bestehen spezifische Herausforderungen: Domänenspezifisches Vokabular erfordert gezielte Spezialisierung der Modelle. Faktentreue ist bei sicherheitskritischen Informationen unabdingbar, sodass Halluzinationen vermieden werden müssen. Konsistenz in der Ausgabe ist erforderlich, sodass gleiche Eingaben zu identischen Ausgaben führen und ein gewisser Determinismus gewährleistet ist. Mehrsprachigkeit ist technisch möglich, aber ressourcenintensiv und daher für diese Arbeit nicht im Fokus.
        
        
        \subsubsection{Domänenspezifische Sprachmodelle}
        \label{sec:2.1.3}
        
        % Ziel: 1,5-2 Seiten
        
        Während allgemeine Large Language Models wie GPT-4 oder Claude eine beeindruckende Breite an Fähigkeiten demonstrieren, zeigt die Spezialisierung auf die Verkehrsdomäne signifikante Vorteile. Der Trade-off zwischen Generalität und Domänen-Expertise ist dabei zu berücksichtigen.

Domänenspezifisches Fine-Tuning erweist sich als kritischer Erfolgsfaktor [SOURCE-MISSING]. Wie am Beispiel von TrafficSafetyGPT dokumentiert, übertreffen spezialisierte Modelle konsistent allgemeine Modelle in verkehrsspezifischen Aufgaben. Technische Komponenten wie erweiterte Fachterminologie-Datenbanken, Domain-Specific Pre-Training auf Verkehrssicherheits-Korpora und professionelle Ausdrucksfähigkeiten mit technischer Präzision tragen zu diesem Erfolg bei. Die Vorteile manifestieren sich in effizienterer Zuweisung von Rechenressourcen, kürzeren Trainingszeiten bei gleichzeitig höherer Qualität sowie deutlich höherer Informationsabdeckung im Vergleich zu General-Purpose-Modellen.

Die multimodale Integration verschiedener Datenmodalitäten stellt sich als wesentlicher Erfolgsfaktor heraus [SOURCE-MISSING]. Die Kombination von narrativem, unstrukturiertem Text mit strukturierten Unfalldaten in Form von Tabellen und Kategorien sowie Metadaten zu Zeit, Ort und Beteiligten erzielt empirisch nachgewiesene Erfolge. Eine dokumentierte Reduktion der Fehlerrate um 54,2\,\% verdeutlicht den Synergieeffekt: Text liefert Kontext, während strukturierte Daten Präzision gewährleisten. Diese Strategie gilt als die vielversprechendste zur Leistungssteigerung.

Domain-Specific Pre-Training mittels Continued Pretraining auf Verkehrssicherheits-Korpora ermöglicht es Modellen, spezialisierte Terminologie und Kontexte zu erlernen. Signifikante Leistungsverbesserungen sind dokumentiert, analog zum deutschsprachigen Continued Pretraining von LeoLM auf Basis von Mistral.

Die Übertragung dieser Erkenntnisse auf die vorliegende Arbeit erfolgt durch eine stufenweise multimodale Integration verschiedener Optimierungen. Ausgehend vom Base Model LeoLM, das bereits für die deutsche Sprache optimiert ist, erfolgt Fine-Tuning auf domänenspezifischen Verkehrsanweisungen. Knowledge Enhancement integriert strukturierte Linien-Fahrzeug-Zuordnungen, während Prompt Engineering Stilrichtlinien und Beispiele bereitstellt. Das Ziel besteht darin, trotz eines kleinen Datensatzes fehlerfreie Ergebnisse zu erzielen. Die Forschung rechtfertigt diesen Multi-Methoden-Ansatz, da die Kombination verschiedener Techniken den Datenmangel kompensieren kann.

Deutschsprachige Modelle weisen Besonderheiten auf, die für die vorliegende Arbeit relevant sind. Die LeoLM-Familie basiert auf Mistral und durchläuft deutsches Continued Pretraining, was sprachspezifische Optimierung für Komposita und Morphologie ermöglicht. Da der LVB-Anwendungsfall rein deutschsprachig ist, stellt sich die Frage nach dem Vergleich zwischen englischen Modellen mit nachgelagerter Translation und nativen deutschen Modellen. Native Modelle bieten bessere Idiomatik und Präzision, während Translation das Risiko von Artefakten und Stilbrüchen birgt.

Die Herausforderungen deutschsprachiger Verkehrsdaten sind vielfältig [SOURCE-MISSING]. Heterogene Formatvielfalt erschwert die standardisierte Verarbeitung erheblich. Telegrafische Texte sind verkürzt und entbehren Artikel, Tabelleneinträge sind strukturiert und fragmentiert, während Fließtext-Berichte vollständige Sätze enthalten. Diese Heterogenität stellt hohe Anforderungen an die Vorverarbeitung.

Das Cross-Language Transferability-Problem manifestiert sich darin, dass englisch-trainierte Modelle nicht direkt auf deutschsprachige Daten anwendbar sind. Erhebliche Leistungseinbußen ohne deutschsprachiges Training sind dokumentiert. Zwei Lösungswege existieren: separates Training für deutschsprachige Daten, wie es der LeoLM-Ansatz verfolgt, oder mehrsprachige Trainingsdatensätze mit mindestens 20\,\% deutschen Anteilen für vergleichbare Performance. Diese Erkenntnisse begründen die Wahl deutschsprachiger Basismodelle für die vorliegende Arbeit.

Die Fehleranfälligkeit generischer Modelle resultiert aus zwei Hauptproblemen. Primär englischsprachiges Pretraining führt dazu, dass Modelle zwar übersetzen können, die Fehlerrate jedoch deutlich steigt. Idiomatische Fehler und ungünstige Wortwahl sind häufige Konsequenzen. Generisches Training ohne Verkehrsdomäne hat fehlende Fachterminologie und mangelndes Kontextverständnis für verkehrsspezifische Abkürzungen zur Folge. Der einzige verfügbare deutsche Verkehrsdatensatz von [SOURCE-MISSING] ist veraltet und limitiert, was eigenständiges Fine-Tuning unumgänglich macht.

Die Überleitung zu Kapitel~\ref{chap:4} verdeutlicht, dass domänenspezifische Modelle theoretisch überlegen sind, deutsche Verkehrsmodelle in der Praxis jedoch fehlen. Die Lösung besteht in der Kombination eines deutschen Basismodells mit verkehrsspezifischem Fine-Tuning. Die detaillierte Auswahlbegründung erfolgt im Modellauswahl-Kapitel.

Existierende verkehrsspezifische Modelle sind für die vorliegende Arbeit nicht geeignet. TrafficSafetyGPT [SOURCE-MISSING] basiert auf LLaMA von Meta AI und verwendet den TrafficSafety-2k-Datensatz, der behördliche Richtlinien und ChatGPT-generierte Daten umfasst. Der Fokus liegt auf Verkehrssicherheit (Traffic Safety), und die Vorteile bestehen in effizienter Ressourcennutzung und kurzen Trainingszeiten. Dennoch ist das Modell für LVB nicht geeignet, da es englischsprachig ist und keine deutsche Sprachkompetenz aufweist, der Safety-Fokus nicht der Fahrgastinformation entspricht und die Closed-Source-Basis DSGVO-Probleme für lokales Deployment verursacht.

Die TransGPT-Familie [SOURCE-MISSING] umfasst TransGPT-SM als unimodales Text-Modell basierend auf ChatGLM2-6B sowie TransGPT-MM als multimodales Modell mit Text- und Vision-Komponenten. Das Training erfolgte mit Verkehrsunterlagen, Büchern und Berichten für Anwendungen in Dokumentenanalyse und Führerscheinprüfungen. Das Modell ist für LVB nicht geeignet, da es chinesischsprachig ist (ChatGLM2 ist ein chinesisches Modell), der Fokus auf Wissensvermittlung statt Texttransformation liegt und keine Open-Source-Verfügbarkeit für lokales Fine-Tuning besteht.

Anwendungsspezifische Chatbots wie TP-GPT für Verkehrsüberwachung und Echtzeit-Daten [SOURCE-MISSING], TrafficGPT mit Integration von Traffic Foundation Models und Aufgaben-Dekomposition [SOURCE-MISSING] sowie ChatSUMO für SUMO-Simulator-Integration und Szenario-Generierung [SOURCE-MISSING] sind ebenfalls nicht für LVB geeignet. Diese Frameworks sind aufgabenspezifisch und nicht General-Purpose, erfordern Echtzeit-Datenbanken mit erheblichem Infrastruktur-Overhead, fokussieren auf Query-Beantwortung statt Texttransformation und sind englischsprachig.

Multimodale und sicherheitskritische Modelle wie ChatScene für AV-Sicherheitsszenarien [SOURCE-MISSING], AccidentGPT für Kollisionsvermeidung [SOURCE-MISSING], IDM-GPT mit fünf spezialisierten Agenten für Verkehrsanalysen [SOURCE-MISSING] sowie STEP-LLM für räumlich-zeitliche Verkehrsvorhersage [SOURCE-MISSING] sind nicht für LVB geeignet. Der Fokus liegt auf autonomen Fahrzeugen und Predictive Analytics, die Aufgabe besteht nicht in Textgenerierung, sondern in Szenario-Simulation, deutschsprachige Varianten fehlen, und die Modelle sind zu spezialisiert für allgemeine Fahrgastinformation.

Zusammenfassend ist keines dieser Modelle aus mehreren Gründen geeignet: Die Sprachbarriere, da alle Modelle englisch- oder chinesischsprachig sind; der Aufgaben-Mismatch, da Safety, Prediction und Simulation nicht der Text-Transformation entsprechen; Lizenzierungsprobleme, da die meisten Modelle proprietär sind oder auf Closed-Source-Basis beruhen; Infrastruktur-Anforderungen für Echtzeit-Datenbanken und Multi-Agenten-Systeme sowie Deployment-Einschränkungen, da die Modelle nicht für lokale, offline-fähige Ausführung konzipiert sind.

Die Konsequenz für die vorliegende Arbeit besteht darin, dass keine Off-the-Shelf-Lösung verfügbar ist. Dies macht eine eigenständige Modellentwicklung notwendig. Die Strategie besteht in der Verwendung eines deutschen Basismodells (LeoLM) mit domänenspezifischem Fine-Tuning. Dies füllt eine Forschungslücke bei deutschsprachiger Verkehrsinformation, wobei die detaillierte Begründung in Kapitel~\ref{chap:4} erfolgt.

Für kleine Datensätze ergeben sich aus der Forschung bewährte Strategien. Die Herausforderung besteht darin, dass der LVB-Datensatz vergleichsweise klein ist. Strategien aus der Forschung umfassen Transfer Learning von vortrainierten Modellen, multimodale Integration von Text und Strukturdaten, Data Augmentation (siehe Abschnitt~\ref{sec:2.2.3}) sowie parameter-effizientes Fine-Tuning mittels LoRA (siehe Abschnitt~\ref{sec:2.2.1}). Die Kombination dieser Methoden hat sich nachweislich als erfolgreich erwiesen.

Die Überleitung zu Abschnitt~\ref{sec:2.2} verdeutlicht, dass die theoretische Überlegenheit domänenspezifischer Modelle etabliert ist. Der nächste Schritt besteht darin zu klären, wie solche Modelle erstellt werden. Fine-Tuning-Methoden, Datenstrategien und Optimierungen stehen im Fokus des folgenden Abschnitts, der die Perspektive von „Was funktioniert?" zu „Wie macht man es?" verschiebt.


    \subsection{Sprachmodelle und Fine-Tuning}
    \label{sec:2.2}
    
    % Ziel: 4-5 Seiten
    
% ABSCHNITT-EINLEITUNG - STICHPUNKTE:
%
% Problemstellung der Modellanpassung
% - Vortrainierte Sprachmodelle verfügen über umfassendes allgemeines Sprachverständnis
% - Spezialisierung auf domänenspezifische Aufgaben erfordert gezielte Anpassung
% - LeoLM: Deutschsprachige Kompetenz vorhanden, Verkehrsdomäne nicht abgedeckt
% - Transfer von allgemeinem zu spezialisiertem Wissen als zentrale Herausforderung
%
% Ressourcenbeschränkungen beim Fine-Tuning
% - Full Fine-Tuning: Anpassung sämtlicher Modellparameter (7 Milliarden bei Mistral-7B)
% - Rechenaufwand: Training auf Multi-GPU-Clustern über mehrere Wochen
% - Speicherbedarf: Über 100 GB VRAM für Gradientenberechnung und Optimizer-States
% - Kostenstruktur: Cloud-Computing-Ressourcen im vier- bis fünfstelligen Bereich
% - Constraint dieser Arbeit: Consumer-Hardware mit begrenzten Ressourcen
% - Konsequenz: Parameter-effiziente Fine-Tuning-Methoden erforderlich
%
% Herausforderung begrenzter Trainingsdaten
% - LVB-Datensatz: Geschätzte Größenordnung 100-500 Trainingsbeispiele
% - Vergleich: ImageNet mit über 1 Million annotierten Bildern, LAION mit 5 Milliarden Text-Bild-Paaren
% - Risiko: Overfitting bei vollständiger Parameteranpassung mit kleinem Datensatz
% - Vorteil parameter-effizienter Verfahren: Reduzierter Datenbedarf durch gezielte Adaption
%
% Methodische Ansätze
% - Parameter-effiziente Fine-Tuning-Verfahren reduzieren Anzahl trainierbarer Parameter
% - Prompt Engineering als komplementärer Ansatz zur Leistungssteigerung
% - Datenaufbereitung und Augmentation bei limitierter Datenverfügbarkeit
% - Gegenmaßnahmen zu Overfitting und Catastrophic Forgetting
%
% Bezug zur praktischen Implementierung
% - Theoretische Konzepte dieses Abschnitts finden Anwendung in der Implementierung
% - Fundierte Entscheidungsfindung durch Verständnis methodischer Trade-offs
% - Wissenschaftlich fundierte Vorgehensweise statt empirischem Trial-and-Error-Ansatz
    
        \subsubsection{Fine-Tuning-Methoden}
        \label{sec:2.2.1}
        
        % Ziel: 2,5-3 Seiten
        
        % VON FULL FINE-TUNING ZU PEFT:
        % - Herausforderung großer vortrainierter Modelle \cite{smith2023,hua2023}
        %   * Training aller Modellparameter für jede Downstream-Aufgabe
        %   * Mit wachsenden Modellgrößen zunehmend unpraktikabel
        %   * Rechenressourcen und Speicherbedarf steigen exponentiell \cite{hua2023}
        % - Full Fine-Tuning bei 7B-Modellen:
        %   * 7 Milliarden Parameter müssen angepasst werden
        %   * Über 100 GB VRAM für Gradienten und Optimizer-States
        %   * Multi-GPU-Cluster über mehrere Wochen erforderlich
        %   * Kostenstruktur: Vier- bis fünfstelliger Bereich für Cloud-Computing
        % - Konsequenz: Parameter-effiziente Transferlernmethoden erforderlich
        
        % PEFT-TAXONOMIE:
        % Drei Hauptkategorien \cite{liu2025up}:
        %
        % 1. ADDITIVE FINE-TUNING:
        %    - Fügt zusätzliche trainierbare Module zum gefrorenen Basismodell hinzu
        %    - Originalgewichte bleiben unverändert
        %    - Beispiele: Adapter, Prefix Tuning, Prompt Tuning
        %    - Trade-off: Geringe Parameteranzahl vs. potenzielle Inferenz-Latenz
        %
        % 2. REPARAMETERIZED FINE-TUNING:
        %    - Zerlegt Gewichtsupdates in niedrigrangige Matrizen
        %    - Modifiziert indirekt die Originalgewichte
        %    - Hauptvertreter: LoRA (Low-Rank Adaptation)
        %    - Vorteil: Keine Inferenz-Latenz durch Weight Merging
        %
        % 3. SELECTIVE FINE-TUNING:
        %    - Trainiert nur ausgewählte Teilmengen existierender Parameter
        %    - Friert Großteil des Modells ein
        %    - Weniger verbreitet, nicht in dieser Arbeit verwendet
        
        % ============================================================================
        % LORA (LOW-RANK ADAPTATION) - AUSFÜHRLICH
        % ============================================================================
        
        % GRUNDPRINZIP & FUNKTIONSWEISE:
        % - Reparameterized Fine-Tuning-Ansatz \cite{liu2025up,he2025rasa}
        % - Theoretische Hypothese \cite{adegoke2024lora}:
        %   * Gewichtsänderungen während Adaptation haben niedrigen "intrinsic rank"
        %   * ΔW lässt sich effizient durch Low-Rank-Dekomposition approximieren
        %   * Ermöglicht indirekte Optimierung denser Layers via Rank-Decomposition
        % - Gewichtsupdates als niedrigrangige Dekomposition: ΔW = BA
        %   * W₀: Originalgewichte (eingefroren)
        %   * W = W₀ + ΔW = W₀ + BA
        %   * B ∈ ℝ^(d×r), A ∈ ℝ^(r×k), wobei r << d,k
        %   * Rank r als Hyperparameter kontrolliert Expressivität vs. Effizienz
        % - Keine direkten Änderungen an vortrainierten Gewichten
        % - Low-Rank-Matrizen werden separat trainiert \cite{gao2023,he2025rasa}
        % - Minimiert Bedarf an Hyperparameter-Retuning bei Rank-Variation \cite{adegoke2024lora}
        
        % EFFIZIENZGEWINNE:
        % Parameter-Reduktion \cite{hu2021lora}:
        %   * Bis zu 10.000-fache Reduktion trainierbarer Parameter
        %   * Vergleichbare Performance zu Full Fine-Tuning
        %   * Typisch: < 1% der Gesamtparameter trainierbar
        %
        % Speichereffizienz \cite{hu2021lora,adegoke2024lora}:
        %   * 3x weniger GPU-Memory-Bedarf als traditionelles Fine-Tuning \cite{hu2021lora}
        %   * Empirisch: 35% Reduktion der Memory Usage bei GPT-2 \cite{adegoke2024lora}
        %   * Besonders geeignet für ressourcenbeschränkte Umgebungen
        %   * Ermöglicht Training auf Consumer-Hardware
        %   * Bei Adam-Optimizer: VRAM-Reduktion bis zu 2/3 \cite{adegoke2024lora}
        %
        % Trainingszeit-Reduktion \cite{adegoke2024lora}:
        %   * 30% kürzere Trainingszeit im Vergleich zu Full Fine-Tuning
        %   * Benchmark (GPT-2 auf Tesla T4): 5,1 min (LoRA) vs. 7,4 min (Full FT)
        %   * Effizienzgewinn trotz erhöhter Iterationszahl (siehe Konvergenzproblematik)
        %
        % Deployment-Vorteile \cite{hu2021lora,adegoke2024lora}:
        %   * Modulare Architektur: Ein Basismodell, viele Task-Adapter
        %   * Nur Adapter-Parameter müssen pro Aufgabe gespeichert werden \cite{adegoke2024lora}
        %   * Löst "One-Domain-One-Model"-Problem
        %   * Kostengünstiges Task-Switching: Nur LoRA-Gewichte austauschen statt aller Parameter
        %   * Reduziert Storage- und Switching-Overhead im Multi-Task-Deployment \cite{adegoke2024lora}
        
        % INFERENZ-EIGENSCHAFTEN:
        % - Kein zusätzlicher Inferenz-Overhead \cite{hua2023,hu2021lora}
        % - Weight Merging nach Training: W_final = W₀ + BA
        % - Post-Training Integration in Originalgewichte \cite{hu2021lora}
        % - Entscheidender Vorteil gegenüber Adapter- und Prompt-Methoden
        
        % LIMITIERUNGEN & HERAUSFORDERUNGEN:
        %
        % 1. Konvergenzproblematik \cite{wang2025floe}:
        %    * Signifikant langsamere Konvergenz als Full Fine-Tuning
        %    * 5-6x mehr Iterationen und FLOPs für gleiche Performance
        %    * Erhöht Gesamt-Trainingskosten trotz Pro-Iteration-Effizienz
        %    * Kann zu schlechterer Testperformance führen
        %
        % 2. Architektonische Ursachen \cite{hu2021lora}:
        %    * Initialisierung von B mit Nullen: Langsame Trainingsdynamik zwischen A und B
        %    * Dropout nur für lange Trainings-Episoden geeignet
        %    * Skalierungsfaktor verursacht "kurzsichtige" Inter-Layer-Interaktionen
        %
        % 3. Weitere Training-Herausforderungen:
        %    * Potenzielle Catastrophic Forgetting-Probleme
        %    * Beeinträchtigung von Weltwissen in vortrainierten Modellen
        %    * Degradierung von Safety Alignment in fein-justierten Modellen
        %
        % 4. Rank-Sensitivität \cite{wang2025floe}:
        %    * Fixed-Rank-Constraint limitiert Flexibilität
        %    * Hohe Sensitivität gegenüber Rank-Auswahl
        %    * Optimaler Rank muss im Voraus identifiziert werden
        %    * Kostspieliges Retraining bei suboptimaler Wahl
        %
        % 5. Skalierungs-Limitierungen:
        %    * Performance sinkt bei kleineren Modellen (< 7B Parameter)
        %    * Besonders in Vision-Language-Pre-Training-Kontexten
        %
        % 6. Performance-Gap bei Komplexität \cite{wang2025floe}:
        %    * Lücke zu Full Fine-Tuning bei komplexen Datensätzen
        %    * Diverse Sub-Domänen und Task-Typen verschärfen Problem
        %    * Gap wächst mit zunehmender Szenario-Komplexität
        %
        % 7. Batching-Limitierungen \cite{adegoke2024lora}:
        %    * Herausforderung: Inputs verschiedener Tasks mit unterschiedlichen A/B-Matrizen
        %    * Schwierig in einem single Forward Pass zu batchen
        %    * Reduziert Parallelisierungspotenzial bei Multi-Task-Inferenz
        %    * Beeinträchtigt Durchsatz in produktiven Multi-Tenant-Systemen
        
        % QLORA - KOMBINATION MIT QUANTISIERUNG:
        % - Optimierte Implementation \cite{dettmers2023}
        % - LoRA-Adapter + 4-bit Quantisierung der Basisgewichte
        % - Weitere Speicherreduktion (~70%) ohne Qualitätsverlust
        % - Ermöglicht 7B-Modelle auf GPUs mit < 24 GB VRAM
        % - Wird in Abschnitt~\ref{sec:2.3.2} (Quantisierung) vertieft
        
        % KONFIGURATIONSPARAMETER:
        % - Rank (r): Dimensionalität der Low-Rank-Matrizen
        %   * Typisch: 4, 8, 16, 32, 64
        %   * Trade-off: Expressivität vs. Speicherbedarf
        % - Alpha: Skalierungsfaktor für LoRA-Updates
        %   * Beeinflusst Lernrate der Adapter-Gewichte
        % - Target Modules: Welche Transformer-Layer angepasst werden
        %   * Query, Key, Value, Output Projections
        %   * Gate, Up, Down Projections (bei Mistral-Architektur)
        
        % ============================================================================
        % ADAPTER - ADDITIVE FINE-TUNING
        % ============================================================================
        
        % GRUNDKONZEPT \cite{houlsby2019,gao2023}:
        % - Eines der frühesten PEFT-Frameworks
        % - Fügt kleine aufgabenspezifische Module mit Feedforward-Layers hinzu
        % - Skip-Connections integrieren Adapter-Output
        % - Originalgewichte bleiben eingefroren
        
        % ARCHITEKTUR:
        % - Bottleneck-Design: Down-Projektion → Aktivierung → Up-Projektion
        % - Einfügung zwischen Transformer-Schichten
        % - Modularer Austausch für verschiedene Tasks
        
        % EFFIZIENZ [SOURCE-MISSING: Houlsby et al., 2019; Shen et al., 2025; Deng et al., 2025]:
        % - Nur 3,6% zusätzliche Parameter pro Task
        % - Performance innerhalb 0,4% von Full Fine-Tuning
        % - Kompakte, erweiterbare Modelle
        % - Parameteranzahl: O(r(d_in + d_out)) pro Layer [SOURCE-MISSING: Shen, 2025]
        % - Weniger effizient als LoRAs Low-Rank-Dekomposition
        
        % DEPLOYMENT:
        % - Hohe Parameter-Sharing durch gefrorenes Basismodell [SOURCE-MISSING: Houlsby et al., 2019]
        % - Neue Tasks ohne Revisitation vorheriger
        % - Task-spezifische Adapter-Parameter müssen gespeichert werden
        
        % LIMITIERUNG - INFERENZ-LATENZ:
        % - Erhöht Modelltiefe durch zusätzliche Layer [SOURCE-MISSING: Ding et al., 2024; Li et al., 2021; Hu et al., 2021]
        % - Zusätzliche Latenz während Inferenz
        % - Weniger geeignet für latenz-sensitive Anwendungen [SOURCE-MISSING: Li et al., 2025]
        % - Struktureller Nachteil gegenüber LoRA (Weight Merging nicht möglich)
        
        % ============================================================================
        % PREFIX TUNING - ATTENTION-BASIERTES PEFT
        % ============================================================================
        
        % GRUNDKONZEPT [SOURCE-MISSING: Li et al., 2021]:
        % - Optimiert kontinuierliche aufgabenspezifische Vektoren
        % - Prefix-Vektoren vor Input-Embeddings
        % - Nachfolgende Tokens können auf Prefixes "attenden"
        % - Modifiziert Attention-Keys und -Values
        
        % EFFIZIENZ [SOURCE-MISSING: Li et al., 2021; Eyuboglu et al., 2025; Shen, 2025]:
        % - Lernt nur 0,1% der Originalparameter
        % - Extremste Parameter-Effizienz aller PEFT-Methoden
        % - Vergleichbare Performance bei ausreichender Modellgröße
        % - Modifiziert Input-Repräsentationen statt Modellgewichte
        
        % STÄRKEN IN SPEZIFISCHEN KONTEXTEN:
        % Multilingual Adaptation [SOURCE-MISSING: Snegha et al., 2025]:
        % - Überlegen gegenüber LoRA in mehrsprachigen Tasks
        % - Bis zu 28% höhere Accuracy auf XNLI
        % - 13% höhere F1 auf XQUAD
        % - 18% höhere Accuracy auf Belebele vs. Base Models
        % - Zusätzliche 4-6% Verbesserung gegenüber LoRA
        
        % LIMITIERUNGEN:
        % - Reduziert effektive Sequenzlänge durch Prefix-Tokens
        % - Inferenz-Overhead durch zusätzliche Kontext-Tokens [SOURCE-MISSING: Li et al., 2025; Diep et al., 2025]
        % - Erhöhter Computational Cost während Inferenz
        % - Sensitiv gegenüber Initialisierung [SOURCE-MISSING: Huang et al., 2024]
        % - Challenging to train, reduziert verfügbare Sequenzlänge [SOURCE-MISSING: Ding et al., 2024]
        
        % ============================================================================
        % PROMPT TUNING - SOFT PROMPTS
        % ============================================================================
        
        % GRUNDKONZEPT [SOURCE-MISSING: Lester et al., 2021]:
        % - Verwendet lernbare "Soft Prompts"
        % - Kontinuierliche Embeddings statt diskreter Tokens
        % - Training via Backpropagation
        % - Nur Prompt-Embeddings trainierbar
        
        % SKALIERUNGSVERHALTEN [SOURCE-MISSING: Lester et al., 2021]:
        % - Wird kompetitiver mit steigender Modellgröße
        % - Matched Full Fine-Tuning ab mehreren Milliarden Parametern
        % - Bei kleineren Modellen (<11B): Geringere Performance
        
        % EFFIZIENZ [SOURCE-MISSING: Lester et al., 2021; Eyuboglu et al., 2025]:
        % - Extreme Parameter-Effizienz durch diskrete Token-Level-Optimierung
        % - Minimale Speicheranforderungen
        % - Modifiziert Input-Repräsentationen wie Prefix Tuning
        
        % LIMITIERUNGEN:
        % - Inferenz-Overhead durch Prepending task-spezifischer Tokens [SOURCE-MISSING: Li et al., 2025]
        % - Zusätzlicher Computational Cost durch Prefix-Tokens
        % - Erhöhte Latenz vs. gefrorene Backbone-Modelle [SOURCE-MISSING: Diep et al., 2025; Lester et al., 2021]
        % - Sensitive Initialisierung [SOURCE-MISSING: Huang et al., 2024]
        
        % ============================================================================
        % VERGLEICHENDE BEWERTUNG DER PEFT-METHODEN
        % ============================================================================
        
        % PARAMETER-EFFIZIENZ (aufsteigend):
        % 1. Prompt Tuning: ~0,01% - Extremste Reduktion
        % 2. Prefix Tuning: ~0,1% - Sehr gering
        % 3. LoRA: ~0,1-1% (je nach Rank) - Stark reduziert
        % 4. Adapter: ~3,6% - Moderat reduziert
        
        % INFERENZ-OVERHEAD (aufsteigend):
        % 1. LoRA: ≈0 (Weight Merging möglich) - OPTIMAL
        % 2. Prefix Tuning: Gering (zusätzliche Tokens)
        % 3. Prompt Tuning: Gering (zusätzliche Tokens)
        % 4. Adapter: Hoch (zusätzliche Layer) - PROBLEMATISCH
        
        % PERFORMANCE-CHARAKTERISTIKA:
        % - LoRA [SOURCE-MISSING: Hu et al., 2021; Moghadas et al., 2024]:
        %   * On-par oder besser als Full Fine-Tuning (RoBERTa, DeBERTa, GPT-2, GPT-3)
        %   * Übertrifft Prompt Tuning in Gesamtperformance, Memory, Flexibilität
        %   * Performance-Gap bei komplexen, heterogenen Datensätzen [SOURCE-MISSING: Wang et al., 2025]
        %
        % - Adapter [SOURCE-MISSING: Houlsby et al., 2019]:
        %   * Konsistent stark auf Benchmarks (GLUE: innerhalb 0,4% von Full FT)
        %   * Zuverlässige Performance über verschiedene Tasks
        %
        % - Prefix Tuning [SOURCE-MISSING: Snegha et al., 2025; Kim et al., 2023]:
        %   * Superior in multilingualen Adaptation-Tasks
        %   * Besser in zeit-beschränkten Szenarien (3-5 Minuten Training)
        %   * Outperforms LoRA in spezifischen Kontexten
        %
        % - Prompt Tuning:
        %   * Schwächer bei kleinen Modellen
        %   * Konkurrenzfähig ab >11B Parametern
        
        % TRAINING-DYNAMIK:
        % - LoRA: Langsame Konvergenz (5-6x mehr Iterationen) [SOURCE-MISSING: Wang et al., 2024]
        % - Prompt/Prefix: Initialisierungs-sensitiv [SOURCE-MISSING: Huang et al., 2024]
        % - Adapter: Stabile Konvergenz, aber Speicher-intensiv
        
        % WARUM LORA FÜR DIESE ARBEIT:
        % 1. Kein Inferenz-Overhead: Kritisch für Produktionsumgebung
        % 2. Gute Balance: Parameter-Effizienz vs. Performance
        % 3. Modularität: Mehrere Task-Adapter möglich
        % 4. Hardware-Feasibility: Kombination mit QLoRA für Consumer-GPUs
        % 5. Bewährte Performance: Erfolgreich in ähnlichen Domänen-Adaptionen
        % 6. Community-Support: Umfangreiche Implementierungen und Best Practices
        
        % Konvergenz-Limitierung akzeptabel, da:
        % - Kleiner Datensatz (kurze Trainings-Episoden weniger kritisch)
        % - Unsloth-Optimierungen kompensieren teilweise (siehe unten)
        % - Quality over Speed: Finale Performance wichtiger als Trainingszeit
        
        % ============================================================================
        % UNSLOTH - OPTIMIERUNGSFRAMEWORK FÜR EFFIZIENTES TRAINING
        % ============================================================================
        %
        % - UNSLOTH: Optimierungsframework für effizientes Training
        %   
        %   PROBLEMSTELLUNG & MOTIVATION:
        %   * Herausforderung: Auch PEFT-Methoden wie LoRA erfordern bei 7B-Modellen erhebliche Ressourcen
        %   * Traditionelle Frameworks (Hugging Face Transformers, PyTorch): Hardware-Limitierungen
        %   * Instabilitäten bei LoRA-Training: Gradient Explosion, numerische Instabilität \cite{zhong2025}
        %   * Zugangshürde: Enterprise-Grade GPUs oder Multi-GPU-Setups erforderlich
        %   * Relevanz für diese Arbeit: Consumer-Hardware-Constraints (siehe Abschnitt~\ref{sec:6.1})
        %   
        %   TECHNISCHE OPTIMIERUNGEN:
        %   
        %   1. Speichereffizienz (50-80% Reduktion des VRAM-Bedarfs) \cite{hasan2025,pingua2025,lu2025}:
        %      * 4-bit Quantization der Base-Model-Weights (~70% Memory-Reduktion) \cite{zhong2025}
        %      * Optimierte QLoRA-Implementation: LoRA-Adapter + 4-bit Quantization \cite{dettmers2023}
        %      * Gradient Checkpointing für reduzierte Aktivierungs-Speicherung
        %      * Ermöglicht 7B-Modelle auf Consumer-GPUs (RTX 3090, Tesla T4) \cite{tahir2024,phan2025,bandara2025}
        %      * Praktisch: Training ohne Qualitätsverlust bei drastisch reduziertem VRAM
        %   
        %   2. Training-Beschleunigung (2-5x schneller) \cite{hasan2025,lu2025,pingua2025}:
        %      * Flash Attention 2: IO-optimierte Attention-Berechnung \cite{dao2022}
        %        - Reduzierte Transfers zwischen GPU-Memory-Komponenten \cite{lea2025}
        %        - Effizientere Key-Value-Query-Matrix-Operationen
        %      * Custom Kernel Implementations (OpenAI Triton) \cite{lea2025}:
        %        - Fused Operations: Weniger Speicherzugriffe
        %        - Optimierte Matrix-Multiplikationen für LoRA
        %        - Custom PyTorch Autograd-Funktionen
        %      * Praktisch: Training-Epochen in 26 Minuten (statt Stunden) \cite{mohammadi2025}
        %   
        %   3. Numerische Stabilität:
        %      * Gradient Clipping gegen Gradient Explosion \cite{zhong2025}
        %      * Layer Normalization Calibration für LoRA-spezifische Instabilitäten \cite{zhong2025}
        %      * Multi-Precision Support (8-bit, bfloat16) für Hardware-Flexibilität \cite{upadhyay2025,pourcel2025}
        %      * Verhindert Training-Abbrüche durch Overflow
        %   
        %   4. PEFT-Integration:
        %      * Unterstützung für LoRA-Target-Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj \cite{upadhyay2025,rao2025}
        %      * Vollständig kompatibel mit Hugging Face PEFT
        %      * Nahtlose Integration in bestehende Workflows
        %   
        %   DEMOKRATISIERUNG & ZUGÄNGLICHKEIT:
        %   * Hardware-Accessibility: Single-GPU statt Multi-GPU \cite{lea2025}
        %   * Consumer-Grade Hardware: RTX 3090, NVIDIA A40, Tesla T4 \cite{tahir2024,bandara2025,phan2025}
        %   * Free Cloud Platforms: Google Colab (T4, TPU), Kaggle (P100) \cite{phan2025,bandara2025}
        %   * Strukturierte Templates: Guided Notebooks für niedrige Einstiegshürde \cite{jimenez2025}
        %   * Relevanz: Ermöglicht Forschung ohne Enterprise-Budget \cite{huang2025,machlovi2025}
        %   
        %   SYNERGIEEFFEKTE:
        %   * LoRA: Reduziert trainierbare Parameter → weniger Rechenaufwand
        %   * Unsloth: Optimiert verbleibende Operationen → schnellere Ausführung
        %   * QLoRA + Unsloth: Memory-Effizienz + Speed-Optimierung \cite{dettmers2023}
        %   * Keine Qualitätseinbußen gegenüber Standard-Fine-Tuning
        %   
        %   ABGRENZUNG:
        %   * DeepSpeed: Multi-GPU-Verteiltes Training
        %   * Unsloth: Single-GPU-Optimierung
        %   * Komplementär zu PEFT, nicht alternatives Verfahren
        %   * Fokus: Accessibility statt Scale
        %   
        %   PRAKTISCHE IMPLIKATIONEN FÜR DIESE ARBEIT:
        %   * Iterationsgeschwindigkeit: Mehr Experimente in gleicher Zeit
        %   * Hardware-Feasibility: Training auf verfügbarer Hardware möglich
        %   * Energieeffizienz: Reduzierte Trainingszeit → geringerer Energieverbrauch
        %   * Stabilität: Zuverlässigere Training-Runs in Constrained Environments \cite{mansha2025}
        %   
        %   Verweis: Konkrete Anwendung, Konfiguration und Trainings-Logs in Abschnitt~\ref{sec:6.2.1}
        
        
        \subsubsection{Prompt Engineering}
        \label{sec:2.2.2}
        
        % Ziel: 1,5-2 Seiten
        
        % EINLEITUNG - PROMPT ENGINEERING ALS KOMPLEMENTÄRE METHODE:
        % - Fine-Tuning (Abschnitt~\ref{sec:2.2.1}): Permanente Modellanpassung durch Parameteränderung
        % - Prompt Engineering: Verhaltenssteuerung ohne Gewichtsänderungen
        % - Synergieeffekt: Fine-Tuning für domänenspezifische Fähigkeiten, Prompts für aufgabenspezifische Kontrolle
        % - Relevanz für diese Arbeit: Stilrichtlinien, Formatvorgaben, Konsistenz
        
        % VON ZERO-SHOT ZU FEW-SHOT:
        % - Zero-Shot Prompting: Nur Instruktion, keine Beispiele
        %   * Definition: Modell erhält Aufgabenbeschreibung ohne Demonstrationen
        %   * Stärke: Funktioniert bei großen, instruction-tuned Modellen (GPT-4, Claude)
        %   * Limitation: Base Models (wie LeoLM-7B) haben schwache Zero-Shot-Performance
        %   * Nicht ausreichend für komplexe Transformationsaufgaben
        %   * Beispiel: "Schreibe diese Verkehrsanweisung verständlich um." → Unpräzise Ergebnisse
        %
        % - Few-Shot Learning: Aufgabe + Beispiele im Prompt \cite{wei2021finetuned,berryman2024prompt}
        %   * Definition: In-Context Learning durch Demonstration gewünschten Verhaltens
        %   * Mechanismus: Modell erkennt Muster aus Input-Output-Paaren
        %   * Anzahl Beispiele: Typisch 1-10 Shots (Context-Window-limitiert) \cite{berryman2024prompt}
        %   * Empirische Erkenntnisse: Dramatische Leistungssteigerung vs. Zero-Shot \cite{wei2021finetuned}
        %   * Performance skaliert mit Beispielanzahl (bis zu Plateau-Effekt) \cite{berryman2024prompt}
        %
        % - Few-Shot vs. Fine-Tuning \cite{berryman2024prompt}:
        %   * Few-Shot: Keine permanenten Änderungen, flexibel anpassbar
        %   * Fine-Tuning: Modellgewichte ändern sich, domänenspezifische Expertise
        %   * Trade-off: Few-Shot benötigt längere Prompts (Kontext-Token-Kosten)
        %   * Kombination optimal: Fine-Tuning für Grundfähigkeiten + Few-Shot für Nuancen
        
        % FEW-SHOT PROMPT DESIGN - PRAKTISCHE ÜBERLEGUNGEN:
        %
        % 1. Beispielauswahl (Example Selection) \cite{berryman2024prompt}:
        %    - Diversität: Abdeckung verschiedener Störungstypen (Bauarbeiten, technische Störungen, Umleitungen)
        %    - Repräsentativität: Häufigste Fälle zuerst, Edge Cases später
        %    - Qualität über Quantität: 3 perfekte Beispiele > 10 durchschnittliche \cite{berryman2024prompt}
        %    - Balancierung: Gleiche Verteilung wie im Datensatz (siehe Abschnitt~\ref{sec:2.2.3})
        %
        % 2. Beispielreihenfolge (Example Ordering) \cite{berryman2024prompt}:
        %    - Recency Bias: Letzte Beispiele haben stärksten Einfluss
        %    - Strategie: Komplexität aufsteigend (einfach → schwierig)
        %    - Ähnlichkeitsbasierte Sortierung: Relevanteste Beispiele zuerst
        %
        % 3. Formatierung und Struktur \cite{berryman2024prompt}:
        %    - Klare Trennung zwischen Input und Output (z.B. "Input:", "Output:")
        %    - Konsistente Formatierung über alle Beispiele hinweg
        %    - Delimiter zwischen Beispielen (z.B. "---", "\n\n")
        %    - Template-Konsistenz: Gleiche Struktur wie bei Inferenz
        
        % INSTRUCTION PROMPTING - SYSTEMANWEISUNGEN:
        %
        % - Definition: Explizite Aufgabenbeschreibung vor Beispielen
        % - Komponenten effektiver Instructions \cite{berryman2024prompt,ouyang2022training}:
        %   * Rollenspezifikation: "Du bist ein Experte für Fahrgastinformation im ÖPNV"
        %   * Aufgabendefinition: Präzise Beschreibung der Transformation
        %   * Constraints: Was NICHT getan werden darf (keine Halluzinationen, keine Informationsweglassung)
        %   * Stilrichtlinien: Tonalität, Formalität, Zielgruppe
        %   * Ausgabeformat: Erwartete Struktur des generierten Texts
        %
        % - Instruction-Länge und Spezifität \cite{berryman2024prompt}:
        %   * Trade-off: Detailliert vs. flexibel
        %   * Zu vage: "Schreibe verständlich" → Interpretation variiert
        %   * Zu spezifisch: "Nutze genau 2 Sätze mit maximal 15 Wörtern" → Unnatürlich
        %   * Optimal: Prinzipien statt starre Regeln ("Bevorzuge aktive Formulierungen")
        %
        % - Negativbeispiele und Abgrenzungen \cite{berryman2024prompt}:
        %   * "Schreibe NICHT im Telegrammstil" (effektiver als nur positive Vorgaben)
        %   * "Erfinde KEINE Details, die nicht in der Eingabe stehen" (Anti-Halluzination)
        %   * Empirisch: Kombination aus positiven und negativen Constraints am effektivsten
        
        % CHAIN-OF-THOUGHT PROMPTING - KOMPLEXE REASONING:
        %
        % - Grundkonzept \cite{berryman2024prompt,bonstra2024prompt}:
        %   * Standard-Prompting: Direkte Input → Output-Zuordnung
        %   * Chain-of-Thought: Zwischenschritte explizit machen
        %   * Mechanismus: Modell zeigt Denkprozess ("Lass uns Schritt für Schritt denken") \cite{bonstra2024prompt}
        %   * Besonders effektiv bei Multi-Hop-Reasoning und komplexen Transformationen \cite{berryman2024prompt}
        %
        % - Anwendung auf Verkehrsanweisungs-Transformation:
        %   * Schritt 1: Identifiziere Kernaussage (welche Linie, welche Störung)
        %   * Schritt 2: Extrahiere Entitäten (Liniennummern, Haltestellen, Zeitangaben)
        %   * Schritt 3: Bestimme Fahrzeugtyp aus Knowledge Base
        %   * Schritt 4: Formuliere verständlichen Satz mit allen Informationen
        %   * Vorteil: Reduziert Fehler durch strukturierten Prozess
        %
        % - Zero-Shot-CoT vs. Few-Shot-CoT \cite{bonstra2024prompt}:
        %   * Zero-Shot-CoT: "Lass uns Schritt für Schritt denken" als Trigger
        %   * Few-Shot-CoT: Beispiele mit expliziten Reasoning-Schritten
        %   * Few-Shot-CoT überlegen bei domänenspezifischen Aufgaben
        %   * Relevant für diese Arbeit: Strukturierte Transformation mit Reasoning
        %
        % - Limitierungen \cite{berryman2024prompt}:
        %   * Erhöhter Token-Verbrauch durch Zwischenschritte
        %   * Nicht notwendig bei einfachen Transformationen
        %   * Produktionsumgebung: Post-Processing entfernt Reasoning (nur Endergebnis ausgeben)
        
        % PROMPT TEMPLATE DESIGN FÜR AUTOMATISIERUNG:
        %
        % - Motivation: Manuelle Prompts für einzelne Anfragen vs. Templates für Batch-Verarbeitung \cite{berryman2024prompt}
        % - Template-Struktur \cite{berryman2024prompt}:
        %   * System Prompt: Statisch, enthält Rollenspezifikation und Grundregeln
        %   * Few-Shot Examples: Statisch oder dynamisch ausgewählt
        %   * User Prompt: Variabel, enthält konkrete Verkehrsanweisung
        %   * Optional: Knowledge Context (Linien-Fahrzeug-Zuordnungen als JSON)
        %
        % - Variablen-Substitution \cite{berryman2024prompt}:
        %   * Platzhalter für dynamische Inhalte: {INPUT_INSTRUCTION}, {KNOWLEDGE_BASE}
        %   * Formatierung: JSON-Escaping, Whitespace-Normalisierung
        %   * Validierung: Prüfung auf vollständige Substitution vor Inferenz
        %
        % - Template-Typen für verschiedene Störungskategorien:
        %   * Typ A: Bauarbeiten (erfordert Alternativrouten-Information)
        %   * Typ B: Technische Störungen (erfordert Fahrzeugtyp-Präzision)
        %   * Typ C: Umleitungen (erfordert geografische Klarheit)
        %   * Adaptive Template-Auswahl basierend auf Input-Klassifizierung
        
        % PROMPT OPTIMIZATION - ITERATIVE VERBESSERUNG:
        %
        % - Systematische Optimierung \cite{berryman2024prompt}:
        %   * Baseline erstellen: Einfacher Prompt als Ausgangspunkt
        %   * Fehleranalyse: Welche Arten von Fehlern treten auf?
        %   * Gezielte Constraints hinzufügen: Adressierung spezifischer Fehlerklassen
        %   * A/B-Testing: Verschiedene Prompt-Varianten vergleichen
        %   * Iterative Refinement: Schrittweise Verbesserung
        %
        % - Automatische Prompt Optimization (kurz erwähnen) \cite{berryman2024prompt}:
        %   * APE (Automatic Prompt Engineering)
        %   * LLMs generieren und evaluieren Prompt-Varianten
        %   * Nicht in dieser Arbeit verwendet (zu ressourcenintensiv)
        %   * Manuelle Optimierung praktikabler bei kleinem Anwendungsbereich
        %
        % - Metrik-gesteuerte Optimierung \cite{berryman2024prompt}:
        %   * Faktentreue: Prüfung gegen strukturierte Eingabedaten
        %   * Stilkonformität: Einhaltung der Richtlinien (siehe Abschnitt~\ref{sec:3.2})
        %   * Konsistenz: Gleiche Eingaben → identische Ausgaben (deterministische Temperatur)
        %   * Effizienz: Token-Länge minimieren bei gleichbleibender Qualität
        
        % ADVANCED PROMPTING TECHNIQUES (KURZ):
        %
        % - Self-Consistency \cite{bonstra2024prompt}:
        %   * Mehrfache Generierung mit Sampling, dann Mehrheitsentscheidung
        %   * Reduziert Halluzinationen durch Konsensbildung
        %   * Trade-off: Mehrfache Inferenz-Kosten
        %   * Anwendbar bei kritischen Transformationen
        %
        % - Least-to-Most Prompting \cite{berryman2024prompt}:
        %   * Zerlegt komplexe Aufgaben in einfachere Teilschritte
        %   * Jeder Schritt baut auf vorherigem auf
        %   * Ähnlich zu Chain-of-Thought, aber strukturierter
        %
        % - ReAct (Reasoning + Acting) \cite{berryman2024prompt}:
        %   * Kombination aus Reasoning und Tool-Use
        %   * Modell entscheidet, wann externe Informationen benötigt werden
        %   * Nicht direkt relevant für diese Arbeit (keine externen Tools)
        
        % PROMPTING BEI BASE MODELS VS. INSTRUCTION-TUNED MODELS:
        %
        % - Base Models (wie LeoLM-7B Base) \cite{berryman2024prompt}:
        %   * Schwache Instruktionsbefolgung ohne Fine-Tuning
        %   * Tendenz zur Textkontinuation statt Aufgabenlösung
        %   * Erfordern präzisere, strukturiertere Prompts
        %   * Few-Shot Examples essentiell (nicht optional)
        %   * Completion-Framing: "Text: ... Umschreibung:" (statt "Schreibe um:")
        %
        % - Instruction-Tuned Models (wie LeoLM-7B-Instruct) \cite{berryman2024prompt,ouyang2022training}:
        %   * Bessere Zero-Shot-Performance
        %   * Verstehen imperative Formulierungen ("Transformiere", "Schreibe")
        %   * Flexiblere Prompt-Formate akzeptabel
        %   * Few-Shot verbessert weiter, aber weniger kritisch
        %
        % - Implikation für diese Arbeit \cite{berryman2024prompt}:
        %   * Base Model gewählt (siehe Kapitel~\ref{chap:4})
        %   * Prompt Engineering umso wichtiger bei Base Models
        %   * Fine-Tuning kann Prompt-Abhängigkeit reduzieren, aber nicht eliminieren
        %   * Post-Fine-Tuning Prompts: Kürzer und direkter möglich
        
        % PROMPT ENGINEERING IM KONTEXT DIESER ARBEIT:
        %
        % - Einsatz in verschiedenen Phasen:
        %   1. Pre-Fine-Tuning: Evaluation verschiedener Base Models (Kapitel~\ref{chap:4})
        %   2. Training: Few-Shot Examples als Teil des Trainingsformats (Kapitel~\ref{chap:5})
        %   3. Post-Fine-Tuning: Inference-Zeit-Prompts für Konsistenz (Kapitel~\ref{chap:6})
        %
        % - Spezifische Anwendungen:
        %   * System Prompt: LVB-Stilrichtlinien, Zielgruppendefinition (Fahrgäste)
        %   * Knowledge Integration: Linien-Fahrzeug-Zuordnungen als strukturierter Kontext
        %   * Constraint Specification: Keine Halluzinationen, vollständige Informationswiedergabe
        %   * Format Control: Einzelner Fließtext-Satz als Output
        %
        % - Synergieeffekt mit Fine-Tuning:
        %   * Fine-Tuning lernt domänenspezifisches Vokabular und Muster
        %   * Prompts steuern aufgabenspezifische Nuancen und Edge Cases
        %   * Kombination robuster als jede Methode einzeln
        %   * Reduziert Overfitting-Risiko (Abschnitt~\ref{sec:2.2.4}) durch flexible Steuerung
        
        % ÜBERLEITUNG ZU ABSCHNITT 2.2.3:
        % - Prompt Engineering: Verhaltenssteuerung zur Inferenzzeit
        % - Nächster Schritt: Datenqualität und -struktur für Fine-Tuning
        % - Zusammenspiel: Prompts definieren Aufgabe, Daten trainieren Fähigkeiten
        % - Von "Wie kommuniziere ich die Aufgabe?" zu "Wie bereite ich Trainingsdaten auf?"
        
        
        \subsubsection{Datensätze für Fine-Tuning}
        \label{sec:2.2.3}
        
        % Ziel: 2-2,5 Seiten
        
        % EINLEITUNG
        % - Qualität und Zusammenstellung von Trainingsdaten als kritischer Erfolgsfaktor
        % - Besondere Herausforderungen bei kleinen domänenspezifischen Datensätzen
        % - Transfer Learning reduziert Datenbedarf, aber ausreichende Qualität bleibt essentiell
        
        % 1. STRUKTUR VON TRAININGSDATEN
        % - Input-Output-Paare für Supervised Fine-Tuning
        % - Format-Anforderungen (JSON, JSONL, spezifische Modell-Formate)
        % - Konsistenz in Struktur und Formatierung
        % - Annotationsrichtlinien: Klare, eindeutige Richtlinien zur Qualitätssicherung
        % - Inter-Annotator Agreement als Qualitätsmetrik
        %
        % Prompt-Template-basiertes Format-Design \cite{berryman2024prompt}:
        % - Training-Daten sollten Inference-Format widerspiegeln
        % - Instruction + Few-Shot Examples + Task als Trainingsstruktur
        % - Konsistente Delimiter und Formatierung zwischen Training und Inferenz
        % - Vorteil: Modell lernt nicht nur Inhalt, sondern auch Format-Erwartungen
        % - Verweis auf Prompt Engineering-Prinzipien (siehe Abschnitt~\ref{sec:2.2.2})
        
        % 2. DATA AUGMENTATION STRATEGIEN
        % - Definition und Motivation: Vergrößerung des Datensatzes ohne zusätzliche manuelle Annotation
        % - Ziele: Generalisierung verbessern, Overfitting reduzieren, Robustheit erhöhen
        %
        % Methoden der Datenaugmentierung \cite{feng2021survey}:
        % a) RULE-BASED AUGMENTATION:
        %    - Synonym Replacement: Austausch von Wörtern durch Synonyme
        %    - Random Insertion: Einfügen zusätzlicher Wörter
        %    - Random Swap: Vertauschen von Wortpositionen
        %    - Random Deletion: Entfernen einzelner Wörter
        %    - EDA (Easy Data Augmentation) \cite{wei2019eda}: Kombination obiger Techniken
        %    - Effektiv bei kleinen Datensätzen (50-500 Beispiele)
        %
        % b) MODEL-BASED AUGMENTATION:
        %    - Back-Translation: Übersetzung in andere Sprache und zurück
        %    - Paraphrasing mit vortrainierten Modellen
        %    - Contextual Word Embeddings für Ersetzungen
        %    - Template-basierte Generierung \cite{kumar2020data}
        %    - Höhere Qualität, aber rechenintensiver
        %
        % c) SYNTHETIC DATA GENERATION:
        %    - Generierung komplett neuer Beispiele durch LLMs
        %    - Constraint-basierte Generation mit validierten Entitäten \cite{kumar2020data}
        %    - Kontrolle über Diversität und Abdeckung
        %    - Risiko: Halluzinationen und unrealistische Beispiele
        %    - Best Practice: Kombination aus realen und synthetischen Daten (typisch 20-30% synthetisch)
        %
        % Empirische Erkenntnisse:
        % - EDA zeigt 50% der Daten können gleiche Accuracy erreichen wie 100% ohne Augmentierung \cite{wei2019eda}
        % - Effekt verstärkt sich bei kleineren Datensätzen
        % - Qualität wichtiger als Quantität bei synthetischen Daten
        
        % 3. BALANCED DATASETS UND CLASS DISTRIBUTION
        % - Problematik unbalancierter Datensätze:
        %   * Modell tendiert zu häufigen Klassen
        %   * Schlechtere Performance auf seltenen, aber wichtigen Fällen
        %   * Bias in Vorhersagen
        % - Strategien für Balance \cite{feng2021survey}:
        %   * Oversampling unterrepräsentierter Kategorien
        %   * Undersampling überrepräsentierter Kategorien (mit Vorsicht)
        %   * Synthetic Minority Over-sampling (SMOTE-ähnliche Ansätze)
        %   * Class-weighted Loss Functions
        % - Empfohlene Verhältnisse:
        %   * Ideale Balance: Gleiche Anzahl pro Kategorie
        %   * Praxis: Max. 1:3 Verhältnis zwischen seltenster und häufigster Kategorie akzeptabel
        %   * Bei stärkerer Imbalance: Gezielte Augmentierung erforderlich
        
        % 4. UMGANG MIT KLEINEN DATENSÄTZEN
        % - Definition "klein": Typischerweise < 1000 Beispiele für Fine-Tuning
        % - Few-Shot Learning als Alternative \cite{wei2021finetuned}
        % - Data Efficiency Techniques:
        %   * Aggressive Augmentierung (aber Qualität wahren)
        %   * Parameter-effiziente Methoden (LoRA) reduzieren Overfitting-Risiko
        %   * Niedrigere Learning Rates
        %   * Early Stopping basierend auf Validation Loss
        % - Mixed Task Training:
        %   * Kombination domänenspezifischer und allgemeiner Daten
        %   * Verhältnis typisch 85-90% spezifisch, 10-15% allgemein \cite{raffel2020exploring}
        %   * Verhindert Catastrophic Forgetting (siehe Abschnitt~\ref{sec:2.2.4})
        %   * Erhält Generalisierungsfähigkeit
        
        % 5. TRAIN-VALIDATION-TEST-SPLIT
        % - Standard-Aufteilung: 70-80% Training, 10-15% Validation, 10-15% Test
        % - Bei kleinen Datensätzen: 80-10-10 oder Cross-Validation
        % - Wichtig: Stratified Split bei kategorischen Daten (erhält Balance)
        % - Temporale Splits bei zeitabhängigen Daten
        % - Vermeidung von Data Leakage zwischen Splits
        
        % 6. QUALITÄTSSICHERUNG
        % - Automatisierte Validierung (siehe Abschnitt~\ref{sec:5.2.2} für praktische Umsetzung):
        %   * Strukturprüfung (Format, Pflichtfelder)
        %   * Konsistenzprüfung (Entitäten, Fakten)
        %   * Duplikatserkennung
        % - Manuelle Stichprobenprüfung
        % - Iterative Verbesserung basierend auf Modellfehlern
        
        % ÜBERGANG zu Kapitel~\ref{chap:5}:
        % - Diese theoretischen Grundlagen bilden Basis für praktische Datensatzerstellung
        % - Konkrete Anwendung dieser Prinzipien in Abschnitt~\ref{sec:5.2} beschrieben
        % - Anpassung an spezifische Anforderungen der Verkehrsanweisungstransformation
        
        
        \subsubsection{Herausforderungen beim Fine-Tuning}
        \label{sec:2.2.4}
        
        % Ziel: 1,5-2 Seiten
        
        % EINLEITUNG:
        % - Fine-Tuning vortrainierter Modelle bringt spezifische Herausforderungen
        % - Spannungsfeld: Spezialisierung auf neue Aufgabe vs. Erhalt allgemeiner Fähigkeiten
        % - Besonders kritisch bei kleinen domänenspezifischen Datensätzen
        % - Drei zentrale Problemfelder: Overfitting, Catastrophic Forgetting, Konvergenz
        
        % ============================================================================
        % 1. OVERFITTING - ÜBERANPASSUNG AN TRAININGSDATEN
        % ============================================================================
        
        % DEFINITION UND SYMPTOME:
        % - Modell lernt Trainingsdaten auswendig statt Muster zu generalisieren
        % - Charakteristisch: Hohe Training Accuracy, niedrige Validation/Test Accuracy
        % - Divergenz zwischen Training und Validation Loss
        % - Besonders ausgeprägt bei kleinen Datensätzen (< 1000 Beispiele)
        % - LoRA reduziert Overfitting-Risiko gegenüber Full Fine-Tuning, eliminiert es aber nicht
        %
        % URSACHEN:
        % - Modellkomplexität übersteigt Informationsgehalt der Trainingsdaten
        % - Zu hohe Anzahl trainierbarer Parameter relativ zur Datensatzgröße
        % - Zu viele Trainings-Epochen: Modell memoriert statt zu lernen
        % - Unbalancierte Datensätze: Bias zu überrepräsentierten Kategorien
        % - Zu hohe Learning Rate: Instabile, zu schnelle Anpassung an Trainingsdaten
        %
        % Bei LoRA-Fine-Tuning spezifisch:
        % - Höherer Rank (r) → mehr trainierbare Parameter → höheres Overfitting-Risiko
        % - Mehr Target Modules → größere Angriffsfläche für Überanpassung
        % - Trade-off: Expressivität vs. Generalisierung
        %
        % VERMEIDUNGSSTRATEGIEN:
        %
        % a) Regularisierung [SOURCE-MISSING: Goodfellow et al., 2016]:
        %    - L2-Regularisierung (Weight Decay): Bestraft große Gewichtswerte
        %      * Mathematisch: Zusätzlicher Term λ||w||² zur Loss-Funktion
        %      * Fördert kleinere, gleichmäßiger verteilte Gewichte
        %    - L1-Regularisierung: Fördert Sparsity (weniger relevant für LoRA)
        %    - Dropout [SOURCE-MISSING: Srivastava et al., 2014]:
        %      * Zufälliges Deaktivieren von Neuronen während Training
        %      * Bei LoRA: Nur für lange Trainings-Episoden geeignet (siehe Abschnitt~\ref{sec:2.2.1})
        %      * Verhindert Co-Adaptationen zwischen Neuronen
        %
        % b) Early Stopping [SOURCE-MISSING: Prechelt, 1998]:
        %    - Kontinuierliches Monitoring von Validation Loss während Training
        %    - Training stoppen, wenn Validation Loss nicht mehr sinkt oder ansteigt
        %    - Typische Patience-Strategie: 3-5 Epochen ohne Verbesserung
        %    - Verhindert Überanpassung in späten Trainingsphasen
        %    - Erfordert separates Validation Set (nicht Teil der Trainingsdaten)
        %
        % c) Datenaugmentierung (siehe Abschnitt~\ref{sec:2.2.3}):
        %    - Künstliche Vergrößerung des effektiven Datensatzes
        %    - Reduziert Overfitting durch erhöhte Variabilität
        %    - Methoden: EDA, Back-Translation, Synthetic Data Generation
        %    - Erhöht Robustheit gegenüber Formulierungsvariationen
        %
        % d) Cross-Validation:
        %    - K-Fold Cross-Validation bei sehr kleinen Datensätzen
        %    - Robustere Schätzung der Generalisierungsfähigkeit
        %    - Rechenintensiv, aber aussagekräftiger bei limitierten Daten
        %
        % e) PEFT-spezifische Strategien:
        %    - Niedrigerer LoRA-Rank: Reduziert trainierbare Parameter
        %    - Selektive Target Modules: Nur kritische Layer anpassen
        %    - Trade-off: Geringere Expressivität vs. bessere Generalisierung
        %
        % UNDERFITTING VS. OVERFITTING - BALANCIERUNG:
        % - Underfitting: Modell zu einfach, erfasst Muster nicht (zu niedriger Rank, zu wenig Training)
        % - Overfitting: Modell zu komplex, memoriert Daten (zu hoher Rank, zu viel Training)
        % - Optimaler Arbeitspunkt liegt zwischen beiden Extremen
        % - Empirische Bestimmung durch Validation-Set-Performance notwendig
        % - Bias-Variance Trade-off [SOURCE-MISSING: Geman et al., 1992]
        
        % ============================================================================
        % 2. CATASTROPHIC FORGETTING - VERLUST VORTRAINIERTER FÄHIGKEITEN
        % ============================================================================
        
        % DEFINITION:
        % - Phänomen: Modell verliert während Fine-Tuning Fähigkeiten aus Pretraining-Phase
        % - Spezialisierung auf neue Aufgabe geht zu Lasten allgemeiner Kompetenzen
        % - Graduelle oder abrupte Verschlechterung auf Pretraining-Tasks
        % - Erstmals dokumentiert in neuronalen Netzen [SOURCE-MISSING: McCloskey & Cohen, 1989]
        % - Bei LoRA: Weniger ausgeprägt als bei Full Fine-Tuning, aber vorhanden [SOURCE-MISSING: Luo et al., 2025; Yang et al., 2024]
        %
        % MANIFESTATIONEN:
        % - Wissensverlust: Fakten aus Pretraining-Korpora nicht mehr abrufbar
        % - Sprachkompetenz-Degradation: Verschlechterung grammatikalischer Fähigkeiten
        % - Task Interference: Neue Aufgabe überschreibt Wissen über alte Aufgaben
        % - Safety Alignment Degradation [SOURCE-MISSING: Qi et al., 2023; Hsu et al., 2024]:
        %   * Fine-Tuning kann Safety Guardrails schwächen
        %   * Betrifft alle PEFT-Methoden (LoRA, Adapter, Prefix Tuning)
        %   * Selbst bei benign Training Data möglich (nicht nur adversarial)
        %
        % THEORETISCHE URSACHEN:
        % - Weight Interference: Neue Gewichtsanpassungen überschreiben alte Repräsentationen
        % - Gradient Descent Bias: Optimierung favorisiert aktuelle Task über vorherige
        % - Representation Overlap: Shared Weights müssen beide Tasks gleichzeitig kodieren
        % - Distribution Shift: Trainingsverteilung unterscheidet sich stark von Pretraining-Daten
        %
        % LoRA-spezifische Faktoren:
        % - Zero-Initialisierung der B-Matrix kann Konvergenz verlangsamen [SOURCE-MISSING: Luo et al., 2025]
        % - Low-Rank Updates können wichtige Pretraining-Repräsentationen nicht vollständig bewahren
        % - Trotz eingefrorener Basisgewichte: Adapter-Outputs können Aktivierungen verzerren
        %
        % GEGENMAÄSSNAHMEN:
        %
        % a) Niedrigere Learning Rates [SOURCE-MISSING: Howard & Ruder, 2018]:
        %    - Typisch: 1e-4 bis 5e-5 für LoRA (vs. 1e-3 für Full Fine-Tuning)
        %    - Sanftere Anpassung erhält mehr Pretraining-Wissen
        %    - Trade-off: Langsamere Konvergenz
        %
        % b) LoRA als sanftere Alternative zu Full Fine-Tuning:
        %    - Originalgewichte bleiben eingefroren (siehe Abschnitt~\ref{sec:2.2.1})
        %    - Nur Low-Rank-Adapter werden trainiert
        %    - Reduziert Catastrophic Forgetting im Vergleich zu Full Fine-Tuning
        %    - Aber: Nicht vollständig eliminiert [SOURCE-MISSING: Luo et al., 2025]
        %
        % c) Mixed Task Training:
        %    - Kombination domänenspezifischer und allgemeiner Daten
        %    - Typisches Verhältnis: 85-90% spezifisch, 10-15% allgemein \cite{raffel2020exploring}
        %    - Erhält Generalisierungsfähigkeit während Spezialisierung
        %    - Beispiel: LVB-Verkehrsanweisungen + allgemeine deutsche Texte
        %
        % d) Regularisierungstechniken:
        %    - Elastic Weight Consolidation (EWC) [SOURCE-MISSING: Kirkpatrick et al., 2017]
        %    - Knowledge Distillation vom Pretraining-Modell
        %    - Bei LoRA: Weniger kritisch aufgrund gefrorener Basisgewichte
        %
        % e) Early Stopping basierend auf Generalisierungsmetriken:
        %    - Monitoring nicht nur Task-Accuracy, sondern auch allgemeine Sprachfähigkeiten
        %    - Perplexity auf Out-of-Domain-Daten als Indikator
        %    - Training stoppen bei signifikantem Anstieg
        
        % ============================================================================
        % 3. KONVERGENZ-HERAUSFORDERUNGEN BEI LORA
        % ============================================================================
        
        % LANGSAME KONVERGENZ:
        % - LoRA konvergiert signifikant langsamer als Full Fine-Tuning [SOURCE-MISSING: Wang et al., 2024]
        % - Empirisch dokumentiert: 5-6x mehr Iterationen und FLOPs für gleiche Performance
        % - Paradoxe Situation: Pro-Iteration effizienter, aber Gesamt-Trainingskosten höher
        % - Kann zu schlechterer finaler Test-Performance führen
        % - Nicht nur Geschwindigkeitsproblem, sondern fundamentale Optimierungsschwierigkeit
        %
        % ARCHITEKTONISCHE URSACHEN:
        %
        % a) Zero-Initialisierung der B-Matrix [SOURCE-MISSING: Wang et al., 2024]:
        %    - Bei Training-Start: ΔW = BA = 0 (keine initiale Störung)
        %    - Langsame Trainingsdynamik zwischen A und B in frühen Epochen
        %    - "Kurzsichtige" Inter-Layer-Interaktionen
        %    - Verzögert Entwicklung komplexer Feature-Transformationen
        %
        % b) Low-Rank Constraint [SOURCE-MISSING: Shen et al., 2025; Xia et al., 2024]:
        %    - Gewichtsupdates auf niedrigdimensionalen Unterraum beschränkt
        %    - Kann komplexe Adaptionen nicht vollständig abbilden
        %    - Limitiert Expressivität der Weight Updates
        %    - Performance-Gap zu Full Fine-Tuning bei komplexen Datensätzen [SOURCE-MISSING: Wang et al., 2025]
        %
        % c) Imbalancierte Weight Updates [SOURCE-MISSING: Zhang et al., 2025; Yen et al., 2024]:
        %    - Low-Rank-Faktorisierung ist nicht eindeutig (non-unique)
        %    - Verschiedene A-B-Kombinationen ergeben gleiches ΔW
        %    - Führt zu inkonsistenten und imbalancierten Updates
        %    - Suboptimale Konvergenzpfade im Optimierungsraum
        %
        % RANK-AUSWAHL-DILEMMA - "LOW-RANK BOTTLENECK":
        %
        % - Fundamentales Trade-off [SOURCE-MISSING: Dong et al., 2025; Biderman et al., 2024]:
        %   * Niedriger Rank (r=4-8): 
        %     - Hohe Parameter-Effizienz
        %     - Schnelles Training pro Iteration
        %     - ABER: Signifikanter Performance-Gap zu Full Fine-Tuning
        %     - Expressivität zu gering für komplexe Aufgaben
        %   
        %   * Hoher Rank (r=64-128):
        %     - Bessere Performance, nähert sich Full Fine-Tuning an
        %     - Höhere Expressivität der Weight Updates
        %     - ABER: Parameter-Kosten wachsen linear mit Rank
        %     - Schwächt fundamentalen Vorteil der Parameter-Effizienz
        %
        % - Empirische Beobachtungen:
        %   * Performance verbessert sich monoton mit steigendem Rank [SOURCE-MISSING: Dong et al., 2025]
        %   * Aber: Diminishing Returns ab bestimmtem Rank (task-abhängig)
        %   * Optimaler Rank variiert nach Modellgröße, Task-Komplexität, Datensatzgröße
        %   * Keine universelle Heuristik: Empirische Bestimmung erforderlich [SOURCE-MISSING: Rajabzadeh et al., 2024]
        %
        % - "Low-Rank Bottleneck":
        %   * Narrowing Performance-Gap erfordert Rank-Erhöhung
        %   * Bei sehr hohem Rank: Annäherung an Full Fine-Tuning-Kosten
        %   * Fundamentale Limitation der Low-Rank-Annahme
        %   * Theoretische Grenze der PEFT-Effizienz
        %
        % PERFORMANCE-GAP ZU FULL FINE-TUNING:
        % - Konsistent dokumentiert über verschiedene Benchmarks [SOURCE-MISSING: Tastan et al., 2025; Biderman et al., 2024]
        % - Besonders ausgeprägt bei:
        %   * Komplexen Datensätzen mit diversen Sub-Domänen
        %   * Tasks mit hoher semantischer Variabilität
        %   * Kleinen Modellen (< 7B Parameter) [SOURCE-MISSING: Wang et al., 2025]
        % - Gap verringert sich mit:
        %   * Größeren Basismodellen
        %   * Höherem LoRA-Rank
        %   * Längeren Trainings-Episoden
        %
        % OVERFITTING BEI LORA-TRAINING:
        % - Widersprüchliche Phänomene [SOURCE-MISSING: Mao et al., 2024]:
        %   * Höherer Rank bringt nicht zwingend bessere Performance
        %   * Kann zu Overfitting führen, besonders bei kleinen Datensätzen
        %   * Non-monotones Verhalten: Optimum liegt oft bei mittlerem Rank
        % - Initialization Bottleneck [SOURCE-MISSING: Xue, 2025]:
        %   * Zero-Initialisierung limitiert Aktivierung der Originalgewichte
        %   * Kann optimale Performance-Pfade blockieren
        
        % ============================================================================
        % ZUSAMMENFASSUNG & INTERAKTIONEN
        % ============================================================================
        
        % WECHSELWIRKUNGEN ZWISCHEN HERAUSFORDERUNGEN:
        % - Overfitting und Catastrophic Forgetting:
        %   * Beide profitieren von Regularisierung und Early Stopping
        %   * Gegenläufig: Overfitting will Spezialisierung, CF will Generalisierung
        %   * Balance erforderlich: Weder zu spezialisiert noch zu generisch
        %
        % - Konvergenz und Overfitting:
        %   * Langsame Konvergenz kann Overfitting verzögern (implizite Regularisierung)
        %   * Zu viele Epochen: Langsame Konvergenz führt trotzdem zu Overfitting
        %   * Early Stopping muss beide Faktoren berücksichtigen
        %
        % - Learning Rate als zentraler Hebel:
        %   * Zu hoch: Schnelle Konvergenz, aber Overfitting und CF-Risiko
        %   * Zu niedrig: Langsame Konvergenz, paradoxerweise auch Overfitting möglich
        %   * Optimal: Task- und datensatzabhängig, empirisch zu bestimmen
        %
        % THEORETISCHE PERSPEKTIVE:
        % - Fine-Tuning als Optimierungsproblem mit multiplen Constraints
        % - Keine universelle Lösung: Trade-offs zwischen verschiedenen Zielen
        % - PEFT-Methoden wie LoRA: Verschieben Trade-offs, eliminieren sie nicht
        % - Empirische Validierung unerlässlich für praktische Anwendungen


    \subsection{Ressourceneffizienz und Modelloptimierung}
    \label{sec:2.3}
    
    % Ziel: 4-5 Seiten
    
        \subsubsection{Problematik großer Sprachmodelle}
        \label{sec:2.3.1}
        
        % Ziel: 1,5 Seiten
        % - Energieverbrauch und CO2-Bilanz großer Modelle
        % - Überdimensionierung in der Praxis
        % - Wirtschaftliche und ökologische Implikationen
        % - Notwendigkeit ressourceneffizienter Alternativen
        
        
        \subsubsection{Quantisierung}
        \label{sec:2.3.2}
        
        % Ziel: 2,5-3 Seiten
        % AUSFÜHRLICH, da zentral für die Arbeit:
        % - Grundprinzip der Quantisierung
        % - INT8/INT4-Quantisierung
        % - Theoretische Einsparungen bei Speicher und Rechenleistung
        % - Speicherbedarf (konkrete Zahlen aus Literatur)
        % - Inferenzgeschwindigkeit
        % - Energieverbrauch (theoretische Berechnungen)
        % - Trade-off: Effizienz vs. Qualität
        % - Quantisierung vor vs. nach Fine-Tuning
        % - Verfügbare Open-Source-Tools (llama.cpp, bitsandbytes, GPTQ)
        % - Erwartete praktische Implikationen
        
        
        \subsubsection{Weitere Optimierungsansätze}
        \label{sec:2.3.3}
        
        % Ziel: 1,5-2 Seiten
        % - RAG-Systeme (Retrieval-Augmented Generation)
        %   * Konzept: Dynamischer Retrieval + Generation
        %   * Zwei-Stufen-Prozess (Retrieval aus Vektorstore, dann LLM-Generation)
        %   * Vorteile: Skalierung auf große Wissensbasen, Aktualität
        %   * Anwendungsfälle im Verkehrssektor
        %   * Hardware-Anforderungen und Komplexität
        % - Knowledge-Enhanced Prompting als vereinfachte Alternative
        %   * Statische Einbettung von Domänenwissen in System-Prompts
        %   * Vorteile: Einfachere Implementierung, geringere Hardware-Anforderungen
        %   * Limitierungen: Begrenzte Wissensbasis (Context-Window), keine Dynamik
        %   * Geeignet für überschaubare, stabile Domänen
        % - Modellkompression (Pruning, Destillation - kurz erwähnen)
        % - Vergleich verschiedener Ansätze


    \subsection{Qualitätssicherung bei KI-generierten Texten}
    \label{sec:2.4}
    
    % Ziel: 2-2,5 Seiten
    
        \subsubsection{Evaluationsmetriken für NLP}
        \label{sec:2.4.1}
        
        % Ziel: 1 Seite
        
        % AUTOMATISCHE METRIKEN:
        % - BLEU, ROUGE (kurz): N-gram Overlap-basiert
        %   * Ursprünglich für Maschinenübersetzung entwickelt
        %   * Limitiert für semantische Äquivalenz
        %   * Schnell berechenbar, aber oberflächlich
        %
        % - Semantische Ähnlichkeitsmetriken (BERTScore):
        %   * Contextualized Embeddings statt Surface-Form
        %   * Bessere Erfassung von Paraphrasen
        %   * Rechenintensiver, aber aussagekräftiger
        %
        % - Perplexity:
        %   * Misst Konfidenz des Modells
        %   * Niedriger = besseres Language Modeling
        %   * Nicht direkt für Qualität, aber für Training-Monitoring
        %
        % PROMPT-BASIERTE EVALUATION \cite{berryman2024prompt}:
        % - LLM-as-a-Judge: Nutzung größerer Modelle zur Bewertung
        %   * Prompt: "Bewerte die Qualität dieser Transformation auf Skala 1-5"
        %   * Kriterien: Faktentreue, Verständlichkeit, Stilkonformität
        %   * Vorteil: Flexibel, domänenspezifisch anpassbar
        %   * Limitation: Bias des Evaluator-Modells, Kosten
        %
        % - Comparative Evaluation \cite{berryman2024prompt}:
        %   * A/B-Vergleich zweier Outputs
        %   * "Welcher Text ist verständlicher: A oder B?"
        %   * Robuster als absolute Bewertungen
        %   * Nützlich für Prompt-Optimierung (siehe Abschnitt~\ref{sec:2.2.2})
        %
        % DOMÄNENSPEZIFISCHE METRIKEN:
        % - Faktentreue: Prüfung gegen strukturierte Eingabedaten
        %   * Entitätsextraktion: Liniennummer, Haltestellen, Zeitangaben
        %   * Vollständigkeitsprüfung: Alle Input-Informationen im Output?
        %   * Halluzination Detection: Erfundene Details? (siehe Abschnitt~\ref{sec:2.4.2})
        %
        % - Stilkonformität:
        %   * Einhaltung von Stilrichtlinien (siehe Abschnitt~\ref{sec:3.2})
        %   * Regelbasierte Checks: Passiv vs. Aktiv, Fachbegriffe vs. Allgemeinsprache
        %   * Pattern-Matching für verbotene Formulierungen
        %
        % - Konsistenz:
        %   * Gleiche Eingabe → identische Ausgabe (bei deterministischer Temperatur)
        %   * Messung: Wiederholte Generierung, Hamming-Distanz zwischen Outputs
        %   * Wichtig für Produktionsumgebung: Vorhersagbarkeit
        
        
        \subsubsection{Halluzination Detection und Validierung}
        \label{sec:2.4.2}
        
        % Ziel: 1-1,5 Seiten
        
        % PROBLEMSTELLUNG:
        % - Halluzinationen: Modell generiert faktisch inkorrekte oder erfundene Informationen
        % - Besonders kritisch bei sicherheitsrelevanten Verkehrsinformationen
        % - Herausforderung: Plausibel klingende, aber falsche Aussagen
        % - Beispiel: "Straßenbahn Linie 10" statt korrekter "Bus Linie 10"
        %
        % SELF-CONSISTENCY CHECKS \cite{bonstra2024prompt}:
        % - Grundprinzip: Mehrfache Generierung mit Sampling (siehe Abschnitt~\ref{sec:2.2.2})
        % - Mechanismus:
        %   * Gleiche Eingabe → N verschiedene Outputs generieren (typisch N=3-10)
        %   * Sampling mit Temperature > 0 für Variabilität
        %   * Mehrheitsentscheidung oder Konsensanalyse
        % - Annahme: Korrekte Antworten konvergieren, Halluzinationen divergieren
        % - Vorteil: Keine externe Wissensquelle erforderlich
        % - Limitation: Mehrfache Inferenz-Kosten (N-facher Rechenaufwand)
        % - Anwendung: Kritische Transformationen, wo Faktentreue essentiell ist
        %
        % FACT CHECKING GEGEN EINGABEDATEN:
        % - Strukturierte Validierung \cite{berryman2024prompt}:
        %   * Extraktion von Entitäten aus Input und Output
        %   * Vergleich: Sind alle Input-Entitäten im Output enthalten?
        %   * Regel: Output darf KEINE Entitäten enthalten, die nicht im Input sind
        % - Named Entity Recognition (NER) für Extraktion:
        %   * Liniennummern, Haltestellen, Zeitangaben, Fahrzeugtypen
        %   * Pattern-Matching oder NER-Modell
        % - Constraint-basierte Validierung:
        %   * Whitelist-Ansatz: Nur bekannte Liniennummern erlaubt
        %   * Knowledge Base: Linien-Fahrzeug-Zuordnungen als Ground Truth
        %   * Reject-Output bei Verletzung von Constraints
        %
        % PROMPT-BASIERTE HALLUZINATION PREVENTION \cite{berryman2024prompt}:
        % - Negative Constraints in System Prompt (siehe Abschnitt~\ref{sec:2.2.2}):
        %   * "Erfinde KEINE Details, die nicht in der Eingabe stehen"
        %   * "Nutze NUR Informationen aus dem bereitgestellten Kontext"
        %   * "Bei Unsicherheit: NICHT spekulieren, sondern Information weglassen"
        % - Knowledge Grounding:
        %   * Explizite Einbettung von Fakten im Prompt
        %   * Beispiel: "Linie 10 ist ein Bus, Linie 11 ist eine Straßenbahn"
        %   * Reduziert Halluzinationen durch direkte Verfügbarkeit von Wissen
        % - Chain-of-Thought für Faktentreue:
        %   * Zwischenschritt: "Welche Informationen sind in der Eingabe enthalten?"
        %   * Explizite Verifikation vor Generierung
        %   * Erhöht Token-Kosten, aber verbessert Zuverlässigkeit
        %
        % AUTOMATISIERTE VS. MANUELLE EVALUATION:
        % - Automatisiert (skalierbar, aber limitiert):
        %   * Entitätsvergleich: Schnell, deterministisch
        %   * Pattern-Matching: Regelbasiert, wartungsintensiv
        %   * Self-Consistency: Rechenintensiv, keine externe Referenz
        % - Manuell (präzise, aber teuer):
        %   * Human Evaluation: Gold Standard, aber nicht skalierbar
        %   * Stichprobenbasiert: Regelmäßige Quality Checks
        %   * Iterative Verbesserung: Fehleranalyse → Prompt-Anpassung
        % - Hybrid-Ansatz (optimal für Praxis):
        %   * Automatisierte Pre-Filter: Offensichtliche Fehler abfangen
        %   * Manuelle Review: Grenzfälle und Stichproben
        %   * Feedback-Loop: Erkannte Fehler → Prompt-Optimierung
        %
        % PRAKTISCHE ANSÄTZE FÜR RESSOURCENLIMITIERTE UMGEBUNGEN:
        % - Deterministische Temperatur (T=0) reduziert Halluzinationen \cite{berryman2024prompt}
        %   * Trade-off: Weniger kreativ, aber konsistenter
        %   * Geeignet für faktische Transformationen
        % - Kleinere Validierungsmodelle:
        %   * Leichtgewichtige NER-Modelle für Entitätsextraktion
        %   * Regelbasierte Checks statt LLM-as-a-Judge
        % - Cached Knowledge Integration:
        %   * Statische Wissensbasis im Prompt (kein RAG-Overhead)
        %   * Siehe Abschnitt~\ref{sec:2.3.3} für Knowledge Integration ohne RAG
        %
        % ÜBERGANG ZU KAPITEL 7 (EVALUATION):
        % - Theoretische Konzepte hier: Methodik und Prinzipien
        % - Praktische Anwendung in Kapitel~\ref{chap:7}: Konkrete Metriken und Ergebnisse
        % - Kombination automatisierter und manueller Validierung in Implementierung


