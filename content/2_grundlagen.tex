\section{Theoretische Grundlagen und Stand der Forschung} \label{chap:2}

% Ziel: 16-18 Seiten

% ============================================================================
% AGENT-TODO: FORMULATE TEXTS FROM BULLET POINTS
% ============================================================================
% Instructions for agent task:
% 1. Go through ALL subsections with bullet points (marked with %)
% 2. Formulate complete, scientific paragraphs from these bullet points
% 3. Use ONLY sources already mentioned in bullet points (marked with [SOURCE-MISSING])
% 4. DO NOT add new sources or citations on your own
% 5. Keep all [SOURCE-MISSING] tags in the formulated text where citations belong
% 6. Maintain cross-references (\ref{}) to other chapters
% 7. Keep scientific style
%
% Sections to formulate:
% - Kapitel-Einleitung (lines ~5-33)
% - 2.1 Abschnitt-Einleitung (lines ~36-65)
% - 2.1.2 NLP-Anwendungen im Verkehrssektor (lines ~115-203)
% - 2.1.3 Domänenspezifische Sprachmodelle (lines ~205-366)
% - 2.2.1 Fine-Tuning-Methoden (partial - LoRA, other PEFT methods)
% - 2.2.2 Prompt Engineering (complete)
% - 2.2.3 Datensätze (partial - needs completion)
% - 2.2.4 Overfitting & Catastrophic Forgetting (complete)
% - 2.3.1 Quantisierung (complete)
% - 2.3.2 Halluzination Prevention (complete)
% - 2.3.3 RAG & Knowledge Integration (complete)
%
% Do NOT formulate:
% - Already written paragraphs (e.g., 2.1.1 Transformer-Architektur)
% - Sections with only structural comments
% ============================================================================

Die in Kapitel~\ref{chap:1} vorgestellte Problemstellung der Leipziger Verkehrsbetriebe erfordert fundierte Natural Language Processing-Expertise, da die Transformation von fachsprachlichen Verkehrsanweisungen in allgemeinverständliche Fahrgastinformationen keine triviale Übersetzungsaufgabe darstellt. Vielmehr müssen Kontextabhängigkeit, semantische Präzision und kommunikative Verständlichkeit simultan gewährleistet werden, was moderne Sprachverarbeitungsmethoden unerlässlich macht.

Das vorliegende Kapitel legt die theoretischen Grundlagen für die automatisierte Texttransformation und gliedert sich in drei aufeinander aufbauende Themenkomplexe. Abschnitt~\ref{sec:2.1} behandelt zunächst die NLP-Fundamente, wobei die Transformer-Architektur als paradigmatischer Durchbruch im Bereich der Sprachverarbeitung eingeführt wird. Das Pretraining-Finetuning-Paradigma wird erörtert, bevor deutschsprachige Modelle von Mistral bis LeoLM betrachtet werden, welche die sprachliche Basis für die vorliegende Arbeit bilden.

Abschnitt~\ref{sec:2.2} widmet sich der Anpassungsmethodik vortrainierter Sprachmodelle an domänenspezifische Aufgaben. Parameter-effiziente Fine-Tuning-Verfahren wie LoRA werden vorgestellt, ergänzt durch das Unsloth-Optimierungsframework, das Training unter Ressourcenbeschränkungen ermöglicht. Datenstrategien bei limitierter Datenverfügbarkeit sowie Prompt Engineering als komplementärer Ansatz zur Verhaltenssteuerung von Sprachmodellen runden diesen Abschnitt ab.

Der dritte Themenkomplex in Abschnitt~\ref{sec:2.3} fokussiert auf Produktionsreife und praktische Einsatzfähigkeit. Quantisierung als Technik für das Deployment unter Hardware-Constraints wird erörtert, bevor Mechanismen zur Halluzination Prevention und zur Sicherstellung faktentreuer Ausgaben behandelt werden. Strategien zur Knowledge Integration ohne den Overhead vollständiger Retrieval-Augmented Generation-Systeme schließen diesen Abschnitt.

Der rote Faden dieses Kapitels führt von theoretischen Konzepten zu deren praktischer Anwendbarkeit unter realen Constraints. Während Kapitel~\ref{chap:3} den Anwendungskontext beschreibt, der technologische Entscheidungen determiniert, und Kapitel~\ref{chap:6} die konkrete Implementierung darstellt, fundiert das vorliegende Kapitel die entwickelte Lösung in der etablierten wissenschaftlichen Forschung und schafft das konzeptionelle Fundament für die nachfolgenden praktischen Ausführungen.

    \subsection{Natural Language Processing für Verkehrsinformationen}
    \label{sec:2.1}
    
    % Ziel: 5-6 Seiten
    
Die Auswahl geeigneter Sprachmodelle für die Verarbeitung von Verkehrsinformationen bewegt sich im Spannungsfeld zwischen universellen und domänenspezifischen Ansätzen. Während proprietäre Systeme wie GPT-4 oder Claude beeindruckende Generalfähigkeiten demonstrieren, erfordern Verkehrsdomänen spezifische Terminologie, rechtliche Präzision und lokale Kontextkenntnisse. Zudem machen Anforderungen der Datenschutz-Grundverordnung, Datensouveränität und Kosteneffizienz den Einsatz von Open-Source-Modellen für öffentliche Verkehrsbetriebe unerlässlich.

Die Komplexität von Natural Language Processing im Verkehrssektor wird häufig unterschätzt. Es genügt nicht, Sentiment-Analysen oder FAQ-Bots bereitzustellen; die Transformation fachsprachlicher Anweisungen erfordert gleichzeitig tiefes Kontextverständnis, präzise Stiladaption und absolute Faktentreue. Die Umformulierung einer verkürzten Mitteilung wie „Linie 10 fährt nicht" in eine vollständige Fahrgastinformation „Die Straßenbahn der Linie 10 verkehrt aufgrund technischer Störungen gegenwärtig nicht" erfordert implizites Wissen über Liniennummern, Fahrzeugtypen und gegebenenfalls Alternativrouten, das nicht unmittelbar in der Eingabe kodiert ist.

Die Evolution natürlichsprachlicher Generierungsansätze im Verkehrssektor spiegelt den allgemeinen Fortschritt im Natural Language Processing wider. Template-basierte Systeme waren starr, in ihrem Ausdrucksvermögen limitiert und wartungsintensiv. Regelbasierte Natural Language Generation bot zwar bessere Kontrolle über die Ausgabequalität, stieß jedoch bei der Skalierung auf erhebliche Probleme. Moderne Large Language Models vereinen Flexibilität mit Generalisierungsfähigkeit, stellen jedoch neue Herausforderungen hinsichtlich der Kontrollierbarkeit und Faktentreue dar.

Transformer-basierte Sprachmodelle unterscheiden sich fundamental von früheren Ansätzen und ermöglichen erst die geforderte Qualität automatisierter Texttransformation. Der Attention-Mechanismus erfasst semantische Abhängigkeiten über Satzgrenzen hinweg, während das Pretraining auf Milliarden von Tokens ein robustes allgemeines Sprachverständnis etabliert. Fine-Tuning erlaubt anschließend die Domänenanpassung, ohne dass ein vollständiges Training from-scratch erforderlich wäre, was bei begrenzten Ressourcen und Datenmengen entscheidend ist.

Die folgenden Unterabschnitte strukturieren die theoretischen Grundlagen systematisch: Abschnitt~\ref{sec:2.1.1} behandelt die technischen Fundamente der Transformer-Architektur, das Pretraining-Paradigma und Transfer Learning. Abschnitt~\ref{sec:2.1.2} gibt einen Überblick über verkehrsspezifische NLP-Anwendungen und den aktuellen Stand der Technik. Abschnitt~\ref{sec:2.1.3} fokussiert auf deutschsprachige und domänenspezifische Modelle, insbesondere die LeoLM-Familie, die als Basis für die vorliegende Arbeit dient.
    
        \subsubsection{Grundlagen der Sprachverarbeitung}
        \label{sec:2.1.1}
        
        % Bereits ausgearbeitet - siehe unten
        
        Die automatisierte Verarbeitung und Transformation von Verkehrsinformationen stellt hohe Anforderungen an Natural Language Processing-Systeme. Präzision, Kontextverständnis und die Fähigkeit zur semantischen Umformulierung sind dabei zentrale Anforderungen. Moderne Ansätze der Sprachverarbeitung basieren auf der Transformer-Architektur, die seit ihrer Einführung im Jahr 2017 die Entwicklung von Sprachmodellen maßgeblich geprägt hat. Das in dieser Arbeit verwendete Modell LeoLM-7B baut auf dieser Architektur auf und nutzt deren Vorteile für die Verarbeitung deutschsprachiger Texte.
        
        \paragraph{Transformer-Architektur}
        
        Die Transformer-Architektur wurde 2017 von Vaswani et al. mit dem wegweisenden Paper „Attention is All You Need" eingeführt \cite{vaswani2017attention}. Im Gegensatz zu vorherigen Ansätzen wie Recurrent Neural Networks (RNNs) oder Long Short-Term Memory (LSTM) verzichtet die Transformer-Architektur vollständig auf rekurrente Strukturen und basiert stattdessen auf dem Attention-Mechanismus. Dieser fundamentale Paradigmenwechsel ermöglicht die parallele Verarbeitung von Sequenzen und führt zu deutlich schnelleren Trainingszeiten sowie besserer Skalierbarkeit.
        
        Das Kernprinzip der Transformer-Architektur ist der Self-Attention-Mechanismus \cite{vaswani2017attention}. Dieser ermöglicht es jedem Token in einer Sequenz, auf alle anderen Tokens im Kontext zuzugreifen und deren Relevanz für die eigene Repräsentation zu bewerten. Durch den Einsatz von Multi-Head Attention werden mehrere parallele Attention-Mechanismen verwendet, die unterschiedliche Aspekte der Kontextbeziehungen erfassen können \cite{vaswani2017attention}. Diese Architektur ermöglicht es dem Modell, komplexe syntaktische und semantische Abhängigkeiten auch über große Distanzen im Text hinweg zu modellieren, ohne unter dem Vanishing-Gradient-Problem zu leiden, das RNNs bei langen Sequenzen beeinträchtigt.
        
        Transformer-basierte Modelle lassen sich in drei Hauptkategorien einteilen: Encoder-Only-Modelle wie BERT \cite{devlin2018bert}, die primär für bidirektionale Textklassifikation und Embeddings konzipiert sind, Decoder-Only-Modelle wie die GPT-Serie \cite{radford2018improving, radford2019language}, die für autoregressive Textgenerierung optimiert sind, sowie Encoder-Decoder-Architekturen wie der ursprüngliche Transformer \cite{vaswani2017attention} und T5 \cite{raffel2020exploring}, die vor allem für Übersetzungsaufgaben entwickelt wurden. Für die in dieser Arbeit behandelte Aufgabe der Textgenerierung und -transformation ist die Decoder-Only-Architektur besonders geeignet, da sie speziell für die sequenzielle Erzeugung von Text konzipiert wurde.
        
        \paragraph{Vortrainierte Sprachmodelle}
        
        Ein zentrales Konzept moderner NLP-Systeme ist das Pretraining von Sprachmodellen auf großen, unlabeled Textkorpora. Durch Self-Supervised Learning, bei dem das Modell darauf trainiert wird, das jeweils nächste Token in einer Sequenz vorherzusagen \cite{radford2018improving}, entwickeln diese Modelle ein umfassendes Sprachverständnis inklusive Syntax, Semantik und implizitem Weltwissen. Zu den einflussreichsten vortrainierten Modellen gehören BERT \cite{devlin2018bert} mit seinem bidirektionalen Masked Language Modeling-Ansatz, die GPT-Serie \cite{radford2018improving, radford2019language} für autoregressive Textgenerierung sowie T5 \cite{raffel2020exploring} mit seinem universellen Text-to-Text-Framework.
        
        Für die vorliegende Arbeit ist insbesondere die Mistral-7B-Architektur von Bedeutung, da sie die Grundlage für das verwendete LeoLM-Modell bildet. Mistral 7B \cite{jiang2023mistral} ist ein hocheffizienter Decoder-Only-Transformer mit 7 Milliarden Parametern, der mehrere innovative Architekturmerkmale aufweist. Grouped-Query Attention (GQA) reduziert die Größe des Key-Value-Cache und ermöglicht schnellere Inferenz \cite{jiang2023mistral}. Sliding Window Attention erlaubt die effiziente Verarbeitung langer Kontexte, während der Rolling Buffer Cache die Speichernutzung optimiert \cite{jiang2023mistral}. Mit 7 Milliarden Parametern stellt Mistral einen Sweet Spot zwischen Modellleistung und Ressourceneffizienz dar und übertrifft in Benchmarks viele deutlich größere Modelle \cite{jiang2023mistral}. Als Open-Source-Modell unter Apache 2.0 Lizenz ist es besonders für lokale Ausführung und Fine-Tuning geeignet.
        
        Ein wichtiger Unterschied besteht zwischen Base Models und Instruct Models. Base Models wie Mistral-7B \cite{jiang2023mistral} sind auf reines Language Modeling trainiert \cite{radford2019language} und setzen primär Texte fort, ohne notwendigerweise expliziten Anweisungen zu folgen. Sie sind als Ausgangspunkt für aufgabenspezifisches Fine-Tuning konzipiert. Instruct Models hingegen durchlaufen zusätzlich ein Instruction Tuning \cite{wei2021finetuned}, bei dem sie mittels Supervised Fine-Tuning auf Instruktionsdatensätzen trainiert werden, um gezielt Anweisungen zu befolgen. Optional kann dieser Prozess durch Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training} weiter verfeinert werden. Mistral bietet beide Varianten an: Mistral-7B als Base Model und Mistral-7B-Instruct als instruction-tuned Version \cite{jiang2023mistral}. Der detaillierte Vergleich und die Auswahlbegründung zwischen diesen Varianten erfolgt in Kapitel~\ref{chap:4}.
        
        Für deutschsprachige Anwendungen ist die LeoLM-Familie von besonderer Relevanz. Diese Modelle nutzen Mistral-7B als Basis und durchlaufen ein Continued Pretraining auf deutschen Textkorpora \cite{leolm2023}. Das in dieser Arbeit verwendete Modell leo-mistral-hessianai-7b ist eine Base-Variante, die speziell für die deutsche Sprache optimiert wurde \cite{leolm2023}. Diese Adaption kombiniert die architektonischen Vorteile und die Effizienz von Mistral mit erhöhter Sprachkompetenz im Deutschen und erzielt dadurch bessere Ergebnisse für deutschsprachige Anwendungen als rein englischsprachige Basismodelle. Die Tokenization erfolgt bei Mistral mittels Byte Pair Encoding (BPE) mit einem Vokabular von 32.000 Tokens \cite{jiang2023mistral}, wobei LeoLM einen an die deutsche Morphologie angepassten Tokenizer verwendet \cite{leolm2023}, der besser mit Komposita, Umlauten und anderen sprachspezifischen Besonderheiten umgehen kann.
        
        \paragraph{Transfer Learning}
        
        Das Konzept des Transfer Learning bildet die theoretische Grundlage für die Nutzung vortrainierter Modelle in spezifischen Anwendungsdomänen. Transfer Learning bezeichnet den Wissenstransfer von einer Source Domain, in der das Modell vortrainiert wurde, zu einer Target Domain, für die es angepasst werden soll \cite{pan2010survey}. Dieser Ansatz reduziert den Bedarf an aufgabenspezifischen Trainingsdaten und die erforderliche Trainingszeit erheblich.
        
        Das etablierte Pretrain-Finetune-Paradigma verläuft in zwei Phasen: Zunächst erfolgt das Pretraining auf großen, unlabeled Textkorpora mittels unsupervised Learning \cite{radford2018improving}, wodurch das Modell grundlegendes Sprachverständnis, syntaktische Strukturen, semantische Zusammenhänge und implizites Weltwissen erwirbt. In der zweiten Phase wird das Modell mittels Fine-Tuning auf eine spezifische Aufgabe angepasst \cite{howard2018universal}, wobei supervised Learning mit aufgabenspezifischen Daten zum Einsatz kommt. Im Kontext dieser Arbeit bedeutet dies die Spezialisierung auf die Transformation von LVB-Verkehrsanweisungen.
        
        Die Vorteile von Transfer Learning sind vielfältig: Howard und Ruder zeigten mit ULMFiT, dass durch Transfer Learning mit nur 100 gelabelten Beispielen eine vergleichbare Performance erreicht werden kann wie mit 10.000 Beispielen beim Training from-scratch \cite{howard2018universal}. Vortrainierte Gewichte dienen als optimaler Startpunkt und beschleunigen das Training erheblich. Zudem führt das bereits vorhandene Sprachverständnis zu besserer Generalisierung auf neue Daten \cite{pan2010survey}.
        
        Dennoch bestehen Herausforderungen: Catastrophic Forgetting bezeichnet den Verlust vortrainierter Fähigkeiten während des Fine-Tunings, dem in Abschnitt~\ref{sec:2.2.4} weiter nachgegangen wird. Der Domain Shift zwischen Pretraining-Daten und Zieldomäne \cite{pan2010survey} kann zu Leistungseinbußen führen, insbesondere wenn sich Vokabular oder Sprachstil deutlich unterscheiden. Bei kleinen Datensätzen besteht zudem die Gefahr des Overfittings, was ebenfalls in Abschnitt~\ref{sec:2.2.4} behandelt wird.
        
        Für die vorliegende Arbeit ist besonders relevant, dass LeoLM bereits auf umfangreichen deutschen Textkorpora vortrainiert wurde \cite{leolm2023}, was eine solide Basis für die weitere Spezialisierung bildet. Das Fine-Tuning auf den vergleichsweise kleinen Datensatz der LVB-Verkehrsanweisungen wird durch Transfer Learning erst praktikabel und ermöglicht die notwendige domänenspezifische Anpassung an die fachsprachlichen Anforderungen des Verkehrssektors.
        
        \subsubsection{NLP-Anwendungen im Verkehrssektor}
        \label{sec:2.1.2}
        
        % Ziel: 2-3 Seiten
        
        Der Personentransportsektor produziert massive Mengen an Textdaten, darunter Verkehrsberichte in Echtzeit und historischer Form, technische Wartungsprotokolle, Incident Reports zu Unfällen und Störungen, Fahrgastbeschwerden und Feedback sowie ungefilterte, heterogene Beiträge in sozialen Medien. Das Datenvolumen übersteigt die menschliche Verarbeitungskapazität bei weitem, während sich regelbasierte Ansätze als unzureichend erwiesen haben. Starre Pattern-Matching-Logik skaliert nicht mit der Variabilität natürlicher Sprache, und der Wartungsaufwand wächst exponentiell mit zunehmender Systemkomplexität.

Natural Language Processing entwickelt sich in diesem Kontext zur Enabler-Technologie, die den Wandel von reaktivem zu proaktivem Verkehrsmanagement ermöglicht. Die automatisierte Erkenntnisgewinnung aus heterogenen Datenquellen steigert die operative Effizienz erheblich. Während frühe NLP-Ansätze auf Schlüsselwortsuche und einfache Klassifizierung beschränkt waren, beherrschen moderne Systeme komplexes Kontextverständnis und erfassen semantische Zusammenhänge. Diese Evolution transformiert Natural Language Processing vom reinen Mustererkenner zum Bedeutungsversteher.

Laut [SOURCE-MISSING] lassen sich NLP-Anwendungen im Verkehrssektor in mehrere Hauptkategorien einteilen. Verkehrsvorhersage und -management umfasst Predictive Analytics aus Textdaten, Anomalie-Detektion in Verkehrsberichten sowie textbasierte Ressourcenallokation. Sentimentanalyse von Social-Media-Daten ermöglicht ein Echtzeit-Stimmungsbarometer der Fahrgäste, die Früherkennung von Service-Problemen und unterstützt Crisis Management bei Großstörungen. Natürliche Sprachschnittstellen finden Anwendung in Query-Interfaces für Fahrplandatenbanken, Conversational AI für den Kundenservice sowie in Voice-Assistenten für Fahrzeuge. Verkehrsampel-Steuerung mittels NLP für adaptive Systeme und textbasierte Koordination zwischen Systemen ist für die vorliegende Arbeit weniger relevant.

Für diese Arbeit zentral ist die automatisierte Berichtserstellung und Dokumentenanalyse. Jüngste Forschungsarbeiten dokumentieren erhebliche Fortschritte: Die Zusammenfassung von Unfallberichten [SOURCE-MISSING], die Klassifizierung der Unfall-Schwere aus Textbeschreibungen [SOURCE-MISSING] sowie die generelle Verbesserung textbasierter Verarbeitung durch leistungsstarke Sprachmodelle zeigen das Potenzial dieser Technologien.

Direkt relevant für die vorliegende Arbeit sind öffentliche Verkehrsinformationsdienste. Large Language Models erweisen sich als effektiv in der Analyse und Aufbereitung von Fahrgastanfragen [SOURCE-MISSING]. Die Transformation technischer Informationen in endnutzergerechte Formate unterstützt die zentrale Aufgabe dieser Arbeit: die verständliche Aufbereitung von Verkehrsanweisungen für Fahrgäste unter Wahrung konsistenter und präziser Kommunikation. Mehrsprachige Verarbeitung wird mit steigender Vielseitigkeit von Large Language Models zunehmend möglich [SOURCE-MISSING], stellt jedoch aktuell ein sekundäres Ziel dar, das in dieser Arbeit nicht tiefergehend behandelt wird, jedoch Potenzial für zukünftige Erweiterungen bietet.

Der Fokus dieser Arbeit liegt primär auf automatisierter Berichtserstellung und öffentlichen Informationsdiensten. Der konkrete Anwendungsfall der Transformation von LVB-Verkehrsanweisungen schlägt eine Brücke zwischen technischer Fachsprache und Allgemeinverständlichkeit. Diese Aufgabe ist verwandt mit Dokumentenanalyse, Textzusammenfassung und Stiladaption, unterscheidet sich jedoch von reiner Klassifizierung durch die essenzielle generative Komponente.

Empirische Erfolge in verwandten Domänen stützen die Hypothese, dass Large Language Models für die Transformation von Verkehrsanweisungen geeignet sind. State-of-the-Art-Performance wurde bei Zusammenfassung und Klassifizierung nachgewiesen, die effektive Bearbeitung von Fahrgastanfragen ist dokumentiert, und die automatisierte Verarbeitung von Unfallberichten übertrifft regelbasierte Systeme deutlich.

Dennoch bestehen spezifische Herausforderungen: Domänenspezifisches Vokabular erfordert gezielte Spezialisierung der Modelle. Faktentreue ist bei sicherheitskritischen Informationen unabdingbar, sodass Halluzinationen vermieden werden müssen. Konsistenz in der Ausgabe ist erforderlich, sodass gleiche Eingaben zu identischen Ausgaben führen und ein gewisser Determinismus gewährleistet ist. Mehrsprachigkeit ist technisch möglich, aber ressourcenintensiv und daher für diese Arbeit nicht im Fokus.
        
        
        \subsubsection{Domänenspezifische Sprachmodelle}
        \label{sec:2.1.3}
        
        % Ziel: 1,5-2 Seiten
        
        Während allgemeine Large Language Models wie GPT-4 oder Claude eine beeindruckende Breite an Fähigkeiten demonstrieren, zeigt die Spezialisierung auf die Verkehrsdomäne signifikante Vorteile. Der Trade-off zwischen Generalität und Domänen-Expertise ist dabei zu berücksichtigen.

Domänenspezifisches Fine-Tuning erweist sich als kritischer Erfolgsfaktor [SOURCE-MISSING]. Wie am Beispiel von TrafficSafetyGPT dokumentiert, übertreffen spezialisierte Modelle konsistent allgemeine Modelle in verkehrsspezifischen Aufgaben. Technische Komponenten wie erweiterte Fachterminologie-Datenbanken, Domain-Specific Pre-Training auf Verkehrssicherheits-Korpora und professionelle Ausdrucksfähigkeiten mit technischer Präzision tragen zu diesem Erfolg bei. Die Vorteile manifestieren sich in effizienterer Zuweisung von Rechenressourcen, kürzeren Trainingszeiten bei gleichzeitig höherer Qualität sowie deutlich höherer Informationsabdeckung im Vergleich zu General-Purpose-Modellen.

Die multimodale Integration verschiedener Datenmodalitäten stellt sich als wesentlicher Erfolgsfaktor heraus [SOURCE-MISSING]. Die Kombination von narrativem, unstrukturiertem Text mit strukturierten Unfalldaten in Form von Tabellen und Kategorien sowie Metadaten zu Zeit, Ort und Beteiligten erzielt empirisch nachgewiesene Erfolge. Eine dokumentierte Reduktion der Fehlerrate um 54,2\,\% verdeutlicht den Synergieeffekt: Text liefert Kontext, während strukturierte Daten Präzision gewährleisten. Diese Strategie gilt als die vielversprechendste zur Leistungssteigerung.

Domain-Specific Pre-Training mittels Continued Pretraining auf Verkehrssicherheits-Korpora ermöglicht es Modellen, spezialisierte Terminologie und Kontexte zu erlernen. Signifikante Leistungsverbesserungen sind dokumentiert, analog zum deutschsprachigen Continued Pretraining von LeoLM auf Basis von Mistral.

Die Übertragung dieser Erkenntnisse auf die vorliegende Arbeit erfolgt durch eine stufenweise multimodale Integration verschiedener Optimierungen. Ausgehend vom Base Model LeoLM, das bereits für die deutsche Sprache optimiert ist, erfolgt Fine-Tuning auf domänenspezifischen Verkehrsanweisungen. Knowledge Enhancement integriert strukturierte Linien-Fahrzeug-Zuordnungen, während Prompt Engineering Stilrichtlinien und Beispiele bereitstellt. Das Ziel besteht darin, trotz eines kleinen Datensatzes fehlerfreie Ergebnisse zu erzielen. Die Forschung rechtfertigt diesen Multi-Methoden-Ansatz, da die Kombination verschiedener Techniken den Datenmangel kompensieren kann.

Deutschsprachige Modelle weisen Besonderheiten auf, die für die vorliegende Arbeit relevant sind. Die LeoLM-Familie basiert auf Mistral und durchläuft deutsches Continued Pretraining, was sprachspezifische Optimierung für Komposita und Morphologie ermöglicht. Da der LVB-Anwendungsfall rein deutschsprachig ist, stellt sich die Frage nach dem Vergleich zwischen englischen Modellen mit nachgelagerter Translation und nativen deutschen Modellen. Native Modelle bieten bessere Idiomatik und Präzision, während Translation das Risiko von Artefakten und Stilbrüchen birgt.

Die Herausforderungen deutschsprachiger Verkehrsdaten sind vielfältig [SOURCE-MISSING]. Heterogene Formatvielfalt erschwert die standardisierte Verarbeitung erheblich. Telegrafische Texte sind verkürzt und entbehren Artikel, Tabelleneinträge sind strukturiert und fragmentiert, während Fließtext-Berichte vollständige Sätze enthalten. Diese Heterogenität stellt hohe Anforderungen an die Vorverarbeitung.

Das Cross-Language Transferability-Problem manifestiert sich darin, dass englisch-trainierte Modelle nicht direkt auf deutschsprachige Daten anwendbar sind. Erhebliche Leistungseinbußen ohne deutschsprachiges Training sind dokumentiert. Zwei Lösungswege existieren: separates Training für deutschsprachige Daten, wie es der LeoLM-Ansatz verfolgt, oder mehrsprachige Trainingsdatensätze mit mindestens 20\,\% deutschen Anteilen für vergleichbare Performance. Diese Erkenntnisse begründen die Wahl deutschsprachiger Basismodelle für die vorliegende Arbeit.

Die Fehleranfälligkeit generischer Modelle resultiert aus zwei Hauptproblemen. Primär englischsprachiges Pretraining führt dazu, dass Modelle zwar übersetzen können, die Fehlerrate jedoch deutlich steigt. Idiomatische Fehler und ungünstige Wortwahl sind häufige Konsequenzen. Generisches Training ohne Verkehrsdomäne hat fehlende Fachterminologie und mangelndes Kontextverständnis für verkehrsspezifische Abkürzungen zur Folge. Der einzige verfügbare deutsche Verkehrsdatensatz von [SOURCE-MISSING] ist veraltet und limitiert, was eigenständiges Fine-Tuning unumgänglich macht.

Die Überleitung zu Kapitel~\ref{chap:4} verdeutlicht, dass domänenspezifische Modelle theoretisch überlegen sind, deutsche Verkehrsmodelle in der Praxis jedoch fehlen. Die Lösung besteht in der Kombination eines deutschen Basismodells mit verkehrsspezifischem Fine-Tuning. Die detaillierte Auswahlbegründung erfolgt im Modellauswahl-Kapitel.

Existierende verkehrsspezifische Modelle sind für die vorliegende Arbeit nicht geeignet. TrafficSafetyGPT [SOURCE-MISSING] basiert auf LLaMA von Meta AI und verwendet den TrafficSafety-2k-Datensatz, der behördliche Richtlinien und ChatGPT-generierte Daten umfasst. Der Fokus liegt auf Verkehrssicherheit (Traffic Safety), und die Vorteile bestehen in effizienter Ressourcennutzung und kurzen Trainingszeiten. Dennoch ist das Modell für LVB nicht geeignet, da es englischsprachig ist und keine deutsche Sprachkompetenz aufweist, der Safety-Fokus nicht der Fahrgastinformation entspricht und die Closed-Source-Basis DSGVO-Probleme für lokales Deployment verursacht.

Die TransGPT-Familie [SOURCE-MISSING] umfasst TransGPT-SM als unimodales Text-Modell basierend auf ChatGLM2-6B sowie TransGPT-MM als multimodales Modell mit Text- und Vision-Komponenten. Das Training erfolgte mit Verkehrsunterlagen, Büchern und Berichten für Anwendungen in Dokumentenanalyse und Führerscheinprüfungen. Das Modell ist für LVB nicht geeignet, da es chinesischsprachig ist (ChatGLM2 ist ein chinesisches Modell), der Fokus auf Wissensvermittlung statt Texttransformation liegt und keine Open-Source-Verfügbarkeit für lokales Fine-Tuning besteht.

Anwendungsspezifische Chatbots wie TP-GPT für Verkehrsüberwachung und Echtzeit-Daten [SOURCE-MISSING], TrafficGPT mit Integration von Traffic Foundation Models und Aufgaben-Dekomposition [SOURCE-MISSING] sowie ChatSUMO für SUMO-Simulator-Integration und Szenario-Generierung [SOURCE-MISSING] sind ebenfalls nicht für LVB geeignet. Diese Frameworks sind aufgabenspezifisch und nicht General-Purpose, erfordern Echtzeit-Datenbanken mit erheblichem Infrastruktur-Overhead, fokussieren auf Query-Beantwortung statt Texttransformation und sind englischsprachig.

Multimodale und sicherheitskritische Modelle wie ChatScene für AV-Sicherheitsszenarien [SOURCE-MISSING], AccidentGPT für Kollisionsvermeidung [SOURCE-MISSING], IDM-GPT mit fünf spezialisierten Agenten für Verkehrsanalysen [SOURCE-MISSING] sowie STEP-LLM für räumlich-zeitliche Verkehrsvorhersage [SOURCE-MISSING] sind nicht für LVB geeignet. Der Fokus liegt auf autonomen Fahrzeugen und Predictive Analytics, die Aufgabe besteht nicht in Textgenerierung, sondern in Szenario-Simulation, deutschsprachige Varianten fehlen, und die Modelle sind zu spezialisiert für allgemeine Fahrgastinformation.

Zusammenfassend ist keines dieser Modelle aus mehreren Gründen geeignet: Die Sprachbarriere, da alle Modelle englisch- oder chinesischsprachig sind; der Aufgaben-Mismatch, da Safety, Prediction und Simulation nicht der Text-Transformation entsprechen; Lizenzierungsprobleme, da die meisten Modelle proprietär sind oder auf Closed-Source-Basis beruhen; Infrastruktur-Anforderungen für Echtzeit-Datenbanken und Multi-Agenten-Systeme sowie Deployment-Einschränkungen, da die Modelle nicht für lokale, offline-fähige Ausführung konzipiert sind.

Die Konsequenz für die vorliegende Arbeit besteht darin, dass keine Off-the-Shelf-Lösung verfügbar ist. Dies macht eine eigenständige Modellentwicklung notwendig. Die Strategie besteht in der Verwendung eines deutschen Basismodells (LeoLM) mit domänenspezifischem Fine-Tuning. Dies füllt eine Forschungslücke bei deutschsprachiger Verkehrsinformation, wobei die detaillierte Begründung in Kapitel~\ref{chap:4} erfolgt.

Für kleine Datensätze ergeben sich aus der Forschung bewährte Strategien. Die Herausforderung besteht darin, dass der LVB-Datensatz vergleichsweise klein ist. Strategien aus der Forschung umfassen Transfer Learning von vortrainierten Modellen, multimodale Integration von Text und Strukturdaten, Data Augmentation (siehe Abschnitt~\ref{sec:2.2.3}) sowie parameter-effizientes Fine-Tuning mittels LoRA (siehe Abschnitt~\ref{sec:2.2.1}). Die Kombination dieser Methoden hat sich nachweislich als erfolgreich erwiesen.

Die Überleitung zu Abschnitt~\ref{sec:2.2} verdeutlicht, dass die theoretische Überlegenheit domänenspezifischer Modelle etabliert ist. Der nächste Schritt besteht darin zu klären, wie solche Modelle erstellt werden. Fine-Tuning-Methoden, Datenstrategien und Optimierungen stehen im Fokus des folgenden Abschnitts, der die Perspektive von „Was funktioniert?" zu „Wie macht man es?" verschiebt.


    \subsection{Sprachmodelle und Fine-Tuning}
    \label{sec:2.2}
    
    % Ziel: 4-5 Seiten
    
Vortrainierte Sprachmodelle verfügen über umfassendes allgemeines Sprachverständnis, das sie während des Pretrainings auf großen Textkorpora erworben haben. Die Spezialisierung auf domänenspezifische Aufgaben erfordert jedoch gezielte Anpassung. LeoLM besitzt deutschsprachige Kompetenz, deckt die Verkehrsdomäne jedoch nicht ab. Der Transfer von allgemeinem zu spezialisiertem Wissen stellt folglich eine zentrale Herausforderung dar.

Full Fine-Tuning, bei dem sämtliche Modellparameter angepasst werden, ist bei modernen Large Language Models mit Milliarden von Parametern ressourcenintensiv. Bei einem 7-Milliarden-Parameter-Modell wie Mistral-7B müssen alle Parameter aktualisiert werden, was Training auf Multi-GPU-Clustern über mehrere Wochen erfordert. Der Speicherbedarf übersteigt 100\,GB VRAM für Gradientenberechnung und Optimizer-States, während die Kostenstruktur für Cloud-Computing-Ressourcen in den vier- bis fünfstelligen Bereich reicht. Die Constraints dieser Arbeit beschränken sich auf Consumer-Hardware mit begrenzten Ressourcen, sodass parameter-effiziente Fine-Tuning-Methoden erforderlich werden.

Die Herausforderung begrenzter Trainingsdaten verschärft die Situation. Der LVB-Datensatz umfasst eine geschätzte Größenordnung von 100 bis 500 Trainingsbeispielen. Im Vergleich dazu verfügt ImageNet über mehr als eine Million annotierte Bilder, während LAION 5 Milliarden Text-Bild-Paare enthält. Das Risiko des Overfittings bei vollständiger Parameteranpassung mit einem kleinen Datensatz ist erheblich. Parameter-effiziente Verfahren bieten den Vorteil eines reduzierten Datenbedarfs durch gezielte Adaption.

Methodisch kommen verschiedene Ansätze zum Einsatz. Parameter-effiziente Fine-Tuning-Verfahren reduzieren die Anzahl trainierbarer Parameter erheblich. Prompt Engineering dient als komplementärer Ansatz zur Leistungssteigerung. Datenaufbereitung und Augmentation kompensieren limitierte Datenverfügbarkeit. Gegenmaßnahmen zu Overfitting und Catastrophic Forgetting sichern die Qualität des trainierten Modells.

Der Bezug zur praktischen Implementierung verdeutlicht, dass die theoretischen Konzepte dieses Abschnitts direkte Anwendung in der Implementierung finden. Eine fundierte Entscheidungsfindung basiert auf dem Verständnis methodischer Trade-offs. Diese wissenschaftlich fundierte Vorgehensweise ersetzt empirisches Trial-and-Error durch systematische Planung.
    
        \subsubsection{Fine-Tuning-Methoden}
        \label{sec:2.2.1}
        
        % Ziel: 2,5-3 Seiten
        
        Die Anpassung großer vortrainierter Sprachmodelle an spezifische Downstream-Aufgaben stellt erhebliche Herausforderungen dar [SOURCE-MISSING]. Das Training aller Modellparameter für jede neue Aufgabe wird mit wachsenden Modellgrößen zunehmend unpraktikabel, da Rechenressourcen und Speicherbedarf exponentiell steigen [SOURCE-MISSING]. Bei einem 7-Milliarden-Parameter-Modell müssen sämtliche Parameter angepasst werden, was über 100\,GB VRAM für Gradienten und Optimizer-States erfordert. Multi-GPU-Cluster über mehrere Wochen hinweg sind notwendig, und die Kostenstruktur bewegt sich im vier- bis fünfstelligen Bereich für Cloud-Computing. Diese Situation macht parameter-effiziente Transferlernmethoden unerlässlich.

Parameter-effiziente Fine-Tuning-Verfahren (PEFT) lassen sich in drei Hauptkategorien einteilen [SOURCE-MISSING]. Additive Fine-Tuning-Ansätze fügen zusätzliche trainierbare Module zum gefrorenen Basismodell hinzu, wobei die Originalgewichte unverändert bleiben. Beispiele hierfür sind Adapter, Prefix Tuning und Prompt Tuning. Der Trade-off besteht zwischen geringer Parameteranzahl und potenzieller Inferenz-Latenz. Reparameterized Fine-Tuning zerlegt Gewichtsupdates in niedrigrangige Matrizen und modifiziert indirekt die Originalgewichte. Der Hauptvertreter dieser Kategorie ist LoRA (Low-Rank Adaptation), das durch Weight Merging keine Inferenz-Latenz verursacht. Selective Fine-Tuning trainiert nur ausgewählte Teilmengen existierender Parameter und friert den Großteil des Modells ein, wird jedoch weniger verbreitet eingesetzt und findet in dieser Arbeit keine Verwendung.

\paragraph{LoRA (Low-Rank Adaptation)}

LoRA basiert auf der theoretischen Hypothese [SOURCE-MISSING], dass Gewichtsänderungen während der Adaptation einen niedrigen intrinsischen Rang aufweisen. Die Gewichtsmatrix-Änderung $\Delta W$ lässt sich effizient durch Low-Rank-Dekomposition approximieren, was eine indirekte Optimierung dichter Layer via Rank-Decomposition ermöglicht. Gewichtsupdates werden als niedrigrangige Dekomposition $\Delta W = BA$ dargestellt, wobei $W_0$ die eingefrorenen Originalgewichte bezeichnet und $W = W_0 + \Delta W = W_0 + BA$ gilt. Die Matrizen $B \in \mathbb{R}^{d \times r}$ und $A \in \mathbb{R}^{r \times k}$ erfüllen $r \ll d, k$, wobei der Rang $r$ als Hyperparameter die Expressivität gegenüber der Effizienz kontrolliert. Direkte Änderungen an vortrainierten Gewichten finden nicht statt; stattdessen werden Low-Rank-Matrizen separat trainiert [SOURCE-MISSING]. Der Bedarf an Hyperparameter-Retuning wird bei Variation des Rangs minimiert [SOURCE-MISSING].

Die Effizienzgewinne von LoRA sind erheblich. Die Parameter-Reduktion erreicht bis zu 10.000-fache Reduktion trainierbarer Parameter bei vergleichbarer Performance zu Full Fine-Tuning [SOURCE-MISSING]. Typischerweise sind weniger als 1\,\% der Gesamtparameter trainierbar. Speichereffizienz manifestiert sich in 3-fach geringerem GPU-Memory-Bedarf als traditionelles Fine-Tuning [SOURCE-MISSING], wobei empirisch eine 35\,\%-ige Reduktion der Memory Usage bei GPT-2 dokumentiert ist [SOURCE-MISSING]. Dies prädestiniert LoRA für ressourcenbeschränkte Umgebungen und ermöglicht Training auf Consumer-Hardware. Bei Verwendung des Adam-Optimizers reduziert sich der VRAM-Bedarf um bis zu zwei Drittel [SOURCE-MISSING].

Die Trainingszeit-Reduktion beträgt 30\,\% im Vergleich zu Full Fine-Tuning [SOURCE-MISSING]. Benchmarks auf GPT-2 mit Tesla T4 zeigen 5,1 Minuten für LoRA gegenüber 7,4 Minuten für Full Fine-Tuning. Dieser Effizienzgewinn besteht trotz erhöhter Iterationszahl aufgrund der Konvergenzproblematik.

Deployment-Vorteile ergeben sich aus der modularen Architektur [SOURCE-MISSING]. Ein Basismodell kann mit vielen Task-Adaptern kombiniert werden, wobei nur Adapter-Parameter pro Aufgabe gespeichert werden müssen [SOURCE-MISSING]. Dies löst das „One-Domain-One-Model"-Problem. Kostengünstiges Task-Switching erfordert lediglich den Austausch der LoRA-Gewichte statt aller Parameter, was Storage- und Switching-Overhead im Multi-Task-Deployment reduziert [SOURCE-MISSING].

Bezüglich der Inferenz-Eigenschaften verursacht LoRA keinen zusätzlichen Overhead [SOURCE-MISSING]. Weight Merging nach dem Training integriert die Adapter in die Originalgewichte: $W_{\text{final}} = W_0 + BA$ [SOURCE-MISSING]. Diese Post-Training-Integration in Originalgewichte stellt einen entscheidenden Vorteil gegenüber Adapter- und Prompt-Methoden dar.

Limitierungen und Herausforderungen bestehen dennoch. Die Konvergenzproblematik zeigt sich in signifikant langsamerer Konvergenz als Full Fine-Tuning [SOURCE-MISSING]. Empirisch werden 5 bis 6-fach mehr Iterationen und FLOPs für gleiche Performance benötigt, was die Gesamt-Trainingskosten trotz Pro-Iteration-Effizienz erhöht und zu schlechterer Testperformance führen kann.

Architektonische Ursachen liegen in der Initialisierung von B mit Nullen [SOURCE-MISSING], was langsame Trainingsdynamik zwischen A und B in frühen Epochen verursacht. Der Skalierungsfaktor erzeugt kurzsichtige Inter-Layer-Interaktionen. Dropout erweist sich nur für lange Trainings-Episoden als geeignet.

Weitere Training-Herausforderungen umfassen potenzielle Catastrophic Forgetting-Probleme, Beeinträchtigung von Weltwissen in vortrainierten Modellen sowie Degradierung von Safety Alignment in fein-justierten Modellen. Die Rank-Sensitivität manifestiert sich darin, dass der Fixed-Rank-Constraint Flexibilität limitiert [SOURCE-MISSING]. Hohe Sensitivität gegenüber Rank-Auswahl erfordert, dass der optimale Rang im Voraus identifiziert werden muss, wobei suboptimale Wahl kostspieliges Retraining nach sich zieht.

Skalierungs-Limitierungen zeigen sich in sinkender Performance bei kleineren Modellen mit weniger als 7 Milliarden Parametern, besonders in Vision-Language-Pre-Training-Kontexten. Der Performance-Gap zu Full Fine-Tuning wächst bei komplexen Datensätzen [SOURCE-MISSING], wobei diverse Sub-Domänen und Task-Typen das Problem verschärfen. Der Gap wächst mit zunehmender Szenario-Komplexität.

Batching-Limitierungen entstehen durch die Herausforderung [SOURCE-MISSING], Inputs verschiedener Tasks mit unterschiedlichen A/B-Matrizen in einem single Forward Pass zu batchen. Dies reduziert Parallelisierungspotenzial bei Multi-Task-Inferenz und beeinträchtigt den Durchsatz in produktiven Multi-Tenant-Systemen.

QLoRA kombiniert LoRA mit Quantisierung [SOURCE-MISSING] und stellt eine optimierte Implementation dar. LoRA-Adapter werden mit 4-bit Quantisierung der Basisgewichte kombiniert, was weitere Speicherreduktion von etwa 70\,\% ohne Qualitätsverlust ermöglicht. Dies erlaubt 7-Milliarden-Parameter-Modelle auf GPUs mit weniger als 24\,GB VRAM. Die Vertiefung erfolgt in Abschnitt~\ref{sec:2.3.2}.

Konfigurationsparameter umfassen den Rank $r$ als Dimensionalität der Low-Rank-Matrizen, typischerweise 4, 8, 16, 32 oder 64, mit Trade-off zwischen Expressivität und Speicherbedarf. Alpha fungiert als Skalierungsfaktor für LoRA-Updates und beeinflusst die Lernrate der Adapter-Gewichte. Target Modules bestimmen, welche Transformer-Layer angepasst werden, darunter Query, Key, Value und Output Projections sowie bei Mistral-Architektur Gate, Up und Down Projections.
        
        \paragraph{Adapter}

Adapter gehören zu den frühesten PEFT-Frameworks [SOURCE-MISSING] und fügen kleine aufgabenspezifische Module mit Feedforward-Layers hinzu. Skip-Connections integrieren den Adapter-Output, während die Originalgewichte eingefroren bleiben.

Die Architektur folgt einem Bottleneck-Design mit Down-Projektion, Aktivierung und Up-Projektion. Die Einfügung erfolgt zwischen Transformer-Schichten, wobei modularer Austausch für verschiedene Tasks möglich ist.

Bezüglich der Effizienz fügen Adapter nur 3,6\,\% zusätzliche Parameter pro Task hinzu [SOURCE-MISSING], während die Performance innerhalb von 0,4\,\% des Full Fine-Tunings liegt. Dies ermöglicht kompakte, erweiterbare Modelle. Die Parameteranzahl beträgt $O(r(d_{\text{in}} + d_{\text{out}}))$ pro Layer [SOURCE-MISSING], was weniger effizient ist als LoRAs Low-Rank-Dekomposition.

Beim Deployment profitieren Adapter von hohem Parameter-Sharing durch das gefrorene Basismodell [SOURCE-MISSING]. Neue Tasks können ohne Revisitation vorheriger hinzugefügt werden, wobei task-spezifische Adapter-Parameter gespeichert werden müssen.

Die Limitierung besteht in Inferenz-Latenz, da die Modelltiefe durch zusätzliche Layer erhöht wird [SOURCE-MISSING]. Dies verursacht zusätzliche Latenz während der Inferenz und macht Adapter weniger geeignet für latenz-sensitive Anwendungen [SOURCE-MISSING]. Der strukturelle Nachteil gegenüber LoRA besteht darin, dass Weight Merging nicht möglich ist.

\paragraph{Prefix Tuning}

Prefix Tuning optimiert kontinuierliche aufgabenspezifische Vektoren [SOURCE-MISSING]. Prefix-Vektoren werden vor Input-Embeddings platziert, sodass nachfolgende Tokens auf Prefixes attenden können. Dies modifiziert Attention-Keys und -Values.

Die Effizienz zeigt sich darin, dass nur 0,1\,\% der Originalparameter gelernt werden [SOURCE-MISSING]. Dies stellt die extremste Parameter-Effizienz aller PEFT-Methoden dar, wobei vergleichbare Performance bei ausreichender Modellgröße erreicht wird. Input-Repräsentationen werden modifiziert statt Modellgewichte.

Stärken zeigen sich in spezifischen Kontexten, insbesondere bei multilingualer Adaptation [SOURCE-MISSING]. Prefix Tuning ist LoRA in mehrsprachigen Tasks überlegen, mit bis zu 28\,\% höherer Accuracy auf XNLI, 13\,\% höherer F1 auf XQUAD und 18\,\% höherer Accuracy auf Belebele im Vergleich zu Base Models. Zusätzliche 4 bis 6\,\% Verbesserung gegenüber LoRA sind dokumentiert.

Limitierungen bestehen in der Reduktion der effektiven Sequenzlänge durch Prefix-Tokens sowie Inferenz-Overhead durch zusätzliche Kontext-Tokens [SOURCE-MISSING]. Dies erhöht den Computational Cost während der Inferenz. Sensitivität gegenüber Initialisierung [SOURCE-MISSING] und challenging Training bei reduzierter verfügbarer Sequenzlänge [SOURCE-MISSING] kommen hinzu.

\paragraph{Prompt Tuning}

Prompt Tuning verwendet lernbare „Soft Prompts" [SOURCE-MISSING]. Dies sind kontinuierliche Embeddings statt diskreter Tokens, die via Backpropagation trainiert werden. Nur Prompt-Embeddings sind trainierbar.

Das Skalierungsverhalten zeigt, dass Prompt Tuning mit steigender Modellgröße kompetitiver wird [SOURCE-MISSING]. Ab mehreren Milliarden Parametern matcht es Full Fine-Tuning. Bei kleineren Modellen mit weniger als 11 Milliarden Parametern zeigt sich geringere Performance.

Die Effizienz manifestiert sich in extremer Parameter-Effizienz durch diskrete Token-Level-Optimierung [SOURCE-MISSING] mit minimalen Speicheranforderungen. Input-Repräsentationen werden modifiziert wie bei Prefix Tuning.

Limitierungen umfassen Inferenz-Overhead durch Prepending task-spezifischer Tokens [SOURCE-MISSING], zusätzlichen Computational Cost durch Prefix-Tokens sowie erhöhte Latenz gegenüber gefrorenen Backbone-Modellen [SOURCE-MISSING]. Sensitive Initialisierung [SOURCE-MISSING] stellt eine weitere Herausforderung dar.

\paragraph{Vergleichende Bewertung}

Die Parameter-Effizienz ordnet sich aufsteigend wie folgt: Prompt Tuning mit circa 0,01\,\% stellt die extremste Reduktion dar, Prefix Tuning mit circa 0,1\,\% ist sehr gering, LoRA mit 0,1 bis 1\,\% je nach Rank ist stark reduziert, während Adapter mit circa 3,6\,\% moderat reduziert sind.

Bezüglich Inferenz-Overhead in aufsteigender Reihenfolge: LoRA verursacht annähernd null Overhead durch Weight Merging und ist optimal. Prefix Tuning und Prompt Tuning verursachen geringen Overhead durch zusätzliche Tokens. Adapter verursachen hohen Overhead durch zusätzliche Layer und sind problematisch.

Die Performance-Charakteristika zeigen für LoRA [SOURCE-MISSING] Performance auf gleichem Niveau oder besser als Full Fine-Tuning bei RoBERTa, DeBERTa, GPT-2 und GPT-3. LoRA übertrifft Prompt Tuning in Gesamtperformance, Memory und Flexibilität, wobei ein Performance-Gap bei komplexen, heterogenen Datensätzen besteht [SOURCE-MISSING].

Adapter [SOURCE-MISSING] zeigen konsistent starke Performance auf Benchmarks, wobei GLUE innerhalb von 0,4\,\% des Full Fine-Tunings liegt. Zuverlässige Performance über verschiedene Tasks hinweg ist dokumentiert.

Prefix Tuning [SOURCE-MISSING] ist superior in multilingualen Adaptation-Tasks und besser in zeit-beschränkten Szenarien mit 3 bis 5 Minuten Training. Es outperformt LoRA in spezifischen Kontexten.

Prompt Tuning zeigt sich schwächer bei kleinen Modellen, ist jedoch konkurrenzfähig ab mehr als 11 Milliarden Parametern.

Die Training-Dynamik zeigt für LoRA langsame Konvergenz mit 5 bis 6-fach mehr Iterationen [SOURCE-MISSING]. Prompt und Prefix Tuning sind initialisierungs-sensitiv [SOURCE-MISSING]. Adapter zeigen stabile Konvergenz, sind aber speicher-intensiv.

Die Wahl von LoRA für diese Arbeit begründet sich durch mehrere Faktoren. Kein Inferenz-Overhead ist kritisch für die Produktionsumgebung. Gute Balance zwischen Parameter-Effizienz und Performance wird erreicht. Modularität ermöglicht mehrere Task-Adapter. Hardware-Feasibility durch Kombination mit QLoRA für Consumer-GPUs ist gegeben. Bewährte Performance in ähnlichen Domänen-Adaptionen ist dokumentiert. Community-Support bietet umfangreiche Implementierungen und Best Practices.

Die Konvergenz-Limitierung ist akzeptabel, da der kleine Datensatz kurze Trainings-Episoden weniger kritisch macht. Unsloth-Optimierungen kompensieren teilweise, und Quality over Speed macht finale Performance wichtiger als Trainingszeit.
        
        \paragraph{Unsloth -- Optimierungsframework für effizientes Training}

Unsloth stellt ein Optimierungsframework für effizientes Training dar, das spezifische Problemstellungen adressiert. Auch PEFT-Methoden wie LoRA erfordern bei 7-Milliarden-Parameter-Modellen erhebliche Ressourcen. Traditionelle Frameworks wie Hugging Face Transformers und PyTorch stoßen an Hardware-Limitierungen. Instabilitäten bei LoRA-Training manifestieren sich in Gradient Explosion und numerischer Instabilität [SOURCE-MISSING]. Die Zugangshürde besteht in der Notwendigkeit von Enterprise-Grade GPUs oder Multi-GPU-Setups. Die Relevanz für diese Arbeit ergibt sich aus Consumer-Hardware-Constraints, wie in Abschnitt~\ref{sec:6.1} dargestellt.

Die technischen Optimierungen umfassen mehrere Komponenten. Speichereffizienz mit 50 bis 80\,\% Reduktion des VRAM-Bedarfs [SOURCE-MISSING] wird durch 4-bit Quantization der Base-Model-Weights mit circa 70\,\% Memory-Reduktion erreicht [SOURCE-MISSING]. Die optimierte QLoRA-Implementation kombiniert LoRA-Adapter mit 4-bit Quantization [SOURCE-MISSING]. Gradient Checkpointing reduziert die Aktivierungs-Speicherung. Dies ermöglicht 7-Milliarden-Parameter-Modelle auf Consumer-GPUs wie RTX 3090 und Tesla T4 [SOURCE-MISSING]. Praktisch bedeutet dies Training ohne Qualitätsverlust bei drastisch reduziertem VRAM.

Training-Beschleunigung um das 2 bis 5-fache [SOURCE-MISSING] wird durch Flash Attention 2 für IO-optimierte Attention-Berechnung [SOURCE-MISSING] erreicht. Dies reduziert Transfers zwischen GPU-Memory-Komponenten [SOURCE-MISSING] und ermöglicht effizientere Key-Value-Query-Matrix-Operationen. Custom Kernel Implementations mittels OpenAI Triton [SOURCE-MISSING] nutzen Fused Operations mit weniger Speicherzugriffen, optimierte Matrix-Multiplikationen für LoRA sowie Custom PyTorch Autograd-Funktionen. Praktisch bedeutet dies Training-Epochen in 26 Minuten statt Stunden [SOURCE-MISSING].

Numerische Stabilität wird durch Gradient Clipping gegen Gradient Explosion [SOURCE-MISSING], Layer Normalization Calibration für LoRA-spezifische Instabilitäten [SOURCE-MISSING] sowie Multi-Precision Support für 8-bit und bfloat16 zur Hardware-Flexibilität [SOURCE-MISSING] gewährleistet. Dies verhindert Training-Abbrüche durch Overflow.

Die PEFT-Integration unterstützt LoRA-Target-Modules wie q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj und down\_proj [SOURCE-MISSING]. Vollständige Kompatibilität mit Hugging Face PEFT sowie nahtlose Integration in bestehende Workflows ist gegeben.

Demokratisierung und Zugänglichkeit manifestieren sich in Hardware-Accessibility durch Single-GPU statt Multi-GPU [SOURCE-MISSING]. Consumer-Grade Hardware wie RTX 3090, NVIDIA A40 und Tesla T4 [SOURCE-MISSING] sowie Free Cloud Platforms wie Google Colab mit T4 und TPU sowie Kaggle mit P100 [SOURCE-MISSING] werden unterstützt. Strukturierte Templates in Form von Guided Notebooks senken die Einstiegshürde [SOURCE-MISSING]. Die Relevanz besteht darin, dass Forschung ohne Enterprise-Budget ermöglicht wird [SOURCE-MISSING].

Synergieeffekte entstehen durch die Kombination mehrerer Ansätze. LoRA reduziert trainierbare Parameter und damit den Rechenaufwand. Unsloth optimiert verbleibende Operationen für schnellere Ausführung. QLoRA kombiniert mit Unsloth Memory-Effizienz mit Speed-Optimierung [SOURCE-MISSING], ohne Qualitätseinbußen gegenüber Standard-Fine-Tuning.

Die Abgrenzung zu anderen Frameworks besteht darin, dass DeepSpeed für Multi-GPU-Verteiltes Training konzipiert ist, während Unsloth auf Single-GPU-Optimierung fokussiert. Unsloth ist komplementär zu PEFT und kein alternatives Verfahren. Der Fokus liegt auf Accessibility statt Scale.

Praktische Implikationen für diese Arbeit umfassen höhere Iterationsgeschwindigkeit, die mehr Experimente in gleicher Zeit ermöglicht. Hardware-Feasibility macht Training auf verfügbarer Hardware möglich. Energieeffizienz durch reduzierte Trainingszeit führt zu geringerem Energieverbrauch. Stabilität gewährleistet zuverlässigere Training-Runs in Constrained Environments [SOURCE-MISSING]. Die konkrete Anwendung, Konfiguration und Trainings-Logs werden in Abschnitt~\ref{sec:6.2.1} dargestellt.
        
        
        \subsubsection{Prompt Engineering}
        \label{sec:2.2.2}
        
        % Ziel: 1,5-2 Seiten
        
        Während Fine-Tuning in Abschnitt~\ref{sec:2.2.1} permanente Modellanpassung durch Parameteränderung bewirkt, ermöglicht Prompt Engineering Verhaltenssteuerung ohne Gewichtsänderungen. Der Synergieeffekt besteht darin, dass Fine-Tuning domänenspezifische Fähigkeiten vermittelt, während Prompts aufgabenspezifische Kontrolle ermöglichen. Die Relevanz für diese Arbeit manifestiert sich in Stilrichtlinien, Formatvorgaben und Konsistenzanforderungen.

Zero-Shot Prompting liefert nur Instruktion ohne Beispiele. Das Modell erhält eine Aufgabenbeschreibung ohne Demonstrationen. Dies funktioniert bei großen, instruction-tuned Modellen wie GPT-4 oder Claude. Die Limitation besteht darin, dass Base Models wie LeoLM-7B schwache Zero-Shot-Performance aufweisen. Für komplexe Transformationsaufgaben ist dieser Ansatz nicht ausreichend. Ein Beispiel wie „Schreibe diese Verkehrsanweisung verständlich um." führt zu unpräzisen Ergebnissen.

Few-Shot Learning kombiniert Aufgabe mit Beispielen im Prompt [SOURCE-MISSING]. In-Context Learning erfolgt durch Demonstration gewünschten Verhaltens. Der Mechanismus beruht darauf, dass das Modell Muster aus Input-Output-Paaren erkennt. Die Anzahl der Beispiele beträgt typisch 1 bis 10 Shots, da sie durch das Context-Window limitiert ist [SOURCE-MISSING]. Empirische Erkenntnisse zeigen dramatische Leistungssteigerung gegenüber Zero-Shot [SOURCE-MISSING]. Die Performance skaliert mit der Beispielanzahl bis zu einem Plateau-Effekt [SOURCE-MISSING].

Der Vergleich zwischen Few-Shot und Fine-Tuning [SOURCE-MISSING] zeigt, dass Few-Shot keine permanenten Änderungen bewirkt und flexibel anpassbar ist. Fine-Tuning ändert Modellgewichte und vermittelt domänenspezifische Expertise. Der Trade-off besteht darin, dass Few-Shot längere Prompts benötigt und Kontext-Token-Kosten verursacht. Die Kombination ist optimal: Fine-Tuning für Grundfähigkeiten, Few-Shot für Nuancen.

Das Few-Shot Prompt Design erfordert praktische Überlegungen. Bei der Beispielauswahl [SOURCE-MISSING] ist Diversität wichtig, um verschiedene Störungstypen wie Bauarbeiten, technische Störungen und Umleitungen abzudecken. Repräsentativität bedeutet, häufigste Fälle zuerst zu behandeln, Edge Cases später. Qualität übertrifft Quantität: 3 perfekte Beispiele sind besser als 10 durchschnittliche [SOURCE-MISSING]. Balancierung folgt der gleichen Verteilung wie im Datensatz (siehe Abschnitt~\ref{sec:2.2.3}).

Die Beispielreihenfolge [SOURCE-MISSING] unterliegt Recency Bias, wobei letzte Beispiele den stärksten Einfluss haben. Die Strategie besteht darin, Komplexität aufsteigend anzuordnen, von einfach zu schwierig. Ähnlichkeitsbasierte Sortierung platziert relevanteste Beispiele zuerst.

Formatierung und Struktur [SOURCE-MISSING] erfordern klare Trennung zwischen Input und Output, beispielsweise durch „Input:" und „Output:". Konsistente Formatierung über alle Beispiele hinweg ist essentiell. Delimiter zwischen Beispielen wie „---" oder doppelte Zeilenumbrüche strukturieren den Prompt. Template-Konsistenz bedeutet, dass die gleiche Struktur wie bei Inferenz verwendet wird.

Instruction Prompting mittels Systemanweisungen definiert explizit die Aufgabenbeschreibung vor Beispielen. Komponenten effektiver Instructions [SOURCE-MISSING] umfassen Rollenspezifikation wie „Du bist ein Experte für Fahrgastinformation im ÖPNV", präzise Aufgabendefinition der Transformation, Constraints bezüglich dessen, was nicht getan werden darf wie keine Halluzinationen und keine Informationsweglassung, Stilrichtlinien für Tonalität, Formalität und Zielgruppe sowie Ausgabeformat für die erwartete Struktur des generierten Texts.

Instruction-Länge und Spezifität [SOURCE-MISSING] beinhalten einen Trade-off zwischen Detailliertheit und Flexibilität. Zu vage Instruktionen wie „Schreibe verständlich" führen zu variierender Interpretation. Zu spezifische Vorgaben wie „Nutze genau 2 Sätze mit maximal 15 Wörtern" wirken unnatürlich. Optimal sind Prinzipien statt starre Regeln, wie „Bevorzuge aktive Formulierungen".

Negativbeispiele und Abgrenzungen [SOURCE-MISSING] sind effektiver als nur positive Vorgaben. „Schreibe NICHT im Telegrammstil" und „Erfinde KEINE Details, die nicht in der Eingabe stehen" zur Anti-Halluzination zeigen, dass empirisch die Kombination aus positiven und negativen Constraints am effektivsten ist.

Chain-of-Thought Prompting für komplexes Reasoning [SOURCE-MISSING] unterscheidet sich von Standard-Prompting durch direkte Input-Output-Zuordnung. Chain-of-Thought macht Zwischenschritte explizit. Der Mechanismus besteht darin, dass das Modell den Denkprozess zeigt mit „Lass uns Schritt für Schritt denken" [SOURCE-MISSING]. Dies ist besonders effektiv bei Multi-Hop-Reasoning und komplexen Transformationen [SOURCE-MISSING].

Die Anwendung auf Verkehrsanweisungs-Transformation erfolgt schrittweise: Identifiziere Kernaussage bezüglich Linie und Störung, extrahiere Entitäten wie Liniennummern, Haltestellen und Zeitangaben, bestimme Fahrzeugtyp aus Knowledge Base, formuliere verständlichen Satz mit allen Informationen. Der Vorteil besteht in reduzierter Fehlerrate durch strukturierten Prozess.

Zero-Shot-CoT versus Few-Shot-CoT [SOURCE-MISSING] zeigt, dass Zero-Shot-CoT „Lass uns Schritt für Schritt denken" als Trigger nutzt. Few-Shot-CoT verwendet Beispiele mit expliziten Reasoning-Schritten. Few-Shot-CoT ist überlegen bei domänenspezifischen Aufgaben und relevant für strukturierte Transformation mit Reasoning in dieser Arbeit.

Limitierungen [SOURCE-MISSING] bestehen in erhöhtem Token-Verbrauch durch Zwischenschritte. Bei einfachen Transformationen ist dies nicht notwendig. In Produktionsumgebung entfernt Post-Processing Reasoning, sodass nur das Endergebnis ausgegeben wird.

Prompt Template Design für Automatisierung unterscheidet manuelle Prompts für einzelne Anfragen von Templates für Batch-Verarbeitung [SOURCE-MISSING]. Die Template-Struktur [SOURCE-MISSING] umfasst System Prompt als statische Komponente mit Rollenspezifikation und Grundregeln, Few-Shot Examples als statisch oder dynamisch ausgewählt, User Prompt als variable Komponente mit konkreter Verkehrsanweisung sowie optional Knowledge Context mit Linien-Fahrzeug-Zuordnungen als JSON.

Variablen-Substitution [SOURCE-MISSING] nutzt Platzhalter für dynamische Inhalte wie \{INPUT\_INSTRUCTION\} und \{KNOWLEDGE\_BASE\}. Formatierung umfasst JSON-Escaping und Whitespace-Normalisierung. Validierung prüft vollständige Substitution vor Inferenz.

Template-Typen für verschiedene Störungskategorien differenzieren Typ A für Bauarbeiten mit Alternativrouten-Information, Typ B für technische Störungen mit Fahrzeugtyp-Präzision, Typ C für Umleitungen mit geografischer Klarheit. Adaptive Template-Auswahl basiert auf Input-Klassifizierung.

Prompt Optimization erfolgt durch iterative Verbesserung. Systematische Optimierung [SOURCE-MISSING] erstellt eine Baseline mit einfachem Prompt als Ausgangspunkt, führt Fehleranalyse durch, um Fehlerarten zu identifizieren, fügt gezielte Constraints zur Adressierung spezifischer Fehlerklassen hinzu, vergleicht verschiedene Prompt-Varianten durch A/B-Testing und verbessert schrittweise durch iterative Refinement.

Automatische Prompt Optimization [SOURCE-MISSING] mittels APE (Automatic Prompt Engineering) nutzt LLMs zur Generierung und Evaluation von Prompt-Varianten. Diese Methode wird in dieser Arbeit nicht verwendet, da sie zu ressourcenintensiv ist. Manuelle Optimierung ist praktikabler bei kleinem Anwendungsbereich.

Metrik-gesteuerte Optimierung [SOURCE-MISSING] prüft Faktentreue gegen strukturierte Eingabedaten, überwacht Stilkonformität zur Einhaltung der Richtlinien (siehe Abschnitt~\ref{sec:3.2}), gewährleistet Konsistenz, sodass gleiche Eingaben identische Ausgaben bei deterministischer Temperatur erzeugen, und optimiert Effizienz durch Minimierung der Token-Länge bei gleichbleibender Qualität.

Advanced Prompting Techniques umfassen Self-Consistency [SOURCE-MISSING] mit mehrfacher Generierung durch Sampling und anschließender Mehrheitsentscheidung. Dies reduziert Halluzinationen durch Konsensbildung, verursacht jedoch Trade-off durch mehrfache Inferenz-Kosten. Die Anwendung eignet sich bei kritischen Transformationen.

Least-to-Most Prompting [SOURCE-MISSING] zerlegt komplexe Aufgaben in einfachere Teilschritte, wobei jeder Schritt auf vorherigem aufbaut. Dies ist ähnlich zu Chain-of-Thought, aber strukturierter.

ReAct (Reasoning + Acting) [SOURCE-MISSING] kombiniert Reasoning mit Tool-Use. Das Modell entscheidet, wann externe Informationen benötigt werden. Dies ist nicht direkt relevant für diese Arbeit, da keine externen Tools verwendet werden.

Prompting bei Base Models versus Instruction-Tuned Models zeigt Unterschiede. Base Models wie LeoLM-7B Base [SOURCE-MISSING] weisen schwache Instruktionsbefolgung ohne Fine-Tuning auf, tendieren zur Textkontinuation statt Aufgabenlösung, erfordern präzisere, strukturiertere Prompts, machen Few-Shot Examples essentiell und nicht optional und nutzen Completion-Framing wie „Text: ... Umschreibung:" statt „Schreibe um:".

Instruction-Tuned Models wie LeoLM-7B-Instruct [SOURCE-MISSING] zeigen bessere Zero-Shot-Performance, verstehen imperative Formulierungen wie „Transformiere" oder „Schreibe", akzeptieren flexiblere Prompt-Formate, profitieren von Few-Shot zur weiteren Verbesserung, wobei dies weniger kritisch ist.

Die Implikation für diese Arbeit [SOURCE-MISSING] besteht darin, dass das Base Model gewählt wurde (siehe Kapitel~\ref{chap:4}). Prompt Engineering ist umso wichtiger bei Base Models. Fine-Tuning kann Prompt-Abhängigkeit reduzieren, aber nicht eliminieren. Post-Fine-Tuning Prompts können kürzer und direkter sein.

Prompt Engineering im Kontext dieser Arbeit findet Einsatz in verschiedenen Phasen: Pre-Fine-Tuning zur Evaluation verschiedener Base Models (Kapitel~\ref{chap:4}), Training mit Few-Shot Examples als Teil des Trainingsformats (Kapitel~\ref{chap:5}) und Post-Fine-Tuning mit Inference-Zeit-Prompts für Konsistenz (Kapitel~\ref{chap:6}).

Spezifische Anwendungen umfassen System Prompt mit LVB-Stilrichtlinien und Zielgruppendefinition für Fahrgäste, Knowledge Integration mit Linien-Fahrzeug-Zuordnungen als strukturierter Kontext, Constraint Specification ohne Halluzinationen bei vollständiger Informationswiedergabe sowie Format Control für einzelnen Fließtext-Satz als Output.

Der Synergieeffekt mit Fine-Tuning besteht darin, dass Fine-Tuning domänenspezifisches Vokabular und Muster lernt, während Prompts aufgabenspezifische Nuancen und Edge Cases steuern. Die Kombination ist robuster als jede Methode einzeln und reduziert Overfitting-Risiko (Abschnitt~\ref{sec:2.2.4}) durch flexible Steuerung.

Die Überleitung zu Abschnitt~\ref{sec:2.2.3} verdeutlicht, dass Prompt Engineering Verhaltenssteuerung zur Inferenzzeit ermöglicht. Der nächste Schritt betrifft Datenqualität und -struktur für Fine-Tuning. Das Zusammenspiel besteht darin, dass Prompts die Aufgabe definieren, während Daten Fähigkeiten trainieren. Die Perspektive verschiebt sich von „Wie kommuniziere ich die Aufgabe?" zu „Wie bereite ich Trainingsdaten auf?".
        
        
        \subsubsection{Datensätze für Fine-Tuning}
        \label{sec:2.2.3}
        
        % Ziel: 2-2,5 Seiten
        
        Die Qualität und Zusammenstellung von Trainingsdaten stellen einen kritischen Erfolgsfaktor für das Fine-Tuning dar. Besondere Herausforderungen ergeben sich bei kleinen domänenspezifischen Datensätzen. Transfer Learning reduziert den Datenbedarf, aber ausreichende Qualität bleibt essentiell.

Trainingsdaten für Supervised Fine-Tuning bestehen aus Input-Output-Paaren. Format-Anforderungen umfassen JSON, JSONL oder spezifische Modell-Formate. Konsistenz in Struktur und Formatierung ist unabdingbar. Annotationsrichtlinien sichern durch klare, eindeutige Vorgaben die Qualität. Inter-Annotator Agreement dient als Qualitätsmetrik.

Prompt-Template-basiertes Format-Design [SOURCE-MISSING] erfordert, dass Training-Daten das Inference-Format widerspiegeln. Instruction, Few-Shot Examples und Task bilden die Trainingsstruktur. Konsistente Delimiter und Formatierung zwischen Training und Inferenz sind erforderlich. Der Vorteil besteht darin, dass das Modell nicht nur Inhalt, sondern auch Format-Erwartungen lernt. Ein Verweis auf Prompt Engineering-Prinzipien erfolgt in Abschnitt~\ref{sec:2.2.2}.

Data Augmentation vergrößert den Datensatz ohne zusätzliche manuelle Annotation. Die Ziele umfassen verbesserte Generalisierung, reduziertes Overfitting und erhöhte Robustheit.

Rule-based Augmentation [SOURCE-MISSING] umfasst Synonym Replacement durch Austausch von Wörtern, Random Insertion durch Einfügen zusätzlicher Wörter, Random Swap durch Vertauschen von Wortpositionen, Random Deletion durch Entfernen einzelner Wörter sowie EDA (Easy Data Augmentation) [SOURCE-MISSING] als Kombination obiger Techniken. Dies ist effektiv bei kleinen Datensätzen mit 50 bis 500 Beispielen.

Model-based Augmentation nutzt Back-Translation durch Übersetzung in andere Sprache und zurück, Paraphrasing mit vortrainierten Modellen, Contextual Word Embeddings für Ersetzungen sowie Template-basierte Generierung [SOURCE-MISSING]. Diese Methoden bieten höhere Qualität, sind aber rechenintensiver.

Synthetic Data Generation erzeugt komplett neue Beispiele durch LLMs. Constraint-basierte Generation mit validierten Entitäten [SOURCE-MISSING] ermöglicht Kontrolle über Diversität und Abdeckung. Das Risiko besteht in Halluzinationen und unrealistischen Beispielen. Best Practice kombiniert reale und synthetische Daten, typisch 20 bis 30\,\% synthetisch.

Empirische Erkenntnisse zeigen, dass EDA mit 50\,\% der Daten gleiche Accuracy erreichen kann wie 100\,\% ohne Augmentierung [SOURCE-MISSING]. Der Effekt verstärkt sich bei kleineren Datensätzen. Qualität ist wichtiger als Quantität bei synthetischen Daten.

Unbalancierte Datensätze führen dazu, dass Modelle zu häufigen Klassen tendieren, schlechtere Performance auf seltenen aber wichtigen Fällen zeigen und Bias in Vorhersagen aufweisen. Strategien für Balance [SOURCE-MISSING] umfassen Oversampling unterrepräsentierter Kategorien, vorsichtiges Undersampling überrepräsentierter Kategorien, SMOTE-ähnliche Ansätze sowie Class-weighted Loss Functions.

Empfohlene Verhältnisse beinhalten ideale Balance mit gleicher Anzahl pro Kategorie. In der Praxis ist ein maximales 1:3-Verhältnis zwischen seltenster und häufigster Kategorie akzeptabel. Bei stärkerer Imbalance ist gezielte Augmentierung erforderlich.

Der Umgang mit kleinen Datensätzen definiert „klein" typischerweise als weniger als 1000 Beispiele für Fine-Tuning. Few-Shot Learning stellt eine Alternative dar [SOURCE-MISSING]. Data Efficiency Techniques umfassen aggressive Augmentierung bei Qualitätswahrung, parameter-effiziente Methoden wie LoRA zur Reduktion des Overfitting-Risikos, niedrigere Learning Rates sowie Early Stopping basierend auf Validation Loss.

Mixed Task Training kombiniert domänenspezifische und allgemeine Daten. Das Verhältnis beträgt typisch 85 bis 90\,\% spezifisch und 10 bis 15\,\% allgemein [SOURCE-MISSING]. Dies verhindert Catastrophic Forgetting (siehe Abschnitt~\ref{sec:2.2.4}) und erhält Generalisierungsfähigkeit.

Die Standard-Aufteilung für Train-Validation-Test-Split beträgt 70 bis 80\,\% Training, 10 bis 15\,\% Validation und 10 bis 15\,\% Test. Bei kleinen Datensätzen wird 80-10-10 oder Cross-Validation empfohlen. Stratified Split bei kategorischen Daten erhält Balance. Temporale Splits sind bei zeitabhängigen Daten wichtig. Vermeidung von Data Leakage zwischen Splits ist essentiell.

Qualitätssicherung erfolgt durch automatisierte Validierung (siehe Abschnitt~\ref{sec:5.2.2} für praktische Umsetzung) mit Strukturprüfung von Format und Pflichtfeldern, Konsistenzprüfung von Entitäten und Fakten sowie Duplikatserkennung. Manuelle Stichprobenprüfung und iterative Verbesserung basierend auf Modellfehlern ergänzen dies.

Der Übergang zu Kapitel~\ref{chap:5} zeigt, dass diese theoretischen Grundlagen die Basis für praktische Datensatzerstellung bilden. Die konkrete Anwendung dieser Prinzipien wird in Abschnitt~\ref{sec:5.2} beschrieben. Anpassung an spezifische Anforderungen der Verkehrsanweisungstransformation erfolgt dort.
        
        
        \subsubsection{Herausforderungen beim Fine-Tuning}
        \label{sec:2.2.4}
        
        % Ziel: 1,5-2 Seiten
        
        Fine-Tuning vortrainierter Modelle bringt spezifische Herausforderungen mit sich. Das Spannungsfeld zwischen Spezialisierung auf neue Aufgaben und Erhalt allgemeiner Fähigkeiten ist besonders kritisch bei kleinen domänenspezifischen Datensätzen. Drei zentrale Problemfelder sind Overfitting, Catastrophic Forgetting und Konvergenz.

\paragraph{Overfitting -- Überanpassung an Trainingsdaten}

Overfitting tritt auf, wenn das Modell Trainingsdaten auswendig lernt, statt Muster zu generalisieren. Charakteristisch sind hohe Training Accuracy bei niedriger Validation/Test Accuracy sowie Divergenz zwischen Training und Validation Loss. Besonders ausgeprägt ist dies bei kleinen Datensätzen mit weniger als 1000 Beispielen. LoRA reduziert das Overfitting-Risiko gegenüber Full Fine-Tuning, eliminiert es aber nicht.

Ursachen umfassen Modellkomplexität, die den Informationsgehalt der Trainingsdaten übersteigt, zu hohe Anzahl trainierbarer Parameter relativ zur Datensatzgröße, zu viele Trainings-Epochen, wodurch das Modell memoriert statt zu lernen, unbalancierte Datensätze mit Bias zu überrepräsentierten Kategorien sowie zu hohe Learning Rate, die instabile, zu schnelle Anpassung an Trainingsdaten verursacht.

Bei LoRA-Fine-Tuning spezifisch führt höherer Rank zu mehr trainierbaren Parametern und höherem Overfitting-Risiko. Mehr Target Modules vergrößern die Angriffsfläche für Überanpassung. Der Trade-off besteht zwischen Expressivität und Generalisierung.

Vermeidungsstrategien umfassen mehrere Ansätze. Regularisierung [SOURCE-MISSING] nutzt L2-Regularisierung (Weight Decay) zur Bestrafung großer Gewichtswerte. Mathematisch wird ein zusätzlicher Term $\lambda||w||^2$ zur Loss-Funktion hinzugefügt, was kleinere, gleichmäßiger verteilte Gewichte fördert. L1-Regularisierung fördert Sparsity, ist aber weniger relevant für LoRA. Dropout [SOURCE-MISSING] deaktiviert zufällig Neuronen während des Trainings, ist bei LoRA jedoch nur für lange Trainings-Episoden geeignet (siehe Abschnitt~\ref{sec:2.2.1}) und verhindert Co-Adaptationen zwischen Neuronen.

Early Stopping [SOURCE-MISSING] erfordert kontinuierliches Monitoring von Validation Loss während des Trainings. Das Training wird gestoppt, wenn Validation Loss nicht mehr sinkt oder ansteigt. Typische Patience-Strategie umfasst 3 bis 5 Epochen ohne Verbesserung. Dies verhindert Überanpassung in späten Trainingsphasen und erfordert ein separates Validation Set.

Datenaugmentierung (siehe Abschnitt~\ref{sec:2.2.3}) vergrößert künstlich den effektiven Datensatz und reduziert Overfitting durch erhöhte Variabilität. Methoden umfassen EDA, Back-Translation und Synthetic Data Generation, was Robustheit gegenüber Formulierungsvariationen erhöht.

Cross-Validation mittels K-Fold Cross-Validation bei sehr kleinen Datensätzen ermöglicht robustere Schätzung der Generalisierungsfähigkeit. Dies ist rechenintensiv, aber aussagekräftiger bei limitierten Daten.

PEFT-spezifische Strategien nutzen niedrigeren LoRA-Rank zur Reduktion trainierbarer Parameter sowie selektive Target Modules, um nur kritische Layer anzupassen. Der Trade-off besteht zwischen geringerer Expressivität und besserer Generalisierung.

Die Balancierung zwischen Underfitting und Overfitting zeigt, dass Underfitting bei zu einfachem Modell auftritt, das Muster nicht erfasst (zu niedriger Rank, zu wenig Training). Overfitting tritt bei zu komplexem Modell auf, das Daten memoriert (zu hoher Rank, zu viel Training). Der optimale Arbeitspunkt liegt zwischen beiden Extremen. Empirische Bestimmung durch Validation-Set-Performance ist notwendig. Der Bias-Variance Trade-off [SOURCE-MISSING] verdeutlicht dieses Dilemma.

\paragraph{Catastrophic Forgetting -- Verlust vortrainierter Fähigkeiten}

Catastrophic Forgetting bezeichnet das Phänomen, dass Modelle während des Fine-Tunings Fähigkeiten aus der Pretraining-Phase verlieren. Spezialisierung auf neue Aufgaben geht zu Lasten allgemeiner Kompetenzen. Graduelle oder abrupte Verschlechterung auf Pretraining-Tasks tritt auf. Erstmals dokumentiert in neuronalen Netzen [SOURCE-MISSING], ist es bei LoRA weniger ausgeprägt als bei Full Fine-Tuning, aber dennoch vorhanden [SOURCE-MISSING].

Manifestationen umfassen Wissensverlust, wobei Fakten aus Pretraining-Korpora nicht mehr abrufbar sind, Sprachkompetenz-Degradation durch Verschlechterung grammatikalischer Fähigkeiten sowie Task Interference, bei der neue Aufgaben Wissen über alte Aufgaben überschreiben. Safety Alignment Degradation [SOURCE-MISSING] zeigt, dass Fine-Tuning Safety Guardrails schwächen kann, was alle PEFT-Methoden betrifft und selbst bei benign Training Data möglich ist.

Theoretische Ursachen liegen in Weight Interference, wobei neue Gewichtsanpassungen alte Repräsentationen überschreiben. Gradient Descent Bias favorisiert aktuelle Task über vorherige. Representation Overlap erfordert, dass Shared Weights beide Tasks gleichzeitig kodieren. Distribution Shift entsteht, wenn Trainingsverteilung sich stark von Pretraining-Daten unterscheidet.

LoRA-spezifische Faktoren umfassen Zero-Initialisierung der B-Matrix, die Konvergenz verlangsamen kann [SOURCE-MISSING]. Low-Rank Updates können wichtige Pretraining-Repräsentationen nicht vollständig bewahren. Trotz eingefrorener Basisgewichte können Adapter-Outputs Aktivierungen verzerren.

Gegenmaßnahmen umfassen niedrigere Learning Rates [SOURCE-MISSING], typisch 1e-4 bis 5e-5 für LoRA gegenüber 1e-3 für Full Fine-Tuning. Sanftere Anpassung erhält mehr Pretraining-Wissen, mit Trade-off in langsamerer Konvergenz.

LoRA als sanftere Alternative zu Full Fine-Tuning friert Originalgewichte ein (siehe Abschnitt~\ref{sec:2.2.1}). Nur Low-Rank-Adapter werden trainiert, was Catastrophic Forgetting im Vergleich zu Full Fine-Tuning reduziert, aber nicht vollständig eliminiert [SOURCE-MISSING].

Mixed Task Training kombiniert domänenspezifische und allgemeine Daten. Typisches Verhältnis beträgt 85 bis 90\,\% spezifisch und 10 bis 15\,\% allgemein [SOURCE-MISSING]. Dies erhält Generalisierungsfähigkeit während Spezialisierung, beispielsweise LVB-Verkehrsanweisungen kombiniert mit allgemeinen deutschen Texten.

Regularisierungstechniken umfassen Elastic Weight Consolidation (EWC) [SOURCE-MISSING] sowie Knowledge Distillation vom Pretraining-Modell. Bei LoRA ist dies weniger kritisch aufgrund gefrorener Basisgewichte.

Early Stopping basierend auf Generalisierungsmetriken monitort nicht nur Task-Accuracy, sondern auch allgemeine Sprachfähigkeiten. Perplexity auf Out-of-Domain-Daten dient als Indikator. Training wird bei signifikantem Anstieg gestoppt.

\paragraph{Konvergenz-Herausforderungen bei LoRA}

LoRA konvergiert signifikant langsamer als Full Fine-Tuning [SOURCE-MISSING]. Empirisch dokumentiert sind 5 bis 6-fach mehr Iterationen und FLOPs für gleiche Performance. Dies führt zu einer paradoxen Situation: Pro-Iteration effizienter, aber Gesamt-Trainingskosten höher. Dies kann zu schlechterer finaler Test-Performance führen und ist nicht nur ein Geschwindigkeitsproblem, sondern eine fundamentale Optimierungsschwierigkeit.

Architektonische Ursachen liegen in Zero-Initialisierung der B-Matrix [SOURCE-MISSING]. Bei Training-Start gilt $\Delta W = BA = 0$ (keine initiale Störung), was langsame Trainingsdynamik zwischen A und B in frühen Epochen verursacht. Kurzsichtige Inter-Layer-Interaktionen verzögern Entwicklung komplexer Feature-Transformationen.

Der Low-Rank Constraint [SOURCE-MISSING] beschränkt Gewichtsupdates auf niedrigdimensionalen Unterraum, kann komplexe Adaptionen nicht vollständig abbilden, limitiert Expressivität der Weight Updates und führt zum Performance-Gap zu Full Fine-Tuning bei komplexen Datensätzen [SOURCE-MISSING].

Imbalancierte Weight Updates [SOURCE-MISSING] entstehen, weil Low-Rank-Faktorisierung nicht eindeutig ist (non-unique). Verschiedene A-B-Kombinationen ergeben gleiches $\Delta W$, was zu inkonsistenten und imbalancierten Updates führt und suboptimale Konvergenzpfade im Optimierungsraum verursacht.

Das Rank-Auswahl-Dilemma zeigt ein fundamentales Trade-off [SOURCE-MISSING]. Niedriger Rank (r=4-8) bietet hohe Parameter-Effizienz und schnelles Training pro Iteration, aber signifikanten Performance-Gap zu Full Fine-Tuning bei zu geringer Expressivität für komplexe Aufgaben. Hoher Rank (r=64-128) bietet bessere Performance, die sich Full Fine-Tuning nähert, und höhere Expressivität der Weight Updates, aber Parameter-Kosten wachsen linear mit Rank, was den fundamentalen Vorteil der Parameter-Effizienz schwächt.

Empirische Beobachtungen zeigen, dass Performance sich monoton mit steigendem Rank verbessert [SOURCE-MISSING], aber Diminishing Returns ab bestimmtem Rank (task-abhängig) eintreten. Optimaler Rank variiert nach Modellgröße, Task-Komplexität und Datensatzgröße. Keine universelle Heuristik existiert: Empirische Bestimmung ist erforderlich [SOURCE-MISSING].

Der „Low-Rank Bottleneck" zeigt, dass Narrowing Performance-Gap Rank-Erhöhung erfordert. Bei sehr hohem Rank nähert man sich Full Fine-Tuning-Kosten an. Dies ist eine fundamentale Limitation der Low-Rank-Annahme und theoretische Grenze der PEFT-Effizienz.

Der Performance-Gap zu Full Fine-Tuning ist konsistent über verschiedene Benchmarks dokumentiert [SOURCE-MISSING]. Besonders ausgeprägt bei komplexen Datensätzen mit diversen Sub-Domänen, Tasks mit hoher semantischer Variabilität sowie kleinen Modellen mit weniger als 7 Milliarden Parametern [SOURCE-MISSING]. Der Gap verringert sich mit größeren Basismodellen, höherem LoRA-Rank und längeren Trainings-Episoden.

Overfitting bei LoRA-Training zeigt widersprüchliche Phänomene [SOURCE-MISSING]. Höherer Rank bringt nicht zwingend bessere Performance und kann zu Overfitting führen, besonders bei kleinen Datensätzen. Non-monotones Verhalten tritt auf: Optimum liegt oft bei mittlerem Rank. Initialization Bottleneck [SOURCE-MISSING] zeigt, dass Zero-Initialisierung Aktivierung der Originalgewichte limitiert und optimale Performance-Pfade blockieren kann.

Die Wechselwirkungen zwischen Herausforderungen zeigen, dass Overfitting und Catastrophic Forgetting beide von Regularisierung und Early Stopping profitieren. Gegenläufig ist, dass Overfitting Spezialisierung will, CF Generalisierung. Balance ist erforderlich: weder zu spezialisiert noch zu generisch.

Konvergenz und Overfitting interagieren dahingehend, dass langsame Konvergenz Overfitting verzögern kann (implizite Regularisierung). Zu viele Epochen führen trotz langsamer Konvergenz zu Overfitting. Early Stopping muss beide Faktoren berücksichtigen.

Learning Rate als zentraler Hebel zeigt: zu hoch führt zu schneller Konvergenz, aber Overfitting und CF-Risiko. Zu niedrig verursacht langsame Konvergenz und paradoxerweise auch Overfitting. Optimal ist task- und datensatzabhängig und empirisch zu bestimmen.

Die theoretische Perspektive betrachtet Fine-Tuning als Optimierungsproblem mit multiplen Constraints. Keine universelle Lösung existiert: Trade-offs zwischen verschiedenen Zielen sind unvermeidbar. PEFT-Methoden wie LoRA verschieben Trade-offs, eliminieren sie aber nicht. Empirische Validierung ist unerlässlich für praktische Anwendungen.


    \subsection{Ressourceneffizienz und Modelloptimierung}
    \label{sec:2.3}
    
    % Ziel: 4-5 Seiten
    
        \subsubsection{Problematik großer Sprachmodelle}
        \label{sec:2.3.1}
        
        % Ziel: 1,5 Seiten
        % - Energieverbrauch und CO2-Bilanz großer Modelle
        % - Überdimensionierung in der Praxis
        % - Wirtschaftliche und ökologische Implikationen
        % - Notwendigkeit ressourceneffizienter Alternativen
        
        
        \subsubsection{Quantisierung}
        \label{sec:2.3.2}
        
        % Ziel: 2,5-3 Seiten
        % AUSFÜHRLICH, da zentral für die Arbeit:
        % - Grundprinzip der Quantisierung
        % - INT8/INT4-Quantisierung
        % - Theoretische Einsparungen bei Speicher und Rechenleistung
        % - Speicherbedarf (konkrete Zahlen aus Literatur)
        % - Inferenzgeschwindigkeit
        % - Energieverbrauch (theoretische Berechnungen)
        % - Trade-off: Effizienz vs. Qualität
        % - Quantisierung vor vs. nach Fine-Tuning
        % - Verfügbare Open-Source-Tools (llama.cpp, bitsandbytes, GPTQ)
        % - Erwartete praktische Implikationen
        
        
        \subsubsection{Weitere Optimierungsansätze}
        \label{sec:2.3.3}
        
        % Ziel: 1,5-2 Seiten
        % - RAG-Systeme (Retrieval-Augmented Generation)
        %   * Konzept: Dynamischer Retrieval + Generation
        %   * Zwei-Stufen-Prozess (Retrieval aus Vektorstore, dann LLM-Generation)
        %   * Vorteile: Skalierung auf große Wissensbasen, Aktualität
        %   * Anwendungsfälle im Verkehrssektor
        %   * Hardware-Anforderungen und Komplexität
        % - Knowledge-Enhanced Prompting als vereinfachte Alternative
        %   * Statische Einbettung von Domänenwissen in System-Prompts
        %   * Vorteile: Einfachere Implementierung, geringere Hardware-Anforderungen
        %   * Limitierungen: Begrenzte Wissensbasis (Context-Window), keine Dynamik
        %   * Geeignet für überschaubare, stabile Domänen
        % - Modellkompression (Pruning, Destillation - kurz erwähnen)
        % - Vergleich verschiedener Ansätze


    \subsection{Qualitätssicherung bei KI-generierten Texten}
    \label{sec:2.4}
    
    % Ziel: 2-2,5 Seiten
    
        \subsubsection{Evaluationsmetriken für NLP}
        \label{sec:2.4.1}
        
        % Ziel: 1 Seite
        
        % AUTOMATISCHE METRIKEN:
        % - BLEU, ROUGE (kurz): N-gram Overlap-basiert
        %   * Ursprünglich für Maschinenübersetzung entwickelt
        %   * Limitiert für semantische Äquivalenz
        %   * Schnell berechenbar, aber oberflächlich
        %
        % - Semantische Ähnlichkeitsmetriken (BERTScore):
        %   * Contextualized Embeddings statt Surface-Form
        %   * Bessere Erfassung von Paraphrasen
        %   * Rechenintensiver, aber aussagekräftiger
        %
        % - Perplexity:
        %   * Misst Konfidenz des Modells
        %   * Niedriger = besseres Language Modeling
        %   * Nicht direkt für Qualität, aber für Training-Monitoring
        %
        % PROMPT-BASIERTE EVALUATION \cite{berryman2024prompt}:
        % - LLM-as-a-Judge: Nutzung größerer Modelle zur Bewertung
        %   * Prompt: "Bewerte die Qualität dieser Transformation auf Skala 1-5"
        %   * Kriterien: Faktentreue, Verständlichkeit, Stilkonformität
        %   * Vorteil: Flexibel, domänenspezifisch anpassbar
        %   * Limitation: Bias des Evaluator-Modells, Kosten
        %
        % - Comparative Evaluation \cite{berryman2024prompt}:
        %   * A/B-Vergleich zweier Outputs
        %   * "Welcher Text ist verständlicher: A oder B?"
        %   * Robuster als absolute Bewertungen
        %   * Nützlich für Prompt-Optimierung (siehe Abschnitt~\ref{sec:2.2.2})
        %
        % DOMÄNENSPEZIFISCHE METRIKEN:
        % - Faktentreue: Prüfung gegen strukturierte Eingabedaten
        %   * Entitätsextraktion: Liniennummer, Haltestellen, Zeitangaben
        %   * Vollständigkeitsprüfung: Alle Input-Informationen im Output?
        %   * Halluzination Detection: Erfundene Details? (siehe Abschnitt~\ref{sec:2.4.2})
        %
        % - Stilkonformität:
        %   * Einhaltung von Stilrichtlinien (siehe Abschnitt~\ref{sec:3.2})
        %   * Regelbasierte Checks: Passiv vs. Aktiv, Fachbegriffe vs. Allgemeinsprache
        %   * Pattern-Matching für verbotene Formulierungen
        %
        % - Konsistenz:
        %   * Gleiche Eingabe → identische Ausgabe (bei deterministischer Temperatur)
        %   * Messung: Wiederholte Generierung, Hamming-Distanz zwischen Outputs
        %   * Wichtig für Produktionsumgebung: Vorhersagbarkeit
        
        
        \subsubsection{Halluzination Detection und Validierung}
        \label{sec:2.4.2}
        
        % Ziel: 1-1,5 Seiten
        
        % PROBLEMSTELLUNG:
        % - Halluzinationen: Modell generiert faktisch inkorrekte oder erfundene Informationen
        % - Besonders kritisch bei sicherheitsrelevanten Verkehrsinformationen
        % - Herausforderung: Plausibel klingende, aber falsche Aussagen
        % - Beispiel: "Straßenbahn Linie 10" statt korrekter "Bus Linie 10"
        %
        % SELF-CONSISTENCY CHECKS \cite{bonstra2024prompt}:
        % - Grundprinzip: Mehrfache Generierung mit Sampling (siehe Abschnitt~\ref{sec:2.2.2})
        % - Mechanismus:
        %   * Gleiche Eingabe → N verschiedene Outputs generieren (typisch N=3-10)
        %   * Sampling mit Temperature > 0 für Variabilität
        %   * Mehrheitsentscheidung oder Konsensanalyse
        % - Annahme: Korrekte Antworten konvergieren, Halluzinationen divergieren
        % - Vorteil: Keine externe Wissensquelle erforderlich
        % - Limitation: Mehrfache Inferenz-Kosten (N-facher Rechenaufwand)
        % - Anwendung: Kritische Transformationen, wo Faktentreue essentiell ist
        %
        % FACT CHECKING GEGEN EINGABEDATEN:
        % - Strukturierte Validierung \cite{berryman2024prompt}:
        %   * Extraktion von Entitäten aus Input und Output
        %   * Vergleich: Sind alle Input-Entitäten im Output enthalten?
        %   * Regel: Output darf KEINE Entitäten enthalten, die nicht im Input sind
        % - Named Entity Recognition (NER) für Extraktion:
        %   * Liniennummern, Haltestellen, Zeitangaben, Fahrzeugtypen
        %   * Pattern-Matching oder NER-Modell
        % - Constraint-basierte Validierung:
        %   * Whitelist-Ansatz: Nur bekannte Liniennummern erlaubt
        %   * Knowledge Base: Linien-Fahrzeug-Zuordnungen als Ground Truth
        %   * Reject-Output bei Verletzung von Constraints
        %
        % PROMPT-BASIERTE HALLUZINATION PREVENTION \cite{berryman2024prompt}:
        % - Negative Constraints in System Prompt (siehe Abschnitt~\ref{sec:2.2.2}):
        %   * "Erfinde KEINE Details, die nicht in der Eingabe stehen"
        %   * "Nutze NUR Informationen aus dem bereitgestellten Kontext"
        %   * "Bei Unsicherheit: NICHT spekulieren, sondern Information weglassen"
        % - Knowledge Grounding:
        %   * Explizite Einbettung von Fakten im Prompt
        %   * Beispiel: "Linie 10 ist ein Bus, Linie 11 ist eine Straßenbahn"
        %   * Reduziert Halluzinationen durch direkte Verfügbarkeit von Wissen
        % - Chain-of-Thought für Faktentreue:
        %   * Zwischenschritt: "Welche Informationen sind in der Eingabe enthalten?"
        %   * Explizite Verifikation vor Generierung
        %   * Erhöht Token-Kosten, aber verbessert Zuverlässigkeit
        %
        % AUTOMATISIERTE VS. MANUELLE EVALUATION:
        % - Automatisiert (skalierbar, aber limitiert):
        %   * Entitätsvergleich: Schnell, deterministisch
        %   * Pattern-Matching: Regelbasiert, wartungsintensiv
        %   * Self-Consistency: Rechenintensiv, keine externe Referenz
        % - Manuell (präzise, aber teuer):
        %   * Human Evaluation: Gold Standard, aber nicht skalierbar
        %   * Stichprobenbasiert: Regelmäßige Quality Checks
        %   * Iterative Verbesserung: Fehleranalyse → Prompt-Anpassung
        % - Hybrid-Ansatz (optimal für Praxis):
        %   * Automatisierte Pre-Filter: Offensichtliche Fehler abfangen
        %   * Manuelle Review: Grenzfälle und Stichproben
        %   * Feedback-Loop: Erkannte Fehler → Prompt-Optimierung
        %
        % PRAKTISCHE ANSÄTZE FÜR RESSOURCENLIMITIERTE UMGEBUNGEN:
        % - Deterministische Temperatur (T=0) reduziert Halluzinationen \cite{berryman2024prompt}
        %   * Trade-off: Weniger kreativ, aber konsistenter
        %   * Geeignet für faktische Transformationen
        % - Kleinere Validierungsmodelle:
        %   * Leichtgewichtige NER-Modelle für Entitätsextraktion
        %   * Regelbasierte Checks statt LLM-as-a-Judge
        % - Cached Knowledge Integration:
        %   * Statische Wissensbasis im Prompt (kein RAG-Overhead)
        %   * Siehe Abschnitt~\ref{sec:2.3.3} für Knowledge Integration ohne RAG
        %
        % ÜBERGANG ZU KAPITEL 7 (EVALUATION):
        % - Theoretische Konzepte hier: Methodik und Prinzipien
        % - Praktische Anwendung in Kapitel~\ref{chap:7}: Konkrete Metriken und Ergebnisse
        % - Kombination automatisierter und manueller Validierung in Implementierung


