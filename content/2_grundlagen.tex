\section{Theoretische Grundlagen und Stand der Forschung}
\label{chap:2}

Die in Kapitel~\ref{chap:1} dargelegte Problemstellung der automatisierten Transformation von LVB-Verkehrsanweisungen erfordert fundierte Expertise im Bereich Natural Language Processing. Diese Transformation stellt keine triviale Übersetzungsaufgabe dar, sondern verlangt die simultane Berücksichtigung von Kontextabhängigkeit, fachlicher Präzision und allgemeiner Verständlichkeit bei der Konversion von Fachsprache in Allgemeinsprache \cite{navigli2018natural}.

Das vorliegende Kapitel strukturiert die theoretischen Grundlagen entlang dreier komplementärer Dimensionen. Abschnitt~\ref{sec:2.1} etabliert die NLP-Fundamente, beginnend mit der Transformer-Architektur als paradigmatischem Durchbruch in der Sprachverarbeitung, über das Pretraining-Finetuning-Paradigma bis hin zu deutschsprachigen Modellen, insbesondere der Entwicklung von Mistral zu LeoLM. Abschnitt~\ref{sec:2.2} fokussiert auf Anpassungsmethodik, wobei Parameter-effizientes Fine-Tuning durch Low-Rank Adaptation sowie Datenstrategien bei limitierten Ressourcen behandelt werden. Abschnitt~\ref{sec:2.3} adressiert Aspekte der Produktionsreife, einschließlich Quantisierung für Deployment unter Hardware-Constraints sowie Halluzination Prevention zur Sicherstellung von Faktentreue.

Der rote Faden verbindet theoretische Konzepte mit praktischen Constraints: Kapitel~\ref{chap:3} demonstriert, wie der Anwendungskontext technologische Entscheidungen determiniert, während Kapitel~\ref{chap:6} die Operationalisierung theoretischer Konzepte in der Implementierung darlegt. Das vorliegende Kapitel fundiert die entwickelte Lösung in etablierter Forschung und schafft die konzeptionelle Basis für die nachfolgenden Kapitel \cite{brown2020language}.

\subsection{Natural Language Processing für Verkehrsinformationen}
\label{sec:2.1}

Die Anwendung von Natural Language Processing im Verkehrssektor bewegt sich im Spannungsfeld zwischen universellen Sprachmodellen und domänenspezifischen Anforderungen. Während proprietäre Systeme wie GPT-4 oder Claude beeindruckende Generalfähigkeiten demonstrieren, erfordert die Verkehrsdomäne spezifische Terminologie, rechtliche Präzision und Lokalkontexte, die eine spezialisierte Adaption notwendig machen \cite{zhang2023towards}. Hinzu tritt die Notwendigkeit von Open-Source-Lösungen, motiviert durch Datenschutz-Grundverordnung-Konformität, Datensouveränität und Kosteneffizienz \cite{zhao2023survey}.

Die Transformation von Verkehrsinformationen erfordert simultanes Kontextverständnis, Stiladaption und Faktentreue. Der qualitative Unterschied zwischen einer knappen Feststellung wie \enquote{Linie 10 fährt nicht} und einer vollständigen Information \enquote{Straßenbahn der Linie 10 verkehrt aufgrund von Bauarbeiten...} illustriert die Notwendigkeit impliziten Wissens über Linientypen, Fahrzeugkategorien und Alternativrouten. Moderne Large Language Models auf Basis der Transformer-Architektur vereinen Flexibilität und Generalisierung, bringen jedoch neue Herausforderungen bezüglich Kontrolle und Faktentreue mit sich \cite{bommasani2021opportunities}.

Die nachfolgenden Unterabschnitte strukturieren diese Grundlagen: Abschnitt~\ref{sec:2.1.1} behandelt technische Fundamente, Abschnitt~\ref{sec:2.1.2} diskutiert verkehrsspezifische NLP-Anwendungen im Stand der Technik, und Abschnitt~\ref{sec:2.1.3} fokussiert auf deutschsprachige und domänenspezifische Modelle, insbesondere die LeoLM-Familie.

\subsubsection{Grundlagen der Sprachverarbeitung}
\label{sec:2.1.1}

Die automatisierte Verarbeitung und Transformation von Verkehrsinformationen stellt hohe Anforderungen an Natural Language Processing-Systeme. Moderne Ansätze der Sprachverarbeitung basieren auf der Transformer-Architektur, die seit ihrer Einführung im Jahr 2017 die Entwicklung von Sprachmodellen maßgeblich geprägt hat \cite{vaswani2017attention}. Das in dieser Arbeit verwendete Modell LeoLM-7B baut auf dieser Architektur auf und nutzt deren Vorteile für die Verarbeitung deutschsprachiger Texte.

Im Gegensatz zu früheren sequenziellen Ansätzen basiert die Transformer-Architektur auf dem Self-Attention-Mechanismus, der es ermöglicht, kontextabhängige Repräsentationen für jedes Token zu berechnen. Durch Multi-Head Attention können unterschiedliche Aspekte der Kontextbeziehungen erfasst werden \cite{vaswani2017attention}. Diese Architektur ermöglicht die parallele Verarbeitung von Sequenzen und führt zu deutlich schnelleren Trainingszeiten sowie besserer Skalierbarkeit.

Für die in dieser Arbeit behandelte Aufgabe der Textgenerierung und -transformation ist die Decoder-Only-Architektur besonders geeignet, da sie speziell für die autoregressive Erzeugung von Text konzipiert wurde \cite{radford2019language}. Diese Architektur bildet die Grundlage des verwendeten LeoLM-Modells.

Ein zentrales Konzept moderner NLP-Systeme ist das Pretraining auf großen, unlabeled Textkorpora. Durch Self-Supervised Learning entwickeln diese Modelle ein umfassendes Sprachverständnis inklusive Syntax, Semantik und implizitem Weltwissen \cite{devlin2019bert}.

Für die vorliegende Arbeit ist insbesondere die Mistral-7B-Architektur von Bedeutung, da sie die Grundlage für das verwendete LeoLM-Modell bildet. Mistral 7B ist ein hocheffizienter Decoder-Only-Transformer mit 7 Milliarden Parametern, der mehrere innovative Architekturmerkmale aufweist \cite{jiang2023mistral}. Grouped-Query Attention reduziert die Größe des Key-Value-Cache und ermöglicht schnellere Inferenz. Sliding Window Attention erlaubt die effiziente Verarbeitung langer Kontexte, während der Rolling Buffer Cache die Speichernutzung optimiert. Mit 7 Milliarden Parametern stellt Mistral einen Sweet Spot zwischen Modellleistung und Ressourceneffizienz dar und übertrifft in Benchmarks viele deutlich größere Modelle \cite{jiang2023mistral}.

Ein wichtiger Unterschied besteht zwischen Base Models und Instruct Models. Base Models wie Mistral-7B sind auf reines Language Modeling trainiert und setzen primär Texte fort, ohne notwendigerweise expliziten Anweisungen zu folgen. Instruct Models hingegen durchlaufen zusätzlich ein Instruction Tuning, bei dem sie mittels Supervised Fine-Tuning auf Instruktionsdatensätzen trainiert werden, um gezielt Anweisungen zu befolgen \cite{ouyang2022training}. Optional kann dieser Prozess durch Reinforcement Learning from Human Feedback weiter verfeinert werden.

Für deutschsprachige Anwendungen ist die LeoLM-Familie von besonderer Relevanz. Diese Modelle nutzen Mistral-7B als Basis und durchlaufen ein Continued Pretraining auf deutschen Textkorpora \cite{huggingface2024leolm}. Das in dieser Arbeit verwendete Modell leo-mistral-hessianai-7b ist eine Base-Variante, die speziell für die deutsche Sprache optimiert wurde. Diese Adaption kombiniert die architektonischen Vorteile und die Effizienz von Mistral mit erhöhter Sprachkompetenz im Deutschen.

Die Tokenization erfolgt bei Mistral mittels Byte Pair Encoding mit einem Vokabular von 32.000 Tokens, wobei LeoLM einen an die deutsche Morphologie angepassten Tokenizer verwendet, der besser mit Komposita, Umlauten und anderen sprachspezifischen Besonderheiten umgehen kann \cite{sennrich2016neural}.

Das Konzept des Transfer Learning bildet die theoretische Grundlage für die Nutzung vortrainierter Modelle in spezifischen Anwendungsdomänen. Transfer Learning bezeichnet den Wissenstransfer von einer Source Domain, in der das Modell vortrainiert wurde, zu einer Target Domain, für die es angepasst werden soll \cite{ruder2019transfer}. Dieser Ansatz reduziert den Bedarf an aufgabenspezifischen Trainingsdaten und die erforderliche Trainingszeit erheblich.

Das etablierte Pretrain-Finetune-Paradigma verläuft in zwei Phasen: Zunächst erfolgt das Pretraining auf großen, unlabeled Textkorpora mittels unsupervised Learning, wodurch das Modell grundlegendes Sprachverständnis erwirbt. In der zweiten Phase wird das Modell mittels Fine-Tuning auf eine spezifische Aufgabe angepasst, wobei supervised Learning mit aufgabenspezifischen Daten zum Einsatz kommt \cite{howard2018universal}. Im Kontext dieser Arbeit bedeutet dies die Spezialisierung auf die Transformation von LVB-Verkehrsanweisungen.

Die Vorteile von Transfer Learning sind vielfältig: Vortrainierte Gewichte dienen als optimaler Startpunkt und beschleunigen das Training erheblich. Das bereits vorhandene Sprachverständnis führt zu besserer Generalisierung auf neue Daten \cite{ruder2019transfer}. Dennoch bestehen Herausforderungen: Catastrophic Forgetting bezeichnet den Verlust vortrainierter Fähigkeiten während des Fine-Tunings. Der Domain Shift zwischen Pretraining-Daten und Zieldomäne kann zu Leistungseinbußen führen, insbesondere wenn sich Vokabular oder Sprachstil deutlich unterscheiden \cite{kirkpatrick2017overcoming}. Bei kleinen Datensätzen besteht zudem die Gefahr des Overfittings.

Für die vorliegende Arbeit ist besonders relevant, dass LeoLM bereits auf umfangreichen deutschen Textkorpora vortrainiert wurde, was eine solide Basis für die weitere Spezialisierung bildet. Das Fine-Tuning auf den vergleichsweise kleinen Datensatz der LVB-Verkehrsanweisungen wird durch Transfer Learning erst praktikabel und ermöglicht die notwendige domänenspezifische Anpassung an die fachsprachlichen Anforderungen des Verkehrssektors.

\subsubsection{NLP-Anwendungen im Verkehrssektor}
\label{sec:2.1.2}

Natural Language Processing fungiert als Enabler-Technologie für moderne Verkehrsmanagement-Strategien. Die technologische Evolution vollzog sich vom regelbasierten Mustererkennen zu bedeutungsverstehenden Large Language Models, wodurch die systematische Automatisierung der Informationsverarbeitung ermöglicht wurde \cite{zhang2020deep}. Die Anwendung von Deep Learning-Methoden im intelligenten Verkehrswesen umfasst dabei ein breites Spektrum von Aufgaben, darunter Verkehrsprognose, Unfallanalyse und die Verarbeitung natürlichsprachlicher Anfragen.

Die Kernrelevanz für die vorliegende Untersuchung manifestiert sich in der automatisierten Berichtserstellung und Dokumentenanalyse. Empirische Studien demonstrieren substantielle Fortschritte bei der Zusammenfassung von Unfallberichten und der erfolgreichen Klassifizierung der Unfall-Schwere aus Textbeschreibungen \cite{silva2021natural}. Zudem belegen Untersuchungen die Effektivität von LLMs bei der Analyse und Aufbereitung von Fahrgastanfragen. Eine systematische Literaturübersicht zu NLP-Anwendungen in intelligenten Verkehrssystemen zeigt die zunehmende Bedeutung sprachverarbeitender Methoden für die Transformation und Aufbereitung verkehrsbezogener Informationen.

Die Transformation technischer Informationen in Endnutzerformate erfordert simultane Berücksichtigung von Fachpräzision und Verständlichkeit. Diese Brückenfunktion zwischen technischer Fachsprache und Allgemeinverständlichkeit konstituiert die zentrale Aufgabe der Verkehrsanweisungs-Transformation und unterscheidet sich fundamental von reiner Klassifizierung durch ihre essenzielle generative Komponente \cite{gkatzia2016natural}.

Die empirischen Erfolge rechtfertigen die Hypothese, dass LLMs für Verkehrsanweisungs-Transformation geeignet sind. Gleichwohl persistieren domänenspezifische Herausforderungen: Spezialisiertes Vokabular erfordert gezielte Adaption, Faktentreue muss bei sicherheitskritischen Informationen absolute Priorität genießen, und Konsistenz stellt deterministische Anforderungen an probabilistische Systeme \cite{ji2023survey}.

\subsubsection{Domänenspezifische Sprachmodelle}
\label{sec:2.1.3}

Die Forschung dokumentiert konsistente Überlegenheit domänenspezifisch angepasster Modelle gegenüber General-Purpose-Systemen in verkehrsspezifischen Aufgaben \cite{lee2020biobert}. Existierende spezialisierte Systeme eignen sich jedoch nicht für deutschsprachige Fahrgastinformation.

Verschiedene spezialisierte Systeme wurden für den Verkehrssektor entwickelt, sind jedoch primär englischsprachig und auf andere Anwendungsfälle fokussiert. Die Systeme konzentrieren sich auf Verkehrssicherheit, chinesischsprachige Wissensvermittlung oder englischsprachige Query-Beantwortung mit Echtzeit-Datenbanken \cite{zhang2023towards}. Multimodale Modelle fokussieren auf autonome Fahrzeuge und Predictive Analytics.

Die systematische Analyse offenbart fünf Hindernisse für die Verwendung existierender Systeme: erstens die Sprachbarriere, da alle Systeme englisch- oder chinesischsprachig sind. Zweitens ein Aufgaben-Mismatch, da Safety, Prediction und Simulation nicht äquivalent zu Text-Transformation sind. Drittens Lizenzierungsprobleme, da meist proprietäre oder Closed-Source-Basis vorliegt. Viertens Infrastrukturanforderungen, da Echtzeit-Datenbanken erforderlich sind. Fünftens Deployment-Beschränkungen, da die Systeme nicht für lokale, offline-fähige Ausführung konzipiert sind.

Daraus erwächst die Notwendigkeit eigenständiger Modellentwicklung mit der Strategie: Deutsches Basismodell plus domänenspezifisches Fine-Tuning. Diese Rechtfertigung adressiert eine Forschungslücke bei deutschsprachiger Verkehrsinformation, deren detaillierte Begründung in Kapitel~\ref{chap:4} erfolgt. Die theoretische Überlegenheit domänenspezifischer Modelle etabliert, stellt sich nun die Frage: Wie erstellt man solche Modelle? Abschnitt~\ref{sec:2.2} behandelt Fine-Tuning-Methoden, Datenstrategien und Optimierungen \cite{gururangan2020dont}.

\subsection{Sprachmodelle und Fine-Tuning}
\label{sec:2.2}

Die Problemstellung der Modellanpassung manifestiert sich im Spannungsfeld zwischen allgemeinem Sprachverständnis und domänenspezifischer Expertise. Vortrainierte Sprachmodelle verfügen über umfassendes Verständnis natürlicher Sprache, erworben durch Training auf Milliarden von Tokens. Die Spezialisierung auf domänenspezifische Aufgaben erfordert jedoch gezielte Anpassung \cite{howard2018universal}.

Ressourcenbeschränkungen beim Full Fine-Tuning stellen prohibitive Barrieren dar. Die Anpassung sämtlicher Modellparameter erfordert Training auf Multi-GPU-Clustern mit Speicherbedarf über 100 GB VRAM \cite{houlsby2019parameter}. Die Constraint dieser Arbeit, nämlich Consumer-Hardware mit begrenzten Ressourcen, erzwingt die Anwendung parameter-effizienter Fine-Tuning-Methoden.

Das Risiko des Overfitting bei vollständiger Parameteranpassung mit kleinem Datensatz ist substanziell. Parameter-effiziente Verfahren bieten den Vorteil reduzierten Datenbedarfs durch gezielte Adaption und wirken dem Overfitting entgegen \cite{he2022towards}.

Die methodischen Ansätze gliedern sich in vier Kategorien: Parameter-effiziente Fine-Tuning-Verfahren reduzieren die Anzahl trainierbarer Parameter (Abschnitt~\ref{sec:2.2.1}). Prompt Engineering fungiert als komplementärer Ansatz zur Leistungssteigerung ohne Parametermodifikation (Abschnitt~\ref{sec:2.2.2}). Datenaufbereitung und Augmentation adressieren limitierte Datenverfügbarkeit (Abschnitt~\ref{sec:2.2.3}). Gegenmaßnahmen zu Overfitting und Catastrophic Forgetting sichern die Generalisierungsfähigkeit (Abschnitt~\ref{sec:2.2.4}).

\subsubsection{Fine-Tuning-Methoden}
\label{sec:2.2.1}

Das Training großer vortrainierter Sprachmodelle für spezifische Downstream-Aufgaben stellt eine fundamentale Herausforderung dar. Während das klassische Full Fine-Tuning alle Modellparameter anpasst, führt diese Strategie bei wachsenden Modellgrößen zu zunehmend unpraktikablen Ressourcenanforderungen \cite{houlsby2019parameter}. Ein Training auf Multi-GPU-Clustern kann mehrere Wochen dauern und Kosten im vier- bis fünfstelligen Bereich verursachen.

Als Antwort auf diese Herausforderung haben sich Parameter-Efficient Fine-Tuning-Methoden etabliert, die eine Modellspezialisierung mit drastisch reduzierten Rechenressourcen ermöglichen. Die PEFT-Landschaft lässt sich in drei Hauptkategorien einteilen \cite{ding2023parameter}: Additive Fine-Tuning fügt zusätzliche trainierbare Module zum gefrorenen Basismodell hinzu. Zu dieser Kategorie zählen Methoden wie Adapter, Prefix Tuning und Prompt Tuning. Reparameterized Fine-Tuning hingegen zerlegt Gewichtsupdates in niedrigrangige Matrizen. Der prominenteste Vertreter ist Low-Rank Adaptation, dessen entscheidender Vorteil darin besteht, dass durch Post-Training Weight Merging keine Inferenz-Latenz entsteht. Die dritte Kategorie, Selective Fine-Tuning, trainiert nur ausgewählte Teilmengen existierender Parameter und findet in dieser Arbeit keine Anwendung.

Low-Rank Adaptation basiert auf der Hypothese, dass Gewichtsänderungen während der Modelladaptation einen niedrigen intrinsischen Rang aufweisen \cite{hu2021lora}. Das Update wird als Produkt zweier niedrigrangiger Matrizen dargestellt: $\Delta W = BA$, wobei $B \in \mathbb{R}^{d \times r}$ und $A \in \mathbb{R}^{r \times k}$ mit $r \ll d,k$. Die finale Gewichtsmatrix ergibt sich als $W = W_0 + BA$, wobei $W_0$ die eingefrorenen Originalgewichte darstellt und der Rang $r$ als Hyperparameter die Balance zwischen Expressivität und Effizienz kontrolliert.

Während des Trainings bleiben die vortrainierten Gewichte eingefroren, und ausschließlich die Low-Rank-Matrizen werden trainiert. Die Effizienzgewinne von LoRA sind substanziell: Eine bis zu 10.000-fache Reduktion trainierbarer Parameter bei vergleichbarer Performance zu Full Fine-Tuning, dreifach reduzierter GPU-Memory-Bedarf, und 30-prozentige Reduktion der Trainingszeit \cite{hu2021lora}.

Die modulare Architektur bietet signifikante Deployment-Vorteile: Ein einzelnes Basismodell kann mit verschiedenen aufgabenspezifischen Adaptern kombiniert werden. Ein kritischer Vorteil liegt in der Möglichkeit des Weight Merging nach dem Training: Die trainierten Matrizen werden zu $W_{final} = W_0 + BA$ zusammengeführt, wodurch während der Inferenz kein zusätzlicher Overhead entsteht \cite{hu2021lora}.

Trotz dieser substanziellen Vorteile weist LoRA mehrere bedeutende Limitierungen auf. Die Konvergenzproblematik stellt eine zentrale Herausforderung dar: LoRA konvergiert signifikant langsamer als Full Fine-Tuning und benötigt empirisch fünf- bis sechsmal mehr Iterationen \cite{zhang2023adaptive}. Die Rank-Sensitivität erfordert sorgfältige Hyperparameter-Auswahl, wobei suboptimale Wahl kostspieliges Retraining erfordert. Der Performance-Gap zu Full Fine-Tuning verschärft sich bei komplexen Datensätzen mit diversen Sub-Domänen.

Eine wesentliche Erweiterung stellt Quantized Low-Rank Adaptation dar, das LoRA-Adapter mit 4-bit-Quantisierung der Basisgewichte kombiniert \cite{dettmers2023qlora}. Diese optimierte Implementation ermöglicht eine weitere Speicherreduktion von circa 70 Prozent ohne Qualitätsverlust und macht 7-Milliarden-Parameter-Modelle auf GPUs mit weniger als 24 GB VRAM trainierbar. Die detaillierte Diskussion der Quantisierungstechniken erfolgt in Abschnitt~\ref{sec:2.3.2}.

Die Konfiguration von LoRA erfordert die Festlegung mehrerer Hyperparameter. Der Rang bestimmt die Dimensionalität der Low-Rank-Matrizen, wobei typische Werte 4, 8, 16, 32 oder 64 betragen. Der Alpha-Parameter fungiert als Skalierungsfaktor für die LoRA-Updates. Die Target Modules definieren, welche Transformer-Layer angepasst werden, typischerweise Query-, Key-, Value- und Output-Projektionen sowie bei der Mistral-Architektur zusätzlich Gate-, Up- und Down-Projektionen \cite{hu2021lora}.

Adapter-Methoden stellen eines der frühesten Parameter-Efficient Fine-Tuning-Frameworks dar und folgen dem Paradigm des Additive Fine-Tuning. Der Kernansatz besteht darin, kleine aufgabenspezifische Module mit Feedforward-Layern in das gefrorene Basismodell einzufügen \cite{houlsby2019parameter}. Adapter erfordern lediglich 3,6 Prozent zusätzliche Parameter pro Task, verursachen jedoch strukturbedingte Inferenz-Latenz durch die zusätzlichen Layer.

Prefix Tuning und Prompt Tuning repräsentieren attention- bzw. input-basierte PEFT-Ansätze. Diese Methoden optimieren kontinuierliche, aufgabenspezifische Vektoren, die den Input-Embeddings vorangestellt werden, und erreichen extreme Parameter-Effizienz \cite{lester2021power}. Die zentrale Limitation liegt im Inferenz-Overhead durch zusätzliche Kontext-Tokens und hoher Sensitivität gegenüber der Initialisierung.

LoRA erzielt on-par oder bessere Performance als Full Fine-Tuning und übertrifft Prompt Tuning in Gesamtperformance, Memory-Effizienz und Flexibilität \cite{ding2023parameter}. Adapter zeigen konsistent starke Performance auf Benchmarks, leiden jedoch unter Inferenz-Latenz. Prefix Tuning demonstriert Superiorität in multilingualen Adaptations-Tasks, während Prompt Tuning Schwächen bei kleinen Modellen zeigt.

Die Wahl von LoRA für diese Arbeit begründet sich durch sechs Faktoren: erstens kein Inferenz-Overhead durch Weight Merging, kritisch für Produktionsumgebung. Zweitens optimale Balance zwischen Parameter-Effizienz und Performance. Drittens ermöglicht Modularität mehrere Task-Adapter. Viertens Hardware-Feasibility durch Kombination mit QLoRA für Consumer-GPUs. Fünftens bewährte Performance in ähnlichen Domänen-Adaptionen. Sechstens umfangreicher Community-Support mit etablierten Best Practices.

Auch PEFT-Methoden wie LoRA erfordern bei 7-Milliarden-Parameter-Modellen erhebliche Ressourcen. Unsloth adressiert diese Herausforderungen durch drei technische Optimierungskategorien \cite{unsloth2024github}. Erstens Speichereffizienz mit 50 bis 80 Prozent Reduktion des VRAM-Bedarfs: 4-bit Quantization der Base-Model-Weights erzielt circa 70 Prozent Memory-Reduktion. Die optimierte QLoRA-Implementation kombiniert LoRA-Adapter mit 4-bit Quantization. Dies ermöglicht 7-Milliarden-Parameter-Modelle auf Consumer-GPUs ohne Qualitätsverlust.

Zweitens Training-Beschleunigung durch Flash Attention 2 und Custom Kernel Implementations in OpenAI Triton \cite{dao2022flashattention}. Drittens numerische Stabilität durch Gradient Clipping, Layer Normalization Calibration und Multi-Precision Support.

Die Demokratisierung des Zugangs manifestiert sich in Hardware-Accessibility auf Single-GPU statt Multi-GPU, Consumer-Grade Hardware und Free Cloud Platforms. Praktische Implikationen für diese Arbeit umfassen: Iterationsgeschwindigkeit, Hardware-Feasibility und Stabilität.

\subsubsection{Prompt Engineering}
\label{sec:2.2.2}

Fine-Tuning realisiert permanente Modellanpassung durch Parameteränderung, während Prompt Engineering Verhaltenssteuerung ohne Gewichtsänderungen ermöglicht. Der Synergieeffekt manifestiert sich in der Kombination: Fine-Tuning etabliert domänenspezifische Fähigkeiten, Prompts steuern aufgabenspezifische Kontrolle \cite{reynolds2021prompt}.

Zero-Shot Prompting beschränkt sich auf reine Instruktion ohne Demonstrationen. Base Models wie LeoLM-7B zeigen jedoch schwache Zero-Shot-Fähigkeiten. Few-Shot Learning integriert Aufgabenbeschreibung und demonstrative Beispiele im Prompt \cite{brown2020language}. Der Mechanismus ist In-Context Learning: Das Modell erkennt Muster aus Input-Output-Paaren ohne Gewichtsanpassung. Empirische Evidenz dokumentiert dramatische Leistungssteigerung gegenüber Zero-Shot.

Die Beispielauswahl folgt spezifischen Prinzipien: Diversität gewährleistet Abdeckung verschiedener Störungstypen. Qualität dominiert Quantität, drei perfekte Beispiele übertreffen zehn durchschnittliche. Die Beispielreihenfolge berücksichtigt Recency Bias: Letzte Beispiele haben den stärksten Einfluss auf das Modellverhalten \cite{zhao2021calibrate}.

Instruction Prompting definiert sich als explizite Aufgabenbeschreibung vor den Beispielen. Effektive Instructions umfassen Rollenspezifikation, präzise Aufgabendefinition, Constraints bezüglich unerwünschten Verhaltens, Stilrichtlinien und Ausgabeformat \cite{ouyang2022training}.

Template-Strukturen umfassen vier Komponenten: System Prompt, Few-Shot Examples, User Prompt und optional Knowledge Context. Die Differenzierung zwischen Base Models und Instruction-Tuned Models ist essenziell: Base Models wie LeoLM-7B zeigen schwache Instruktionsbefolgung ohne Fine-Tuning, tendieren zur Textkontinuation statt Aufgabenlösung und benötigen Few-Shot Examples essentiell \cite{wei2022finetuned}.

Der Synergieeffekt mit Fine-Tuning manifestiert sich: Fine-Tuning lernt domänenspezifisches Vokabular und Muster, Prompts steuern aufgabenspezifische Nuancen. Die Kombination ist robuster als jede Methode einzeln und reduziert Overfitting-Risiko durch flexible Steuerung.

\subsubsection{Datensätze für Fine-Tuning}
\label{sec:2.2.3}

Qualität und Zusammenstellung von Trainingsdaten konstituieren kritische Erfolgsfaktoren beim Fine-Tuning. Besondere Herausforderungen manifestieren sich bei kleinen domänenspezifischen Datensätzen, wie sie in dieser Arbeit vorliegen \cite{kaplan2020scaling}. Transfer Learning reduziert den Datenbedarf substanziell, ausreichende Qualität bleibt jedoch essentiell für erfolgreiche Adaption.

Supervised Fine-Tuning erfordert strukturierte Input-Output-Paare. Format-Anforderungen variieren zwischen JSON, JSONL und modellspezifischen Formaten. Konsistenz in Struktur und Formatierung ist obligatorisch. Prompt-Template-basiertes Format-Design folgt dem Prinzip, dass Training-Daten das Inference-Format widerspiegeln sollten \cite{sanh2022multitask}.

Data Augmentation zielt auf Vergrößerung des Datensatzes ohne zusätzliche manuelle Annotation. Die Ziele umfassen Verbesserung der Generalisierung, Reduktion von Overfitting und Erhöhung der Robustheit \cite{wei2019eda}. Drei methodische Kategorien existieren.

Rule-Based Augmentation umfasst Synonym Replacement, Random Insertion, Random Swap und Random Deletion. Easy Data Augmentation kombiniert diese Techniken und ist effektiv bei kleinen Datensätzen \cite{wei2019eda}. Die empirische Evaluation zeigt, dass bereits einfache regelbasierte Augmentierungstechniken die Performance deutlich steigern können.

Model-Based Augmentation nutzt Back-Translation, Paraphrasing mit vortrainierten Modellen und template-basierte Generierung. Diese Methoden bieten höhere Qualität bei erhöhtem Rechenaufwand \cite{sennrich2016improving}.

Synthetic Data Generation erzeugt komplett neue Beispiele durch LLMs. Constraint-basierte Generation mit validierten Entitäten ermöglicht Kontrolle über Diversität und Abdeckung \cite{dai2023chataug}. Das Risiko liegt in Halluzinationen und unrealistischen Beispielen. Best Practice empfiehlt Kombination aus realen und synthetischen Daten.

Empirische Erkenntnisse zeigen: EDA ermöglicht mit 50 Prozent der Daten gleiche Accuracy wie 100 Prozent ohne Augmentierung \cite{wei2019eda}. Der Effekt verstärkt sich bei kleineren Datensätzen. Qualität dominiert Quantität bei synthetischen Daten.

Unbalancierte Datensätze verursachen systematische Probleme: Modelle tendieren zu häufigen Klassen und zeigen schlechtere Performance auf seltenen aber wichtigen Fällen. Strategien für Balance umfassen Oversampling unterrepräsentierter Kategorien, SMOTE-ähnliche Ansätze und class-weighted Loss Functions \cite{chawla2002smote}. Empfohlen ist ein maximales Verhältnis von 1:3 zwischen seltenster und häufigster Kategorie.

Als klein gelten Datensätze mit typischerweise weniger als 1000 Beispielen für Fine-Tuning. Data Efficiency Techniques umfassen aggressive Augmentierung unter Wahrung der Qualität, parameter-effiziente Methoden wie LoRA zur Reduktion des Overfitting-Risikos, niedrigere Learning Rates und Early Stopping basierend auf Validation Loss \cite{dodge2020fine}.

Mixed Task Training kombiniert domänenspezifische und allgemeine Daten. Das Verhältnis liegt typisch bei 85 bis 90 Prozent spezifisch, 10 bis 15 Prozent allgemein. Dies verhindert Catastrophic Forgetting und erhält Generalisierungsfähigkeit \cite{mccloskey1989catastrophic}.

Standard-Aufteilung: 70 bis 80 Prozent Training, 10 bis 15 Prozent Validation, 10 bis 15 Prozent Test. Bei kleinen Datensätzen: 80-10-10 oder Cross-Validation. Stratified Split bei kategorischen Daten erhält Balance. Vermeidung von Data Leakage zwischen Splits ist kritisch.

Qualitätssicherung umfasst automatisierte Validierung, manuelle Stichprobenprüfung und iterative Verbesserung basierend auf Modellfehlern \cite{sambasivan2021everyone}.

\subsubsection{Herausforderungen beim Fine-Tuning}
\label{sec:2.2.4}

Fine-Tuning vortrainierter Modelle manifestiert spezifische Herausforderungen im Spannungsfeld zwischen Spezialisierung auf neue Aufgaben und Erhalt allgemeiner Fähigkeiten. Besonders kritisch sind diese bei kleinen domänenspezifischen Datensätzen \cite{mosbach2021stability}.

Overfitting manifestiert sich, wenn das Modell Trainingsdaten auswendig lernt statt Muster zu generalisieren. Charakteristische Symptome sind hohe Training Accuracy bei niedriger Validation bzw. Test Accuracy. Besonders ausgeprägt ist Overfitting bei kleinen Datensätzen. LoRA reduziert das Overfitting-Risiko gegenüber Full Fine-Tuning, eliminiert es jedoch nicht \cite{hu2021lora}.

Die Ursachen umfassen: Modellkomplexität übersteigt Informationsgehalt der Trainingsdaten, zu viele Trainings-Epochen, unbalancierte Datensätze und zu hohe Learning Rate. Bei LoRA-Fine-Tuning spezifisch: Höherer Rank erhöht trainierbare Parameter und damit Overfitting-Risiko.

Vermeidungsstrategien gliedern sich in fünf Kategorien: Regularisierung nutzt L2-Regularisierung zur Bestrafung großer Gewichtswerte und Dropout \cite{srivastava2014dropout}. Early Stopping monitort kontinuierlich Validation Loss während Training und stoppt Training wenn Validation Loss stagniert oder ansteigt \cite{prechelt1998early}. Datenaugmentierung vergrößert den effektiven Datensatz künstlich und reduziert Overfitting durch erhöhte Variabilität. Cross-Validation nutzt K-Fold Cross-Validation bei sehr kleinen Datensätzen für robustere Schätzung der Generalisierungsfähigkeit. PEFT-spezifische Strategien: Niedrigerer LoRA-Rank reduziert trainierbare Parameter, selektive Target Modules passen nur kritische Layer an.

Catastrophic Forgetting beschreibt das Phänomen, dass Modelle während Fine-Tuning Fähigkeiten aus der Pretraining-Phase verlieren \cite{kirkpatrick2017overcoming}. Spezialisierung auf neue Aufgaben geht zu Lasten allgemeiner Kompetenzen. Bei LoRA ist dies weniger ausgeprägt als bei Full Fine-Tuning, aber vorhanden.

Manifestationen umfassen Wissensverlust, Sprachkompetenz-Degradation und Task Interference. Gegenmaßnahmen umfassen niedrigere Learning Rates, LoRA als sanftere Alternative zu Full Fine-Tuning und Mixed Task Training \cite{mccloskey1989catastrophic}.

LoRA zeigt inhärente Konvergenz-Limitierungen: Langsame Konvergenz erfordert fünf- bis sechsmal mehr Iterationen als Full Fine-Tuning \cite{zhang2023adaptive}. Die Low-Rank-Constraint limitiert die Expressivität der Gewichtsanpassungen. Trade-off zwischen Rank und Konvergenzgeschwindigkeit ist omnipräsent.

Performance-Gap zu Full Fine-Tuning ist konsistent dokumentiert, besonders ausgeprägt bei komplexen Datensätzen mit diversen Sub-Domänen. Der Gap verringert sich mit größeren Basismodellen, höherem LoRA-Rank und längeren Trainings-Episoden.

\subsection{Ressourceneffizienz und Modelloptimierung}
\label{sec:2.3}

Die in Abschnitt 2.2 dargestellten Fine-Tuning-Methoden ermöglichen die gezielte Anpassung von Large Language Models an spezifische Anwendungsdomänen. Während diese Verfahren die technische Grundlage schaffen, stellt sich unmittelbar die Frage nach der praktischen Realisierbarkeit: Wie können LLMs ressourceneffizient in produktiven Umgebungen deployed werden, insbesondere auf Consumer-Hardware mit begrenzten Ressourcen \cite{schwartz2020green}.

Das Spannungsfeld zwischen Modellperformance und Ressourcenverbrauch prägt die praktische Anwendung von Large Language Models in erheblichem Maße. Die Balance zwischen Qualität und Effizienz berührt dabei nicht nur technische, sondern auch wirtschaftliche und ökologische Aspekte der Modellentwicklung und -nutzung \cite{strubell2019energy}.

Die Ressourceneffizienz lässt sich in drei wesentliche Dimensionen untergliedern: Computational Resources umfassen die hardwareseitigen Anforderungen an Arbeitsspeicher, Rechenleistung und spezialisierte Hardware-Komponenten. Environmental Impact bezieht sich auf den Energieverbrauch und die damit verbundene CO2-Bilanz. Economic Barriers manifestieren sich in den Kosten für Training, Inferenz und Deployment von Sprachmodellen \cite{bender2021dangers}.

\subsubsection{Problematik großer Sprachmodelle}
\label{sec:2.3.1}

Große Sprachmodelle sind mit fundamentalen Ressourcenproblemen konfrontiert. Die Skalierung auf Milliarden von Parametern schafft signifikante Barrieren für Deployment und praktische Nutzung \cite{brown2020language}.

Moderne große Sprachmodelle erfordern außergewöhnliche Speicherkapazitäten. GPT-3 mit 175 Milliarden Parametern benötigt über 326 Gigabyte Speicher im FP16-Format \cite{brown2020language}. Diese Anforderung überschreitet die Kapazität selbst fortgeschrittener Grafikprozessoren bei weitem. Der zusätzliche Speicheraufwand durch Backpropagation zur Gradientenberechnung stellt insbesondere für das Training auf Endgeräten ein erhebliches Problem.

Die hohe Rechenkomplexität zeigt sich in Training und Inferenz. Bei verteiltem Training entsteht zusätzliche Komplexität durch die Vielzahl konfigurierbarer Strategien \cite{rajbhandari2020zero}. Konventionelle Optimierungsalgorithmen benötigen massive Rechenressourcen und weisen mangelnde Flexibilität bei der Parameteranpassung auf.

Die signifikanten Ressourcenanforderungen machen Cloud-Hosting häufig notwendig. Dies erfordert umfangreiche GPU-Cluster und verursacht substanzielle Kosten im vier- bis fünfstelligen Bereich. Für Forschende und kleine Organisationen entstehen dadurch erhebliche Zugangsbeschränkungen \cite{bender2021dangers}.

Training und Deployment großer Sprachmodelle sind aufgrund ihrer Größe und Komplexität energieintensiv. Das Training von GPT-3 verbrauchte etwa 700.000 Liter Frischwasser für die Wasserkühlung und verursachte Kosten von über 100 Millionen US-Dollar \cite{li2023making}. Die rasche Verbreitung großer Sprachmodelle führt zu signifikantem Energieverbrauch und CO2-Emissionen. Projektionen zeigen, dass der Energieverbrauch von Rechenzentren bis 2030 etwa 9,1 bis 11,7 Prozent des gesamten Energiebedarfs der Vereinigten Staaten ausmachen könnte \cite{strubell2019energy}.

In der Literatur wird zwischen Red AI und Green AI unterschieden \cite{schwartz2020green}. Ein Paradigmenwechsel hin zu nachhaltiger Entwicklung erscheint erforderlich. Die Forschung zu energieeffizienten Ansätzen im maschinellen Lernen hat gezeigt, dass durch gezielte Optimierungen substantielle Reduktionen des Ressourcenbedarfs möglich sind.

Das Training großer Sprachmodelle erfordert signifikante Rechenressourcen und umfangreiche Datensätze. Dies führt zu eskalierenden Forschungs- und Entwicklungskosten \cite{bender2021dangers}. Die hohen Rechenkosten führen dazu, dass fortgeschrittene künstliche Intelligenz nur für gut finanzierte Organisationen verfügbar ist.

Die finanzielle Belastung beschränkt sich nicht auf das initiale Training, sondern umfasst auch laufende Betriebskosten. In industriellen Deployments entstehen exorbitante Betriebskosten durch explosiven Overhead bei API-Aufrufen.

Cloud-Deployment wirft Datenschutzbedenken auf, verursacht Latenzprobleme und führt zu Nutzungsbeschränkungen. Edge-Deployment steht vor inhärenten Beschränkungen durch eingeschränkte Rechenleistung sowie begrenzten Speicher \cite{zhou2019edge}.

Für den Anwendungsfall kommunaler Organisationen ergeben sich spezifische Implikationen. Kommunale Einrichtungen verfügen über begrenzte Budgets und Ressourcen. Die Anforderungen der Datenschutz-Grundverordnung favorisieren lokales Deployment gegenüber Cloud-Lösungen. Die Beschränkung auf Consumer-Hardware schließt Enterprise-Grade-Infrastruktur aus. Diese Problemstellung erfordert Lösungsansätze, die eine effiziente Nutzung großer Sprachmodelle unter den genannten Rahmenbedingungen ermöglichen.

\subsubsection{Quantisierung}
\label{sec:2.3.2}

Quantisierung bezeichnet die Reduktion der numerischen Präzision von Modellparametern und stellt eine zentrale Optimierungstechnik zur Effizienzsteigerung großer Sprachmodelle dar \cite{gholami2022survey}. Das Grundprinzip besteht in der Umwandlung von 32-Bit-Gleitkommazahlen zu niedrigpräzisen 8-Bit- oder 4-Bit-Ganzzahldarstellungen. Diese Transformation verfolgt einen dualen Zweck: die Reduktion des Speicherbedarfs und die Beschleunigung der Berechnungen. Neuronale Netze erweisen sich als überparametrisiert und tolerieren daher einen gewissen Präzisionsverlust ohne signifikante Leistungseinbußen.

Die Quantisierung kann unterschiedlich auf Gewichte und Aktivierungen angewendet werden. Bei W8A8 werden sowohl Gewichte als auch Aktivierungen mit 8 Bit dargestellt. W4A16 nutzt 4-Bit-Gewichte bei 16-Bit-Aktivierungen, während W4A8 4-Bit-Gewichte mit 8-Bit-Aktivierungen kombiniert \cite{dettmers2022llmint8}. Die Quantisierung von Gewichten gestaltet sich einfacher als die von Aktivierungen, da letztere höhere Varianz und Ausreißer aufweisen.

Die Darstellung in 32-Bit Floating Point erfordert 4 Bytes pro Parameter, während 8-Bit Integer lediglich 1 Byte benötigt. Dies resultiert in einer vierfachen Kompression \cite{jacob2018quantization}. Empirische Untersuchungen zeigen eine Reduktion der Rechenkosten um 40 Prozent und der Modellgröße um 68 Prozent. Die Leistung bleibt dabei innerhalb von 6 Prozent der FP32-Baseline.

INT8-Quantisierung nutzt ausschließlich Ganzzahlarithmetik, was zu einer Geschwindigkeitssteigerung um den Faktor 2 bis 3 führt. INT8-GEMM-Operationen erweisen sich als 1,67-mal schneller als FP32 \cite{jacob2018quantization}. Auf Edge-Geräten zeigt sich eine Durchsatzsteigerung um den Faktor 2,4.

INT8-Quantisierung reduziert den Energieverbrauch um 40 Prozent. Die Framerate verbessert sich um 27 Prozent, während die Inferenzzeit um 19 Prozent sinkt. Diese Eigenschaften erweisen sich als essenziell für mobile, eingebettete und Edge-Plattformen.

Theoretisch ermöglicht 4-Bit Integer-Quantisierung eine 16-fache Reduktion des Speicherbedarfs, praktisch werden jedoch Faktoren zwischen 4 und 8 erreicht \cite{frantar2023gptq}. Moderne Methoden realisieren eine Speicherreduktion um den Faktor 5,6. Dies ermöglicht die Ausführung von Modellen mit 7 Milliarden Parametern auf Consumer-Laptops.

INT4-Quantisierung führt zu einer Beschleunigung um den Faktor 1,45 gegenüber INT8. Mixed-Precision-Ansätze mit INT4 und INT8 reduzieren die Latenz um 23 Prozent gegenüber reiner INT8-Quantisierung.

Post-Training-INT4-Quantisierung steht jedoch vor Schwierigkeiten, die ursprüngliche Modellqualität zu erreichen \cite{frantar2023gptq}. Das Risiko für Genauigkeitseinbußen ist bei INT4 höher als bei INT8. GPTQ erfordert eine ausgefeilte Kalibrierung zur Erhaltung der Genauigkeit.

Die Überparametrisierung vieler neuronaler Netze ermöglicht einen minimalen Genauigkeitsverlust bei Quantisierung. INT8-Quantisierung verursacht einen Genauigkeitsverlust von unter 1 Prozent bei vierfacher Kompression \cite{gholami2022survey}. INT4-Quantisierung stellt höhere Anforderungen an die Genauigkeitserhaltung.

Die Quantisierung von Aktivierungen erweist sich als anfälliger für Qualitätsverluste als die Quantisierung von Gewichten. Ausschließliche Gewichtsquantisierung erhält die Präzision der Aktivierungen und gewährleistet numerische Stabilität.

Post-Training-Quantisierung zeichnet sich durch schnelle Ausführung und Ressourceneffizienz aus, weist jedoch höhere Genauigkeitseinbußen auf. Quantization-Aware Training erhält die Genauigkeit besser, verursacht jedoch höheren Aufwand \cite{jacob2018quantization}. QLoRA kombiniert ein 4-Bit-Basismodell mit höherpräzisen LoRA-Adaptern \cite{dettmers2023qlora}. Dieser hybride Ansatz verbindet Speichereffizienz während der Feinabstimmung mit Qualitätserhaltung.

INT8-Quantisierung lässt sich unkompliziert deployen und verfügt über gute Framework-Unterstützung. INT4-Quantisierung steht vor begrenzter Software-Infrastruktur. Die Leistungsgewinne hängen von Hardware-Faktoren wie Speicherbandbreite, Cache und Befehlssatzunterstützung ab \cite{gholami2022survey}.

Für den Anwendungsfall dieser Arbeit ergibt sich folgende Bewertung: INT8-Quantisierung bietet einen optimalen Kompromiss mit vierfacher Speicherreduktion, zwei- bis dreifacher Geschwindigkeitssteigerung und Genauigkeitsverlusten unter 1 Prozent. INT4-Quantisierung ermöglicht zusätzliche Einsparungen um den Faktor 5,6, birgt jedoch höhere Genauigkeitsrisiken. QLoRA kombiniert ein 4-Bit-Basismodell mit höherpräzisen Adaptern und balanciert Effizienz mit Qualität.

\subsubsection{Weitere Optimierungsansätze}
\label{sec:2.3.3}

Neben Fine-Tuning und Quantisierung existieren weitere Ansätze zur Optimierung von Large Language Models für spezifische Anwendungsdomänen.

Das Konzept der Retrieval-Augmented Generation stellt einen hybriden Ansatz dar, der die generativen Fähigkeiten von Large Language Models mit externen Wissensdatenbanken kombiniert \cite{lewis2020retrieval}. Die grundlegende Idee besteht darin, relevante Informationen dynamisch aus einer strukturierten Wissensbasis abzurufen und diese dem Sprachmodell als Kontext zur Verfügung zu stellen.

Der technische Ablauf unterteilt sich in zwei Phasen. In der Retrieval-Phase wird eine semantische Suche in einer Vektorendatenbank durchgeführt. In der Generation-Phase werden die abgerufenen Informationen zusammen mit der ursprünglichen Anfrage an das Large Language Model übergeben.

Die Vorteile manifestieren sich in effizienter Skalierung mit der Größe der Wissensbasis und kontinuierlicher Aktualisierbarkeit ohne erneutes Training \cite{lewis2020retrieval}. Die Implementierung bringt jedoch erhebliche Herausforderungen mit sich. Die technische Komplexität ist deutlich höher als bei einfacheren Ansätzen, da neben dem Sprachmodell zusätzliche Komponenten wie Embedding-Modelle und Vektorendatenbanken betrieben werden müssen.

Als praktikable Alternative hat sich das Knowledge-Enhanced Prompting etabliert. Dieser Ansatz verzichtet auf dynamische Retrieval-Mechanismen und integriert stattdessen relevantes Domänenwissen statisch in die System-Prompts \cite{liu2023pretraining}. Die Implementierung gestaltet sich deutlich unkomplizierter als bei RAG-Systemen. Domänenspezifisches Wissen wird direkt in den Prompt eingebettet, ohne dass zusätzliche Infrastruktur erforderlich ist.

Die Menge an Wissen ist jedoch durch das Context-Window des Sprachmodells begrenzt. Zudem fehlt die Dynamik eines RAG-Systems: Aktualisierungen erfordern manuelle Anpassungen der Prompts. Für Anwendungsfälle im Verkehrssektor eignet sich Knowledge-Enhanced Prompting besonders dann, wenn die relevante Wissensbasis überschaubar und relativ stabil ist.

\subsection{Qualitätssicherung bei KI-generierten Texten}
\label{sec:2.4}

Die in den Abschnitten 2.2 und 2.3 diskutierten Optimierungsverfahren schaffen die technische Grundlage für die Anpassung von Large Language Models an spezifische Anwendungsdomänen. Damit stellt sich jedoch eine entscheidende Frage: Wie kann sichergestellt werden, dass die generierten Texte qualitativ hochwertig sind und den Anforderungen einer produktiven Anwendung genügen \cite{dhuliawala2023chain}.

Die Qualitätssicherung bei LLM-generierten Texten stellt besondere Herausforderungen dar. Während deterministische Systeme bei identischen Eingaben reproduzierbare Ausgaben liefern, ist das Verhalten generativer Modelle durch ihre stochastische Natur geprägt. Dieser Non-Determinismus birgt das Risiko inkonsistenter Outputs und erfordert spezifische Evaluationsstrategien \cite{ji2023survey}.

Im Kontext von Verkehrsinformationen ist die Faktentreue nicht optional, sondern essentiell. Falsche oder inkonsistente Angaben zu Liniennummern, Haltestellen oder Betriebszeiten können direkte Auswirkungen auf die Reiseplanung der Fahrgäste haben. Die Balance zwischen automatisierter Evaluation und manueller Qualitätskontrolle stellt dabei eine zentrale methodische Herausforderung dar.

Halluzinationen bezeichnen das Phänomen, dass Sprachmodelle plausibel klingende, aber faktisch falsche Informationen generieren \cite{maynez2020faithfulness}. Besonders problematisch ist dabei, dass halluzinierte Informationen oft syntaktisch und stilistisch korrekt formuliert sind und sich nur durch Abgleich mit den strukturierten Eingabedaten als fehlerhaft identifizieren lassen.

Inkonsistenz resultiert aus der stochastischen Natur von LLMs und manifestiert sich in variierenden Outputs bei identischen Inputs. Der Trade-off zwischen Kreativität und Determinismus muss durch geeignete Hyperparameter wie die Temperature gesteuert werden \cite{holtzman2020curious}.

Stilabweichungen treten auf, wenn generierte Texte von etablierten Formulierungsrichtlinien abweichen. Unvollständigkeit bezeichnet das selektive Weglassen wichtiger Informationen aus der Eingabe. Insbesondere bei sicherheitsrelevanten Angaben ist ein Informationsverlust durch Zusammenfassung kritisch.

Automatische Metriken ermöglichen eine skalierbare Evaluation großer Textmengen, sind jedoch in ihrer Aussagekraft limitiert \cite{celikyilmaz2021evaluation}. Oberflächliche Metriken wie BLEU und ROUGE messen lediglich N-gram Overlaps. Semantische Metriken wie BERTScore beruhen auf Embedding-basierten Vergleichen.

Für domänenspezifische Anforderungen sind spezialisierte Validierungsverfahren erforderlich. Self-Consistency-Ansätze generieren mehrere Outputs für denselben Input und bilden einen Konsens \cite{wang2023selfconsistency}. Fact-Checking-Verfahren validieren die generierten Texte gegen strukturierte Eingabedaten. Prompt-basierte Präventionsstrategien setzen bereits bei der Generierung an, indem sie Constraints im System Prompt definieren.

\subsubsection{Evaluationsmetriken für NLP}
\label{sec:2.4.1}

Die Evaluation von Textgenerierungsmodellen erfordert geeignete Metriken zur Quantifizierung der Ausgabequalität. Im Folgenden werden die drei zentralen Metriken BLEU, ROUGE und BERTScore erläutert.

BLEU wurde für die Evaluation maschineller Übersetzungssysteme entwickelt und basiert auf dem Vergleich von N-Gramm-Übereinstimmungen zwischen generiertem Text und Referenztext \cite{papineni2002bleu}. Die Metrik berechnet die Präzision übereinstimmender N-Gramme verschiedener Längen und kombiniert diese mittels geometrischem Mittel. Die Stärke liegt in der schnellen Berechenbarkeit und der Unabhängigkeit von menschlichen Annotatoren. Allerdings erfasst BLEU lediglich oberflächliche Textübereinstimmungen und kann semantische Äquivalenz nicht adäquat bewerten.

ROUGE wurde für die automatische Evaluation von Textzusammenfassungen entwickelt \cite{lin2004rouge}. ROUGE-N misst die Übereinstimmung von N-Grammen, wobei im Gegensatz zu BLEU der Recall betont wird. ROUGE-L basiert auf der längsten gemeinsamen Teilsequenz zwischen generiertem und Referenztext. Die Fokussierung auf Recall macht ROUGE besonders geeignet für die Evaluation von Zusammenfassungen, bei denen die Vollständigkeit der Informationsabdeckung zentral ist.

BERTScore adressiert die Limitationen oberflächlicher N-Gramm-Metriken durch die Nutzung kontextualisierter Worteinbettungen \cite{zhang2019bertscore}. Die Metrik berechnet die Kosinus-Ähnlichkeit zwischen den BERT-Embeddings der Token im generierten und im Referenztext. Die Verwendung kontextualisierter Embeddings ermöglicht die Erfassung semantischer Äquivalenz jenseits exakter lexikalischer Übereinstimmungen. BERTScore korreliert stärker mit menschlichen Qualitätsurteilen.

\subsubsection{Halluzination Detection und Validierung}
\label{sec:2.4.2}

Halluzinationen bezeichnen die Generierung faktisch inkorrekter oder erfundener Informationen durch Sprachmodelle. Diese Problematik erweist sich als besonders kritisch bei sicherheitsrelevanten Verkehrsinformationen \cite{ji2023survey}.

Self-Consistency beschreibt einen Ansatz zur Qualitätssicherung durch mehrfache Generierung und Konsensanalyse \cite{wang2023selfconsistency}. Das Grundprinzip besteht darin, für dieselbe Eingabe mehrere verschiedene Outputs zu generieren und diese auf Übereinstimmung zu prüfen.

Der Mechanismus umfasst die Generierung von N verschiedenen Outputs für identische Eingaben, typischerweise N zwischen 3 und 10. Das Sampling erfolgt mit einer Temperatur größer null, um Variabilität in den Ausgaben zu ermöglichen. Die zugrunde liegende Annahme besagt, dass korrekte Antworten konvergieren, während Halluzinationen divergieren.

Die Limitation besteht im N-fachen Rechenaufwand durch mehrfache Inferenz. Für die vorliegende Arbeit wird ein modifizierter Self-Consistency-Ansatz implementiert: Eine erste Instanz generiert den initialen Output mit niedriger Temperatur. Eine zweite Instanz desselben Modells prüft diesen Output anhand expliziter Validierungskriterien und korrigiert identifizierte Fehler.

Die strukturierte Validierung generierter Texte erfolgt durch Extraktion und Vergleich von Entitäten \cite{dhuliawala2023chain}. Aus Input und Output werden relevante Entitäten extrahiert und gegenübergestellt. Eine fundamentale Regel besagt, dass der Output keine Entitäten enthalten darf, die nicht im Input vorhanden sind.

Named Entity Recognition ermöglicht die Extraktion spezifischer Informationen wie Liniennummern, Haltestellen, Zeitangaben und Fahrzeugtypen. Dies kann mittels Pattern-Matching oder dedizierter NER-Modelle erfolgen. Constraint-basierte Validierung nutzt einen Whitelist-Ansatz, bei dem ausschließlich bekannte Liniennummern zugelassen werden.

Prompt-Engineering bietet Mechanismen zur präventiven Reduktion von Halluzinationen \cite{arora2023pal}. Negative Constraints im System-Prompt explizieren unerwünschte Verhaltensweisen. Typische Formulierungen umfassen die strikte Anweisung, keine Details zu erfinden, die nicht in der Eingabe enthalten sind, und ausschließlich Informationen aus dem bereitgestellten Kontext zu nutzen.

Knowledge Grounding beschreibt die explizite Einbettung von Fakten im Prompt. Diese Technik reduziert Halluzinationen durch direkte Verfügbarkeit relevanten Wissens im Kontext.

Die Temperatur-Parameter der Generierung beeinflusst die Zufälligkeit der Token-Selektion. Eine Temperatur nahe null induziert deterministische Generierung, bei der stets die wahrscheinlichsten Token gewählt werden \cite{holtzman2020curious}. Dies reduziert Halluzinationen, da unwahrscheinliche und potenziell inkorrekte Token-Sequenzen vermieden werden.

Der Trade-off besteht in reduzierter Kreativität bei erhöhter Konsistenz. Für faktische Transformationen, bei denen Korrektheit gegenüber Variabilität priorisiert wird, erweist sich dieser Ansatz als geeignet. In der vorliegenden Arbeit wird die Temperatur auf null gesetzt, um maximale Determinismus und Reproduzierbarkeit zu gewährleisten.

Automatisierte Evaluationsverfahren bieten Skalierbarkeit, weisen jedoch Limitationen auf. Entitätsvergleich erfolgt schnell und deterministisch. Pattern-Matching basiert auf Regeln, erfordert jedoch kontinuierliche Wartung.

Manuelle Evaluation durch menschliche Annotatoren stellt den Gold Standard dar, skaliert jedoch nicht für große Datenmengen \cite{celikyilmaz2021evaluation}. Ein hybrider Ansatz kombiniert die Vorteile beider Methoden. Automatisierte Pre-Filter fangen offensichtliche Fehler ab. Manuelle Review fokussiert auf Grenzfälle und Stichproben. Dieser Ansatz erweist sich als optimal für praktische Anwendungen unter Ressourcenbeschränkungen.

Die theoretischen Konzepte dieses Abschnitts bilden die methodische Grundlage für die praktische Evaluation. Die konkrete Anwendung der beschriebenen Techniken sowie die Messung ihrer Effektivität anhand spezifischer Metriken erfolgt in Kapitel 7.