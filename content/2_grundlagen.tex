\section{Theoretische Grundlagen und Stand der Forschung} \label{chap:2}

% Ziel: 16-18 Seiten

% ============================================================================
% AGENT-TODO: FORMULATE TEXTS FROM BULLET POINTS
% ============================================================================
% Instructions for agent task:
% 1. Go through ALL subsections with bullet points (marked with %)
% 2. Formulate complete, scientific paragraphs from these bullet points
% 3. Use ONLY sources already mentioned in bullet points (marked with [SOURCE-MISSING])
% 4. DO NOT add new sources or citations on your own
% 5. Keep all [SOURCE-MISSING] tags in the formulated text where citations belong
% 6. Maintain cross-references (\ref{}) to other chapters
% 7. Keep scientific style
%
% Sections to formulate:
% - Kapitel-Einleitung (lines ~5-33)
% - 2.1 Abschnitt-Einleitung (lines ~36-65)
% - 2.1.2 NLP-Anwendungen im Verkehrssektor (lines ~115-203)
% - 2.1.3 Domänenspezifische Sprachmodelle (lines ~205-366)
% - 2.2.1 Fine-Tuning-Methoden (partial - LoRA, other PEFT methods)
% - 2.2.2 Prompt Engineering (complete)
% - 2.2.3 Datensätze (partial - needs completion)
% - 2.2.4 Overfitting & Catastrophic Forgetting (complete)
% - 2.3.1 Quantisierung (complete)
% - 2.3.2 Halluzination Prevention (complete)
% - 2.3.3 RAG & Knowledge Integration (complete)
%
% Do NOT formulate:
% - Already written paragraphs (e.g., 2.1.1 Transformer-Architektur)
% - Sections with only structural comments
% ============================================================================

Die in Kapitel~\ref{chap:1} vorgestellte Problemstellung der Leipziger Verkehrsbetriebe erfordert fundierte Natural Language Processing-Expertise, da die Transformation von fachsprachlichen Verkehrsanweisungen in allgemeinverständliche Fahrgastinformationen keine triviale Übersetzungsaufgabe darstellt. Vielmehr müssen Kontextabhängigkeit, semantische Präzision und kommunikative Verständlichkeit simultan gewährleistet werden, was moderne Sprachverarbeitungsmethoden unerlässlich macht.

Das vorliegende Kapitel legt die theoretischen Grundlagen für die automatisierte Texttransformation und gliedert sich in drei aufeinander aufbauende Themenkomplexe. Abschnitt~\ref{sec:2.1} behandelt zunächst die NLP-Fundamente, wobei die Transformer-Architektur als paradigmatischer Durchbruch im Bereich der Sprachverarbeitung eingeführt wird. Das Pretraining-Finetuning-Paradigma wird erörtert, bevor deutschsprachige Modelle von Mistral bis LeoLM betrachtet werden, welche die sprachliche Basis für die vorliegende Arbeit bilden.

Abschnitt~\ref{sec:2.2} widmet sich der Anpassungsmethodik vortrainierter Sprachmodelle an domänenspezifische Aufgaben. Parameter-effiziente Fine-Tuning-Verfahren wie LoRA werden vorgestellt, ergänzt durch das Unsloth-Optimierungsframework, das Training unter Ressourcenbeschränkungen ermöglicht. Datenstrategien bei limitierter Datenverfügbarkeit sowie Prompt Engineering als komplementärer Ansatz zur Verhaltenssteuerung von Sprachmodellen runden diesen Abschnitt ab.

Der dritte Themenkomplex in Abschnitt~\ref{sec:2.3} fokussiert auf Produktionsreife und praktische Einsatzfähigkeit. Quantisierung als Technik für das Deployment unter Hardware-Constraints wird erörtert, bevor Mechanismen zur Halluzination Prevention und zur Sicherstellung faktentreuer Ausgaben behandelt werden. Strategien zur Knowledge Integration ohne den Overhead vollständiger Retrieval-Augmented Generation-Systeme schließen diesen Abschnitt.

Der rote Faden dieses Kapitels führt von theoretischen Konzepten zu deren praktischer Anwendbarkeit unter realen Constraints. Während Kapitel~\ref{chap:3} den Anwendungskontext beschreibt, der technologische Entscheidungen determiniert, und Kapitel~\ref{chap:6} die konkrete Implementierung darstellt, fundiert das vorliegende Kapitel die entwickelte Lösung in der etablierten wissenschaftlichen Forschung und schafft das konzeptionelle Fundament für die nachfolgenden praktischen Ausführungen.

    \subsection{Natural Language Processing für Verkehrsinformationen}
    \label{sec:2.1}
    
    % Ziel: 5-6 Seiten
    
Die Auswahl geeigneter Sprachmodelle für die Verarbeitung von Verkehrsinformationen bewegt sich im Spannungsfeld zwischen universellen und domänenspezifischen Ansätzen. Während proprietäre Systeme wie GPT-4 oder Claude beeindruckende Generalfähigkeiten demonstrieren, erfordern Verkehrsdomänen spezifische Terminologie, rechtliche Präzision und lokale Kontextkenntnisse. Zudem machen Anforderungen der Datenschutz-Grundverordnung, Datensouveränität und Kosteneffizienz den Einsatz von Open-Source-Modellen für öffentliche Verkehrsbetriebe unerlässlich.

Die Komplexität von Natural Language Processing im Verkehrssektor wird häufig unterschätzt. Es genügt nicht, Sentiment-Analysen oder FAQ-Bots bereitzustellen; die Transformation fachsprachlicher Anweisungen erfordert gleichzeitig tiefes Kontextverständnis, präzise Stiladaption und absolute Faktentreue. Die Umformulierung einer verkürzten Mitteilung wie „Linie 10 fährt nicht" in eine vollständige Fahrgastinformation „Die Straßenbahn der Linie 10 verkehrt aufgrund technischer Störungen gegenwärtig nicht" erfordert implizites Wissen über Liniennummern, Fahrzeugtypen und gegebenenfalls Alternativrouten, das nicht unmittelbar in der Eingabe kodiert ist.

Die Evolution natürlichsprachlicher Generierungsansätze im Verkehrssektor spiegelt den allgemeinen Fortschritt im Natural Language Processing wider. Template-basierte Systeme waren starr, in ihrem Ausdrucksvermögen limitiert und wartungsintensiv. Regelbasierte Natural Language Generation bot zwar bessere Kontrolle über die Ausgabequalität, stieß jedoch bei der Skalierung auf erhebliche Probleme. Moderne Large Language Models vereinen Flexibilität mit Generalisierungsfähigkeit, stellen jedoch neue Herausforderungen hinsichtlich der Kontrollierbarkeit und Faktentreue dar.

Transformer-basierte Sprachmodelle unterscheiden sich fundamental von früheren Ansätzen und ermöglichen erst die geforderte Qualität automatisierter Texttransformation. Der Attention-Mechanismus erfasst semantische Abhängigkeiten über Satzgrenzen hinweg, während das Pretraining auf Milliarden von Tokens ein robustes allgemeines Sprachverständnis etabliert. Fine-Tuning erlaubt anschließend die Domänenanpassung, ohne dass ein vollständiges Training from-scratch erforderlich wäre, was bei begrenzten Ressourcen und Datenmengen entscheidend ist.

Die folgenden Unterabschnitte strukturieren die theoretischen Grundlagen systematisch: Abschnitt~\ref{sec:2.1.1} behandelt die technischen Fundamente der Transformer-Architektur, das Pretraining-Paradigma und Transfer Learning. Abschnitt~\ref{sec:2.1.2} gibt einen Überblick über verkehrsspezifische NLP-Anwendungen und den aktuellen Stand der Technik. Abschnitt~\ref{sec:2.1.3} fokussiert auf deutschsprachige und domänenspezifische Modelle, insbesondere die LeoLM-Familie, die als Basis für die vorliegende Arbeit dient.
    
        \subsubsection{Grundlagen der Sprachverarbeitung}
        \label{sec:2.1.1}
        
        % Bereits ausgearbeitet - siehe unten
        
        Die automatisierte Verarbeitung und Transformation von Verkehrsinformationen stellt hohe Anforderungen an Natural Language Processing-Systeme. Präzision, Kontextverständnis und die Fähigkeit zur semantischen Umformulierung sind dabei zentrale Anforderungen. Moderne Ansätze der Sprachverarbeitung basieren auf der Transformer-Architektur, die seit ihrer Einführung im Jahr 2017 die Entwicklung von Sprachmodellen maßgeblich geprägt hat. Das in dieser Arbeit verwendete Modell LeoLM-7B baut auf dieser Architektur auf und nutzt deren Vorteile für die Verarbeitung deutschsprachiger Texte.
        
        \paragraph{Transformer-Architektur}
        
        Die Transformer-Architektur wurde 2017 von Vaswani et al. mit dem wegweisenden Paper „Attention is All You Need" eingeführt \cite{vaswani2017attention}. Im Gegensatz zu vorherigen Ansätzen wie Recurrent Neural Networks (RNNs) oder Long Short-Term Memory (LSTM) verzichtet die Transformer-Architektur vollständig auf rekurrente Strukturen und basiert stattdessen auf dem Attention-Mechanismus. Dieser fundamentale Paradigmenwechsel ermöglicht die parallele Verarbeitung von Sequenzen und führt zu deutlich schnelleren Trainingszeiten sowie besserer Skalierbarkeit.
        
        Das Kernprinzip der Transformer-Architektur ist der Self-Attention-Mechanismus \cite{vaswani2017attention}. Dieser ermöglicht es jedem Token in einer Sequenz, auf alle anderen Tokens im Kontext zuzugreifen und deren Relevanz für die eigene Repräsentation zu bewerten. Durch den Einsatz von Multi-Head Attention werden mehrere parallele Attention-Mechanismen verwendet, die unterschiedliche Aspekte der Kontextbeziehungen erfassen können \cite{vaswani2017attention}. Diese Architektur ermöglicht es dem Modell, komplexe syntaktische und semantische Abhängigkeiten auch über große Distanzen im Text hinweg zu modellieren, ohne unter dem Vanishing-Gradient-Problem zu leiden, das RNNs bei langen Sequenzen beeinträchtigt.
        
        Transformer-basierte Modelle lassen sich in drei Hauptkategorien einteilen: Encoder-Only-Modelle wie BERT \cite{devlin2018bert}, die primär für bidirektionale Textklassifikation und Embeddings konzipiert sind, Decoder-Only-Modelle wie die GPT-Serie \cite{radford2018improving, radford2019language}, die für autoregressive Textgenerierung optimiert sind, sowie Encoder-Decoder-Architekturen wie der ursprüngliche Transformer \cite{vaswani2017attention} und T5 \cite{raffel2020exploring}, die vor allem für Übersetzungsaufgaben entwickelt wurden. Für die in dieser Arbeit behandelte Aufgabe der Textgenerierung und -transformation ist die Decoder-Only-Architektur besonders geeignet, da sie speziell für die sequenzielle Erzeugung von Text konzipiert wurde.
        
        \paragraph{Vortrainierte Sprachmodelle}
        
        Ein zentrales Konzept moderner NLP-Systeme ist das Pretraining von Sprachmodellen auf großen, unlabeled Textkorpora. Durch Self-Supervised Learning, bei dem das Modell darauf trainiert wird, das jeweils nächste Token in einer Sequenz vorherzusagen \cite{radford2018improving}, entwickeln diese Modelle ein umfassendes Sprachverständnis inklusive Syntax, Semantik und implizitem Weltwissen. Zu den einflussreichsten vortrainierten Modellen gehören BERT \cite{devlin2018bert} mit seinem bidirektionalen Masked Language Modeling-Ansatz, die GPT-Serie \cite{radford2018improving, radford2019language} für autoregressive Textgenerierung sowie T5 \cite{raffel2020exploring} mit seinem universellen Text-to-Text-Framework.
        
        Für die vorliegende Arbeit ist insbesondere die Mistral-7B-Architektur von Bedeutung, da sie die Grundlage für das verwendete LeoLM-Modell bildet. Mistral 7B \cite{jiang2023mistral} ist ein hocheffizienter Decoder-Only-Transformer mit 7 Milliarden Parametern, der mehrere innovative Architekturmerkmale aufweist. Grouped-Query Attention (GQA) reduziert die Größe des Key-Value-Cache und ermöglicht schnellere Inferenz \cite{jiang2023mistral}. Sliding Window Attention erlaubt die effiziente Verarbeitung langer Kontexte, während der Rolling Buffer Cache die Speichernutzung optimiert \cite{jiang2023mistral}. Mit 7 Milliarden Parametern stellt Mistral einen Sweet Spot zwischen Modellleistung und Ressourceneffizienz dar und übertrifft in Benchmarks viele deutlich größere Modelle \cite{jiang2023mistral}. Als Open-Source-Modell unter Apache 2.0 Lizenz ist es besonders für lokale Ausführung und Fine-Tuning geeignet.
        
        Ein wichtiger Unterschied besteht zwischen Base Models und Instruct Models. Base Models wie Mistral-7B \cite{jiang2023mistral} sind auf reines Language Modeling trainiert \cite{radford2019language} und setzen primär Texte fort, ohne notwendigerweise expliziten Anweisungen zu folgen. Sie sind als Ausgangspunkt für aufgabenspezifisches Fine-Tuning konzipiert. Instruct Models hingegen durchlaufen zusätzlich ein Instruction Tuning \cite{wei2021finetuned}, bei dem sie mittels Supervised Fine-Tuning auf Instruktionsdatensätzen trainiert werden, um gezielt Anweisungen zu befolgen. Optional kann dieser Prozess durch Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training} weiter verfeinert werden. Mistral bietet beide Varianten an: Mistral-7B als Base Model und Mistral-7B-Instruct als instruction-tuned Version \cite{jiang2023mistral}. Der detaillierte Vergleich und die Auswahlbegründung zwischen diesen Varianten erfolgt in Kapitel~\ref{chap:4}.
        
        Für deutschsprachige Anwendungen ist die LeoLM-Familie von besonderer Relevanz. Diese Modelle nutzen Mistral-7B als Basis und durchlaufen ein Continued Pretraining auf deutschen Textkorpora \cite{leolm2023}. Das in dieser Arbeit verwendete Modell leo-mistral-hessianai-7b ist eine Base-Variante, die speziell für die deutsche Sprache optimiert wurde \cite{leolm2023}. Diese Adaption kombiniert die architektonischen Vorteile und die Effizienz von Mistral mit erhöhter Sprachkompetenz im Deutschen und erzielt dadurch bessere Ergebnisse für deutschsprachige Anwendungen als rein englischsprachige Basismodelle. Die Tokenization erfolgt bei Mistral mittels Byte Pair Encoding (BPE) mit einem Vokabular von 32.000 Tokens \cite{jiang2023mistral}, wobei LeoLM einen an die deutsche Morphologie angepassten Tokenizer verwendet \cite{leolm2023}, der besser mit Komposita, Umlauten und anderen sprachspezifischen Besonderheiten umgehen kann.
        
        \paragraph{Transfer Learning}
        
        Das Konzept des Transfer Learning bildet die theoretische Grundlage für die Nutzung vortrainierter Modelle in spezifischen Anwendungsdomänen. Transfer Learning bezeichnet den Wissenstransfer von einer Source Domain, in der das Modell vortrainiert wurde, zu einer Target Domain, für die es angepasst werden soll \cite{pan2010survey}. Dieser Ansatz reduziert den Bedarf an aufgabenspezifischen Trainingsdaten und die erforderliche Trainingszeit erheblich.
        
        Das etablierte Pretrain-Finetune-Paradigma verläuft in zwei Phasen: Zunächst erfolgt das Pretraining auf großen, unlabeled Textkorpora mittels unsupervised Learning \cite{radford2018improving}, wodurch das Modell grundlegendes Sprachverständnis, syntaktische Strukturen, semantische Zusammenhänge und implizites Weltwissen erwirbt. In der zweiten Phase wird das Modell mittels Fine-Tuning auf eine spezifische Aufgabe angepasst \cite{howard2018universal}, wobei supervised Learning mit aufgabenspezifischen Daten zum Einsatz kommt. Im Kontext dieser Arbeit bedeutet dies die Spezialisierung auf die Transformation von LVB-Verkehrsanweisungen.
        
        Die Vorteile von Transfer Learning sind vielfältig: Howard und Ruder zeigten mit ULMFiT, dass durch Transfer Learning mit nur 100 gelabelten Beispielen eine vergleichbare Performance erreicht werden kann wie mit 10.000 Beispielen beim Training from-scratch \cite{howard2018universal}. Vortrainierte Gewichte dienen als optimaler Startpunkt und beschleunigen das Training erheblich. Zudem führt das bereits vorhandene Sprachverständnis zu besserer Generalisierung auf neue Daten \cite{pan2010survey}.
        
        Dennoch bestehen Herausforderungen: Catastrophic Forgetting bezeichnet den Verlust vortrainierter Fähigkeiten während des Fine-Tunings, dem in Abschnitt~\ref{sec:2.2.4} weiter nachgegangen wird. Der Domain Shift zwischen Pretraining-Daten und Zieldomäne \cite{pan2010survey} kann zu Leistungseinbußen führen, insbesondere wenn sich Vokabular oder Sprachstil deutlich unterscheiden. Bei kleinen Datensätzen besteht zudem die Gefahr des Overfittings, was ebenfalls in Abschnitt~\ref{sec:2.2.4} behandelt wird.
        
        Für die vorliegende Arbeit ist besonders relevant, dass LeoLM bereits auf umfangreichen deutschen Textkorpora vortrainiert wurde \cite{leolm2023}, was eine solide Basis für die weitere Spezialisierung bildet. Das Fine-Tuning auf den vergleichsweise kleinen Datensatz der LVB-Verkehrsanweisungen wird durch Transfer Learning erst praktikabel und ermöglicht die notwendige domänenspezifische Anpassung an die fachsprachlichen Anforderungen des Verkehrssektors.
        
        \subsubsection{NLP-Anwendungen im Verkehrssektor}
        \label{sec:2.1.2}
        
        % Ziel: 2-3 Seiten
        
        Der Personentransportsektor produziert massive Mengen an Textdaten, darunter Verkehrsberichte in Echtzeit und historischer Form, technische Wartungsprotokolle, Incident Reports zu Unfällen und Störungen, Fahrgastbeschwerden und Feedback sowie ungefilterte, heterogene Beiträge in sozialen Medien. Das Datenvolumen übersteigt die menschliche Verarbeitungskapazität bei weitem, während sich regelbasierte Ansätze als unzureichend erwiesen haben. Starre Pattern-Matching-Logik skaliert nicht mit der Variabilität natürlicher Sprache, und der Wartungsaufwand wächst exponentiell mit zunehmender Systemkomplexität.

Natural Language Processing entwickelt sich in diesem Kontext zur Enabler-Technologie, die den Wandel von reaktivem zu proaktivem Verkehrsmanagement ermöglicht. Die automatisierte Erkenntnisgewinnung aus heterogenen Datenquellen steigert die operative Effizienz erheblich. Während frühe NLP-Ansätze auf Schlüsselwortsuche und einfache Klassifizierung beschränkt waren, beherrschen moderne Systeme komplexes Kontextverständnis und erfassen semantische Zusammenhänge. Diese Evolution transformiert Natural Language Processing vom reinen Mustererkenner zum Bedeutungsversteher.

Laut [SOURCE-MISSING] lassen sich NLP-Anwendungen im Verkehrssektor in mehrere Hauptkategorien einteilen. Verkehrsvorhersage und -management umfasst Predictive Analytics aus Textdaten, Anomalie-Detektion in Verkehrsberichten sowie textbasierte Ressourcenallokation. Sentimentanalyse von Social-Media-Daten ermöglicht ein Echtzeit-Stimmungsbarometer der Fahrgäste, die Früherkennung von Service-Problemen und unterstützt Crisis Management bei Großstörungen. Natürliche Sprachschnittstellen finden Anwendung in Query-Interfaces für Fahrplandatenbanken, Conversational AI für den Kundenservice sowie in Voice-Assistenten für Fahrzeuge. Verkehrsampel-Steuerung mittels NLP für adaptive Systeme und textbasierte Koordination zwischen Systemen ist für die vorliegende Arbeit weniger relevant.

Für diese Arbeit zentral ist die automatisierte Berichtserstellung und Dokumentenanalyse. Jüngste Forschungsarbeiten dokumentieren erhebliche Fortschritte: Die Zusammenfassung von Unfallberichten [SOURCE-MISSING], die Klassifizierung der Unfall-Schwere aus Textbeschreibungen [SOURCE-MISSING] sowie die generelle Verbesserung textbasierter Verarbeitung durch leistungsstarke Sprachmodelle zeigen das Potenzial dieser Technologien.

Direkt relevant für die vorliegende Arbeit sind öffentliche Verkehrsinformationsdienste. Large Language Models erweisen sich als effektiv in der Analyse und Aufbereitung von Fahrgastanfragen [SOURCE-MISSING]. Die Transformation technischer Informationen in endnutzergerechte Formate unterstützt die zentrale Aufgabe dieser Arbeit: die verständliche Aufbereitung von Verkehrsanweisungen für Fahrgäste unter Wahrung konsistenter und präziser Kommunikation. Mehrsprachige Verarbeitung wird mit steigender Vielseitigkeit von Large Language Models zunehmend möglich [SOURCE-MISSING], stellt jedoch aktuell ein sekundäres Ziel dar, das in dieser Arbeit nicht tiefergehend behandelt wird, jedoch Potenzial für zukünftige Erweiterungen bietet.

Der Fokus dieser Arbeit liegt primär auf automatisierter Berichtserstellung und öffentlichen Informationsdiensten. Der konkrete Anwendungsfall der Transformation von LVB-Verkehrsanweisungen schlägt eine Brücke zwischen technischer Fachsprache und Allgemeinverständlichkeit. Diese Aufgabe ist verwandt mit Dokumentenanalyse, Textzusammenfassung und Stiladaption, unterscheidet sich jedoch von reiner Klassifizierung durch die essenzielle generative Komponente.

Empirische Erfolge in verwandten Domänen stützen die Hypothese, dass Large Language Models für die Transformation von Verkehrsanweisungen geeignet sind. State-of-the-Art-Performance wurde bei Zusammenfassung und Klassifizierung nachgewiesen, die effektive Bearbeitung von Fahrgastanfragen ist dokumentiert, und die automatisierte Verarbeitung von Unfallberichten übertrifft regelbasierte Systeme deutlich.

Dennoch bestehen spezifische Herausforderungen: Domänenspezifisches Vokabular erfordert gezielte Spezialisierung der Modelle. Faktentreue ist bei sicherheitskritischen Informationen unabdingbar, sodass Halluzinationen vermieden werden müssen. Konsistenz in der Ausgabe ist erforderlich, sodass gleiche Eingaben zu identischen Ausgaben führen und ein gewisser Determinismus gewährleistet ist. Mehrsprachigkeit ist technisch möglich, aber ressourcenintensiv und daher für diese Arbeit nicht im Fokus.
        
        
        \subsubsection{Domänenspezifische Sprachmodelle}
        \label{sec:2.1.3}
        
        % Ziel: 1,5-2 Seiten
        
        Während allgemeine Large Language Models wie GPT-4 oder Claude eine beeindruckende Breite an Fähigkeiten demonstrieren, zeigt die Spezialisierung auf die Verkehrsdomäne signifikante Vorteile. Der Trade-off zwischen Generalität und Domänen-Expertise ist dabei zu berücksichtigen.

Domänenspezifisches Fine-Tuning erweist sich als kritischer Erfolgsfaktor [SOURCE-MISSING]. Wie am Beispiel von TrafficSafetyGPT dokumentiert, übertreffen spezialisierte Modelle konsistent allgemeine Modelle in verkehrsspezifischen Aufgaben. Technische Komponenten wie erweiterte Fachterminologie-Datenbanken, Domain-Specific Pre-Training auf Verkehrssicherheits-Korpora und professionelle Ausdrucksfähigkeiten mit technischer Präzision tragen zu diesem Erfolg bei. Die Vorteile manifestieren sich in effizienterer Zuweisung von Rechenressourcen, kürzeren Trainingszeiten bei gleichzeitig höherer Qualität sowie deutlich höherer Informationsabdeckung im Vergleich zu General-Purpose-Modellen.

Die multimodale Integration verschiedener Datenmodalitäten stellt sich als wesentlicher Erfolgsfaktor heraus [SOURCE-MISSING]. Die Kombination von narrativem, unstrukturiertem Text mit strukturierten Unfalldaten in Form von Tabellen und Kategorien sowie Metadaten zu Zeit, Ort und Beteiligten erzielt empirisch nachgewiesene Erfolge. Eine dokumentierte Reduktion der Fehlerrate um 54,2\,\% verdeutlicht den Synergieeffekt: Text liefert Kontext, während strukturierte Daten Präzision gewährleisten. Diese Strategie gilt als die vielversprechendste zur Leistungssteigerung.

Domain-Specific Pre-Training mittels Continued Pretraining auf Verkehrssicherheits-Korpora ermöglicht es Modellen, spezialisierte Terminologie und Kontexte zu erlernen. Signifikante Leistungsverbesserungen sind dokumentiert, analog zum deutschsprachigen Continued Pretraining von LeoLM auf Basis von Mistral.

Die Übertragung dieser Erkenntnisse auf die vorliegende Arbeit erfolgt durch eine stufenweise multimodale Integration verschiedener Optimierungen. Ausgehend vom Base Model LeoLM, das bereits für die deutsche Sprache optimiert ist, erfolgt Fine-Tuning auf domänenspezifischen Verkehrsanweisungen. Knowledge Enhancement integriert strukturierte Linien-Fahrzeug-Zuordnungen, während Prompt Engineering Stilrichtlinien und Beispiele bereitstellt. Das Ziel besteht darin, trotz eines kleinen Datensatzes fehlerfreie Ergebnisse zu erzielen. Die Forschung rechtfertigt diesen Multi-Methoden-Ansatz, da die Kombination verschiedener Techniken den Datenmangel kompensieren kann.

Deutschsprachige Modelle weisen Besonderheiten auf, die für die vorliegende Arbeit relevant sind. Die LeoLM-Familie basiert auf Mistral und durchläuft deutsches Continued Pretraining, was sprachspezifische Optimierung für Komposita und Morphologie ermöglicht. Da der LVB-Anwendungsfall rein deutschsprachig ist, stellt sich die Frage nach dem Vergleich zwischen englischen Modellen mit nachgelagerter Translation und nativen deutschen Modellen. Native Modelle bieten bessere Idiomatik und Präzision, während Translation das Risiko von Artefakten und Stilbrüchen birgt.

Die Herausforderungen deutschsprachiger Verkehrsdaten sind vielfältig [SOURCE-MISSING]. Heterogene Formatvielfalt erschwert die standardisierte Verarbeitung erheblich. Telegrafische Texte sind verkürzt und entbehren Artikel, Tabelleneinträge sind strukturiert und fragmentiert, während Fließtext-Berichte vollständige Sätze enthalten. Diese Heterogenität stellt hohe Anforderungen an die Vorverarbeitung.

Das Cross-Language Transferability-Problem manifestiert sich darin, dass englisch-trainierte Modelle nicht direkt auf deutschsprachige Daten anwendbar sind. Erhebliche Leistungseinbußen ohne deutschsprachiges Training sind dokumentiert. Zwei Lösungswege existieren: separates Training für deutschsprachige Daten, wie es der LeoLM-Ansatz verfolgt, oder mehrsprachige Trainingsdatensätze mit mindestens 20\,\% deutschen Anteilen für vergleichbare Performance. Diese Erkenntnisse begründen die Wahl deutschsprachiger Basismodelle für die vorliegende Arbeit.

Die Fehleranfälligkeit generischer Modelle resultiert aus zwei Hauptproblemen. Primär englischsprachiges Pretraining führt dazu, dass Modelle zwar übersetzen können, die Fehlerrate jedoch deutlich steigt. Idiomatische Fehler und ungünstige Wortwahl sind häufige Konsequenzen. Generisches Training ohne Verkehrsdomäne hat fehlende Fachterminologie und mangelndes Kontextverständnis für verkehrsspezifische Abkürzungen zur Folge. Der einzige verfügbare deutsche Verkehrsdatensatz von [SOURCE-MISSING] ist veraltet und limitiert, was eigenständiges Fine-Tuning unumgänglich macht.

Die Überleitung zu Kapitel~\ref{chap:4} verdeutlicht, dass domänenspezifische Modelle theoretisch überlegen sind, deutsche Verkehrsmodelle in der Praxis jedoch fehlen. Die Lösung besteht in der Kombination eines deutschen Basismodells mit verkehrsspezifischem Fine-Tuning. Die detaillierte Auswahlbegründung erfolgt im Modellauswahl-Kapitel.

Existierende verkehrsspezifische Modelle sind für die vorliegende Arbeit nicht geeignet. TrafficSafetyGPT [SOURCE-MISSING] basiert auf LLaMA von Meta AI und verwendet den TrafficSafety-2k-Datensatz, der behördliche Richtlinien und ChatGPT-generierte Daten umfasst. Der Fokus liegt auf Verkehrssicherheit (Traffic Safety), und die Vorteile bestehen in effizienter Ressourcennutzung und kurzen Trainingszeiten. Dennoch ist das Modell für LVB nicht geeignet, da es englischsprachig ist und keine deutsche Sprachkompetenz aufweist, der Safety-Fokus nicht der Fahrgastinformation entspricht und die Closed-Source-Basis DSGVO-Probleme für lokales Deployment verursacht.

Die TransGPT-Familie [SOURCE-MISSING] umfasst TransGPT-SM als unimodales Text-Modell basierend auf ChatGLM2-6B sowie TransGPT-MM als multimodales Modell mit Text- und Vision-Komponenten. Das Training erfolgte mit Verkehrsunterlagen, Büchern und Berichten für Anwendungen in Dokumentenanalyse und Führerscheinprüfungen. Das Modell ist für LVB nicht geeignet, da es chinesischsprachig ist (ChatGLM2 ist ein chinesisches Modell), der Fokus auf Wissensvermittlung statt Texttransformation liegt und keine Open-Source-Verfügbarkeit für lokales Fine-Tuning besteht.

Anwendungsspezifische Chatbots wie TP-GPT für Verkehrsüberwachung und Echtzeit-Daten [SOURCE-MISSING], TrafficGPT mit Integration von Traffic Foundation Models und Aufgaben-Dekomposition [SOURCE-MISSING] sowie ChatSUMO für SUMO-Simulator-Integration und Szenario-Generierung [SOURCE-MISSING] sind ebenfalls nicht für LVB geeignet. Diese Frameworks sind aufgabenspezifisch und nicht General-Purpose, erfordern Echtzeit-Datenbanken mit erheblichem Infrastruktur-Overhead, fokussieren auf Query-Beantwortung statt Texttransformation und sind englischsprachig.

Multimodale und sicherheitskritische Modelle wie ChatScene für AV-Sicherheitsszenarien [SOURCE-MISSING], AccidentGPT für Kollisionsvermeidung [SOURCE-MISSING], IDM-GPT mit fünf spezialisierten Agenten für Verkehrsanalysen [SOURCE-MISSING] sowie STEP-LLM für räumlich-zeitliche Verkehrsvorhersage [SOURCE-MISSING] sind nicht für LVB geeignet. Der Fokus liegt auf autonomen Fahrzeugen und Predictive Analytics, die Aufgabe besteht nicht in Textgenerierung, sondern in Szenario-Simulation, deutschsprachige Varianten fehlen, und die Modelle sind zu spezialisiert für allgemeine Fahrgastinformation.

Zusammenfassend ist keines dieser Modelle aus mehreren Gründen geeignet: Die Sprachbarriere, da alle Modelle englisch- oder chinesischsprachig sind; der Aufgaben-Mismatch, da Safety, Prediction und Simulation nicht der Text-Transformation entsprechen; Lizenzierungsprobleme, da die meisten Modelle proprietär sind oder auf Closed-Source-Basis beruhen; Infrastruktur-Anforderungen für Echtzeit-Datenbanken und Multi-Agenten-Systeme sowie Deployment-Einschränkungen, da die Modelle nicht für lokale, offline-fähige Ausführung konzipiert sind.

Die Konsequenz für die vorliegende Arbeit besteht darin, dass keine Off-the-Shelf-Lösung verfügbar ist. Dies macht eine eigenständige Modellentwicklung notwendig. Die Strategie besteht in der Verwendung eines deutschen Basismodells (LeoLM) mit domänenspezifischem Fine-Tuning. Dies füllt eine Forschungslücke bei deutschsprachiger Verkehrsinformation, wobei die detaillierte Begründung in Kapitel~\ref{chap:4} erfolgt.

Für kleine Datensätze ergeben sich aus der Forschung bewährte Strategien. Die Herausforderung besteht darin, dass der LVB-Datensatz vergleichsweise klein ist. Strategien aus der Forschung umfassen Transfer Learning von vortrainierten Modellen, multimodale Integration von Text und Strukturdaten, Data Augmentation (siehe Abschnitt~\ref{sec:2.2.3}) sowie parameter-effizientes Fine-Tuning mittels LoRA (siehe Abschnitt~\ref{sec:2.2.1}). Die Kombination dieser Methoden hat sich nachweislich als erfolgreich erwiesen.

Die Überleitung zu Abschnitt~\ref{sec:2.2} verdeutlicht, dass die theoretische Überlegenheit domänenspezifischer Modelle etabliert ist. Der nächste Schritt besteht darin zu klären, wie solche Modelle erstellt werden. Fine-Tuning-Methoden, Datenstrategien und Optimierungen stehen im Fokus des folgenden Abschnitts, der die Perspektive von „Was funktioniert?" zu „Wie macht man es?" verschiebt.


    \subsection{Sprachmodelle und Fine-Tuning}
    \label{sec:2.2}
    
    % Ziel: 4-5 Seiten
    
Vortrainierte Sprachmodelle verfügen über umfassendes allgemeines Sprachverständnis, das sie während des Pretrainings auf großen Textkorpora erworben haben. Die Spezialisierung auf domänenspezifische Aufgaben erfordert jedoch gezielte Anpassung. LeoLM besitzt deutschsprachige Kompetenz, deckt die Verkehrsdomäne jedoch nicht ab. Der Transfer von allgemeinem zu spezialisiertem Wissen stellt folglich eine zentrale Herausforderung dar.

Full Fine-Tuning, bei dem sämtliche Modellparameter angepasst werden, ist bei modernen Large Language Models mit Milliarden von Parametern ressourcenintensiv. Bei einem 7-Milliarden-Parameter-Modell wie Mistral-7B müssen alle Parameter aktualisiert werden, was Training auf Multi-GPU-Clustern über mehrere Wochen erfordert. Der Speicherbedarf übersteigt 100\,GB VRAM für Gradientenberechnung und Optimizer-States, während die Kostenstruktur für Cloud-Computing-Ressourcen in den vier- bis fünfstelligen Bereich reicht. Die Constraints dieser Arbeit beschränken sich auf Consumer-Hardware mit begrenzten Ressourcen, sodass parameter-effiziente Fine-Tuning-Methoden erforderlich werden.

Die Herausforderung begrenzter Trainingsdaten verschärft die Situation. Der LVB-Datensatz umfasst eine geschätzte Größenordnung von 100 bis 500 Trainingsbeispielen. Im Vergleich dazu verfügt ImageNet über mehr als eine Million annotierte Bilder, während LAION 5 Milliarden Text-Bild-Paare enthält. Das Risiko des Overfittings bei vollständiger Parameteranpassung mit einem kleinen Datensatz ist erheblich. Parameter-effiziente Verfahren bieten den Vorteil eines reduzierten Datenbedarfs durch gezielte Adaption.

Methodisch kommen verschiedene Ansätze zum Einsatz. Parameter-effiziente Fine-Tuning-Verfahren reduzieren die Anzahl trainierbarer Parameter erheblich. Prompt Engineering dient als komplementärer Ansatz zur Leistungssteigerung. Datenaufbereitung und Augmentation kompensieren limitierte Datenverfügbarkeit. Gegenmaßnahmen zu Overfitting und Catastrophic Forgetting sichern die Qualität des trainierten Modells.

Der Bezug zur praktischen Implementierung verdeutlicht, dass die theoretischen Konzepte dieses Abschnitts direkte Anwendung in der Implementierung finden. Eine fundierte Entscheidungsfindung basiert auf dem Verständnis methodischer Trade-offs. Diese wissenschaftlich fundierte Vorgehensweise ersetzt empirisches Trial-and-Error durch systematische Planung.
    
        \subsubsection{Fine-Tuning-Methoden}
        \label{sec:2.2.1}
        
        % Ziel: 2,5-3 Seiten
        
        Die Anpassung großer vortrainierter Sprachmodelle an spezifische Downstream-Aufgaben stellt erhebliche Herausforderungen dar [SOURCE-MISSING]. Das Training aller Modellparameter für jede neue Aufgabe wird mit wachsenden Modellgrößen zunehmend unpraktikabel, da Rechenressourcen und Speicherbedarf exponentiell steigen [SOURCE-MISSING]. Bei einem 7-Milliarden-Parameter-Modell müssen sämtliche Parameter angepasst werden, was über 100\,GB VRAM für Gradienten und Optimizer-States erfordert. Multi-GPU-Cluster über mehrere Wochen hinweg sind notwendig, und die Kostenstruktur bewegt sich im vier- bis fünfstelligen Bereich für Cloud-Computing. Diese Situation macht parameter-effiziente Transferlernmethoden unerlässlich.

Parameter-effiziente Fine-Tuning-Verfahren (PEFT) lassen sich in drei Hauptkategorien einteilen [SOURCE-MISSING]. Additive Fine-Tuning-Ansätze fügen zusätzliche trainierbare Module zum gefrorenen Basismodell hinzu, wobei die Originalgewichte unverändert bleiben. Beispiele hierfür sind Adapter, Prefix Tuning und Prompt Tuning. Der Trade-off besteht zwischen geringer Parameteranzahl und potenzieller Inferenz-Latenz. Reparameterized Fine-Tuning zerlegt Gewichtsupdates in niedrigrangige Matrizen und modifiziert indirekt die Originalgewichte. Der Hauptvertreter dieser Kategorie ist LoRA (Low-Rank Adaptation), das durch Weight Merging keine Inferenz-Latenz verursacht. Selective Fine-Tuning trainiert nur ausgewählte Teilmengen existierender Parameter und friert den Großteil des Modells ein, wird jedoch weniger verbreitet eingesetzt und findet in dieser Arbeit keine Verwendung.

\paragraph{LoRA (Low-Rank Adaptation)}

LoRA basiert auf der theoretischen Hypothese [SOURCE-MISSING], dass Gewichtsänderungen während der Adaptation einen niedrigen intrinsischen Rang aufweisen. Die Gewichtsmatrix-Änderung $\Delta W$ lässt sich effizient durch Low-Rank-Dekomposition approximieren, was eine indirekte Optimierung dichter Layer via Rank-Decomposition ermöglicht. Gewichtsupdates werden als niedrigrangige Dekomposition $\Delta W = BA$ dargestellt, wobei $W_0$ die eingefrorenen Originalgewichte bezeichnet und $W = W_0 + \Delta W = W_0 + BA$ gilt. Die Matrizen $B \in \mathbb{R}^{d \times r}$ und $A \in \mathbb{R}^{r \times k}$ erfüllen $r \ll d, k$, wobei der Rang $r$ als Hyperparameter die Expressivität gegenüber der Effizienz kontrolliert. Direkte Änderungen an vortrainierten Gewichten finden nicht statt; stattdessen werden Low-Rank-Matrizen separat trainiert [SOURCE-MISSING]. Der Bedarf an Hyperparameter-Retuning wird bei Variation des Rangs minimiert [SOURCE-MISSING].

Die Effizienzgewinne von LoRA sind erheblich. Die Parameter-Reduktion erreicht bis zu 10.000-fache Reduktion trainierbarer Parameter bei vergleichbarer Performance zu Full Fine-Tuning [SOURCE-MISSING]. Typischerweise sind weniger als 1\,\% der Gesamtparameter trainierbar. Speichereffizienz manifestiert sich in 3-fach geringerem GPU-Memory-Bedarf als traditionelles Fine-Tuning [SOURCE-MISSING], wobei empirisch eine 35\,\%-ige Reduktion der Memory Usage bei GPT-2 dokumentiert ist [SOURCE-MISSING]. Dies prädestiniert LoRA für ressourcenbeschränkte Umgebungen und ermöglicht Training auf Consumer-Hardware. Bei Verwendung des Adam-Optimizers reduziert sich der VRAM-Bedarf um bis zu zwei Drittel [SOURCE-MISSING].

Die Trainingszeit-Reduktion beträgt 30\,\% im Vergleich zu Full Fine-Tuning [SOURCE-MISSING]. Benchmarks auf GPT-2 mit Tesla T4 zeigen 5,1 Minuten für LoRA gegenüber 7,4 Minuten für Full Fine-Tuning. Dieser Effizienzgewinn besteht trotz erhöhter Iterationszahl aufgrund der Konvergenzproblematik.

Deployment-Vorteile ergeben sich aus der modularen Architektur [SOURCE-MISSING]. Ein Basismodell kann mit vielen Task-Adaptern kombiniert werden, wobei nur Adapter-Parameter pro Aufgabe gespeichert werden müssen [SOURCE-MISSING]. Dies löst das „One-Domain-One-Model"-Problem. Kostengünstiges Task-Switching erfordert lediglich den Austausch der LoRA-Gewichte statt aller Parameter, was Storage- und Switching-Overhead im Multi-Task-Deployment reduziert [SOURCE-MISSING].

Bezüglich der Inferenz-Eigenschaften verursacht LoRA keinen zusätzlichen Overhead [SOURCE-MISSING]. Weight Merging nach dem Training integriert die Adapter in die Originalgewichte: $W_{\text{final}} = W_0 + BA$ [SOURCE-MISSING]. Diese Post-Training-Integration in Originalgewichte stellt einen entscheidenden Vorteil gegenüber Adapter- und Prompt-Methoden dar.

Limitierungen und Herausforderungen bestehen dennoch. Die Konvergenzproblematik zeigt sich in signifikant langsamerer Konvergenz als Full Fine-Tuning [SOURCE-MISSING]. Empirisch werden 5 bis 6-fach mehr Iterationen und FLOPs für gleiche Performance benötigt, was die Gesamt-Trainingskosten trotz Pro-Iteration-Effizienz erhöht und zu schlechterer Testperformance führen kann.

Architektonische Ursachen liegen in der Initialisierung von B mit Nullen [SOURCE-MISSING], was langsame Trainingsdynamik zwischen A und B in frühen Epochen verursacht. Der Skalierungsfaktor erzeugt kurzsichtige Inter-Layer-Interaktionen. Dropout erweist sich nur für lange Trainings-Episoden als geeignet.

Weitere Training-Herausforderungen umfassen potenzielle Catastrophic Forgetting-Probleme, Beeinträchtigung von Weltwissen in vortrainierten Modellen sowie Degradierung von Safety Alignment in fein-justierten Modellen. Die Rank-Sensitivität manifestiert sich darin, dass der Fixed-Rank-Constraint Flexibilität limitiert [SOURCE-MISSING]. Hohe Sensitivität gegenüber Rank-Auswahl erfordert, dass der optimale Rang im Voraus identifiziert werden muss, wobei suboptimale Wahl kostspieliges Retraining nach sich zieht.

Skalierungs-Limitierungen zeigen sich in sinkender Performance bei kleineren Modellen mit weniger als 7 Milliarden Parametern, besonders in Vision-Language-Pre-Training-Kontexten. Der Performance-Gap zu Full Fine-Tuning wächst bei komplexen Datensätzen [SOURCE-MISSING], wobei diverse Sub-Domänen und Task-Typen das Problem verschärfen. Der Gap wächst mit zunehmender Szenario-Komplexität.

Batching-Limitierungen entstehen durch die Herausforderung [SOURCE-MISSING], Inputs verschiedener Tasks mit unterschiedlichen A/B-Matrizen in einem single Forward Pass zu batchen. Dies reduziert Parallelisierungspotenzial bei Multi-Task-Inferenz und beeinträchtigt den Durchsatz in produktiven Multi-Tenant-Systemen.

QLoRA kombiniert LoRA mit Quantisierung [SOURCE-MISSING] und stellt eine optimierte Implementation dar. LoRA-Adapter werden mit 4-bit Quantisierung der Basisgewichte kombiniert, was weitere Speicherreduktion von etwa 70\,\% ohne Qualitätsverlust ermöglicht. Dies erlaubt 7-Milliarden-Parameter-Modelle auf GPUs mit weniger als 24\,GB VRAM. Die Vertiefung erfolgt in Abschnitt~\ref{sec:2.3.2}.

Konfigurationsparameter umfassen den Rank $r$ als Dimensionalität der Low-Rank-Matrizen, typischerweise 4, 8, 16, 32 oder 64, mit Trade-off zwischen Expressivität und Speicherbedarf. Alpha fungiert als Skalierungsfaktor für LoRA-Updates und beeinflusst die Lernrate der Adapter-Gewichte. Target Modules bestimmen, welche Transformer-Layer angepasst werden, darunter Query, Key, Value und Output Projections sowie bei Mistral-Architektur Gate, Up und Down Projections.
        
        \paragraph{Adapter}

Adapter gehören zu den frühesten PEFT-Frameworks [SOURCE-MISSING] und fügen kleine aufgabenspezifische Module mit Feedforward-Layers hinzu. Skip-Connections integrieren den Adapter-Output, während die Originalgewichte eingefroren bleiben.

Die Architektur folgt einem Bottleneck-Design mit Down-Projektion, Aktivierung und Up-Projektion. Die Einfügung erfolgt zwischen Transformer-Schichten, wobei modularer Austausch für verschiedene Tasks möglich ist.

Bezüglich der Effizienz fügen Adapter nur 3,6\,\% zusätzliche Parameter pro Task hinzu [SOURCE-MISSING], während die Performance innerhalb von 0,4\,\% des Full Fine-Tunings liegt. Dies ermöglicht kompakte, erweiterbare Modelle. Die Parameteranzahl beträgt $O(r(d_{\text{in}} + d_{\text{out}}))$ pro Layer [SOURCE-MISSING], was weniger effizient ist als LoRAs Low-Rank-Dekomposition.

Beim Deployment profitieren Adapter von hohem Parameter-Sharing durch das gefrorene Basismodell [SOURCE-MISSING]. Neue Tasks können ohne Revisitation vorheriger hinzugefügt werden, wobei task-spezifische Adapter-Parameter gespeichert werden müssen.

Die Limitierung besteht in Inferenz-Latenz, da die Modelltiefe durch zusätzliche Layer erhöht wird [SOURCE-MISSING]. Dies verursacht zusätzliche Latenz während der Inferenz und macht Adapter weniger geeignet für latenz-sensitive Anwendungen [SOURCE-MISSING]. Der strukturelle Nachteil gegenüber LoRA besteht darin, dass Weight Merging nicht möglich ist.

\paragraph{Prefix Tuning}

Prefix Tuning optimiert kontinuierliche aufgabenspezifische Vektoren [SOURCE-MISSING]. Prefix-Vektoren werden vor Input-Embeddings platziert, sodass nachfolgende Tokens auf Prefixes attenden können. Dies modifiziert Attention-Keys und -Values.

Die Effizienz zeigt sich darin, dass nur 0,1\,\% der Originalparameter gelernt werden [SOURCE-MISSING]. Dies stellt die extremste Parameter-Effizienz aller PEFT-Methoden dar, wobei vergleichbare Performance bei ausreichender Modellgröße erreicht wird. Input-Repräsentationen werden modifiziert statt Modellgewichte.

Stärken zeigen sich in spezifischen Kontexten, insbesondere bei multilingualer Adaptation [SOURCE-MISSING]. Prefix Tuning ist LoRA in mehrsprachigen Tasks überlegen, mit bis zu 28\,\% höherer Accuracy auf XNLI, 13\,\% höherer F1 auf XQUAD und 18\,\% höherer Accuracy auf Belebele im Vergleich zu Base Models. Zusätzliche 4 bis 6\,\% Verbesserung gegenüber LoRA sind dokumentiert.

Limitierungen bestehen in der Reduktion der effektiven Sequenzlänge durch Prefix-Tokens sowie Inferenz-Overhead durch zusätzliche Kontext-Tokens [SOURCE-MISSING]. Dies erhöht den Computational Cost während der Inferenz. Sensitivität gegenüber Initialisierung [SOURCE-MISSING] und challenging Training bei reduzierter verfügbarer Sequenzlänge [SOURCE-MISSING] kommen hinzu.

\paragraph{Prompt Tuning}

Prompt Tuning verwendet lernbare „Soft Prompts" [SOURCE-MISSING]. Dies sind kontinuierliche Embeddings statt diskreter Tokens, die via Backpropagation trainiert werden. Nur Prompt-Embeddings sind trainierbar.

Das Skalierungsverhalten zeigt, dass Prompt Tuning mit steigender Modellgröße kompetitiver wird [SOURCE-MISSING]. Ab mehreren Milliarden Parametern matcht es Full Fine-Tuning. Bei kleineren Modellen mit weniger als 11 Milliarden Parametern zeigt sich geringere Performance.

Die Effizienz manifestiert sich in extremer Parameter-Effizienz durch diskrete Token-Level-Optimierung [SOURCE-MISSING] mit minimalen Speicheranforderungen. Input-Repräsentationen werden modifiziert wie bei Prefix Tuning.

Limitierungen umfassen Inferenz-Overhead durch Prepending task-spezifischer Tokens [SOURCE-MISSING], zusätzlichen Computational Cost durch Prefix-Tokens sowie erhöhte Latenz gegenüber gefrorenen Backbone-Modellen [SOURCE-MISSING]. Sensitive Initialisierung [SOURCE-MISSING] stellt eine weitere Herausforderung dar.

\paragraph{Vergleichende Bewertung}

Die Parameter-Effizienz ordnet sich aufsteigend wie folgt: Prompt Tuning mit circa 0,01\,\% stellt die extremste Reduktion dar, Prefix Tuning mit circa 0,1\,\% ist sehr gering, LoRA mit 0,1 bis 1\,\% je nach Rank ist stark reduziert, während Adapter mit circa 3,6\,\% moderat reduziert sind.

Bezüglich Inferenz-Overhead in aufsteigender Reihenfolge: LoRA verursacht annähernd null Overhead durch Weight Merging und ist optimal. Prefix Tuning und Prompt Tuning verursachen geringen Overhead durch zusätzliche Tokens. Adapter verursachen hohen Overhead durch zusätzliche Layer und sind problematisch.

Die Performance-Charakteristika zeigen für LoRA [SOURCE-MISSING] Performance auf gleichem Niveau oder besser als Full Fine-Tuning bei RoBERTa, DeBERTa, GPT-2 und GPT-3. LoRA übertrifft Prompt Tuning in Gesamtperformance, Memory und Flexibilität, wobei ein Performance-Gap bei komplexen, heterogenen Datensätzen besteht [SOURCE-MISSING].

Adapter [SOURCE-MISSING] zeigen konsistent starke Performance auf Benchmarks, wobei GLUE innerhalb von 0,4\,\% des Full Fine-Tunings liegt. Zuverlässige Performance über verschiedene Tasks hinweg ist dokumentiert.

Prefix Tuning [SOURCE-MISSING] ist superior in multilingualen Adaptation-Tasks und besser in zeit-beschränkten Szenarien mit 3 bis 5 Minuten Training. Es outperformt LoRA in spezifischen Kontexten.

Prompt Tuning zeigt sich schwächer bei kleinen Modellen, ist jedoch konkurrenzfähig ab mehr als 11 Milliarden Parametern.

Die Training-Dynamik zeigt für LoRA langsame Konvergenz mit 5 bis 6-fach mehr Iterationen [SOURCE-MISSING]. Prompt und Prefix Tuning sind initialisierungs-sensitiv [SOURCE-MISSING]. Adapter zeigen stabile Konvergenz, sind aber speicher-intensiv.

Die Wahl von LoRA für diese Arbeit begründet sich durch mehrere Faktoren. Kein Inferenz-Overhead ist kritisch für die Produktionsumgebung. Gute Balance zwischen Parameter-Effizienz und Performance wird erreicht. Modularität ermöglicht mehrere Task-Adapter. Hardware-Feasibility durch Kombination mit QLoRA für Consumer-GPUs ist gegeben. Bewährte Performance in ähnlichen Domänen-Adaptionen ist dokumentiert. Community-Support bietet umfangreiche Implementierungen und Best Practices.

Die Konvergenz-Limitierung ist akzeptabel, da der kleine Datensatz kurze Trainings-Episoden weniger kritisch macht. Unsloth-Optimierungen kompensieren teilweise, und Quality over Speed macht finale Performance wichtiger als Trainingszeit.
        
        \paragraph{Unsloth -- Optimierungsframework für effizientes Training}

Unsloth stellt ein Optimierungsframework für effizientes Training dar, das spezifische Problemstellungen adressiert. Auch PEFT-Methoden wie LoRA erfordern bei 7-Milliarden-Parameter-Modellen erhebliche Ressourcen. Traditionelle Frameworks wie Hugging Face Transformers und PyTorch stoßen an Hardware-Limitierungen. Instabilitäten bei LoRA-Training manifestieren sich in Gradient Explosion und numerischer Instabilität [SOURCE-MISSING]. Die Zugangshürde besteht in der Notwendigkeit von Enterprise-Grade GPUs oder Multi-GPU-Setups. Die Relevanz für diese Arbeit ergibt sich aus Consumer-Hardware-Constraints, wie in Abschnitt~\ref{sec:6.1} dargestellt.

Die technischen Optimierungen umfassen mehrere Komponenten. Speichereffizienz mit 50 bis 80\,\% Reduktion des VRAM-Bedarfs [SOURCE-MISSING] wird durch 4-bit Quantization der Base-Model-Weights mit circa 70\,\% Memory-Reduktion erreicht [SOURCE-MISSING]. Die optimierte QLoRA-Implementation kombiniert LoRA-Adapter mit 4-bit Quantization [SOURCE-MISSING]. Gradient Checkpointing reduziert die Aktivierungs-Speicherung. Dies ermöglicht 7-Milliarden-Parameter-Modelle auf Consumer-GPUs wie RTX 3090 und Tesla T4 [SOURCE-MISSING]. Praktisch bedeutet dies Training ohne Qualitätsverlust bei drastisch reduziertem VRAM.

Training-Beschleunigung um das 2 bis 5-fache [SOURCE-MISSING] wird durch Flash Attention 2 für IO-optimierte Attention-Berechnung [SOURCE-MISSING] erreicht. Dies reduziert Transfers zwischen GPU-Memory-Komponenten [SOURCE-MISSING] und ermöglicht effizientere Key-Value-Query-Matrix-Operationen. Custom Kernel Implementations mittels OpenAI Triton [SOURCE-MISSING] nutzen Fused Operations mit weniger Speicherzugriffen, optimierte Matrix-Multiplikationen für LoRA sowie Custom PyTorch Autograd-Funktionen. Praktisch bedeutet dies Training-Epochen in 26 Minuten statt Stunden [SOURCE-MISSING].

Numerische Stabilität wird durch Gradient Clipping gegen Gradient Explosion [SOURCE-MISSING], Layer Normalization Calibration für LoRA-spezifische Instabilitäten [SOURCE-MISSING] sowie Multi-Precision Support für 8-bit und bfloat16 zur Hardware-Flexibilität [SOURCE-MISSING] gewährleistet. Dies verhindert Training-Abbrüche durch Overflow.

Die PEFT-Integration unterstützt LoRA-Target-Modules wie q\_proj, k\_proj, v\_proj, o\_proj, gate\_proj, up\_proj und down\_proj [SOURCE-MISSING]. Vollständige Kompatibilität mit Hugging Face PEFT sowie nahtlose Integration in bestehende Workflows ist gegeben.

Demokratisierung und Zugänglichkeit manifestieren sich in Hardware-Accessibility durch Single-GPU statt Multi-GPU [SOURCE-MISSING]. Consumer-Grade Hardware wie RTX 3090, NVIDIA A40 und Tesla T4 [SOURCE-MISSING] sowie Free Cloud Platforms wie Google Colab mit T4 und TPU sowie Kaggle mit P100 [SOURCE-MISSING] werden unterstützt. Strukturierte Templates in Form von Guided Notebooks senken die Einstiegshürde [SOURCE-MISSING]. Die Relevanz besteht darin, dass Forschung ohne Enterprise-Budget ermöglicht wird [SOURCE-MISSING].

Synergieeffekte entstehen durch die Kombination mehrerer Ansätze. LoRA reduziert trainierbare Parameter und damit den Rechenaufwand. Unsloth optimiert verbleibende Operationen für schnellere Ausführung. QLoRA kombiniert mit Unsloth Memory-Effizienz mit Speed-Optimierung [SOURCE-MISSING], ohne Qualitätseinbußen gegenüber Standard-Fine-Tuning.

Die Abgrenzung zu anderen Frameworks besteht darin, dass DeepSpeed für Multi-GPU-Verteiltes Training konzipiert ist, während Unsloth auf Single-GPU-Optimierung fokussiert. Unsloth ist komplementär zu PEFT und kein alternatives Verfahren. Der Fokus liegt auf Accessibility statt Scale.

Praktische Implikationen für diese Arbeit umfassen höhere Iterationsgeschwindigkeit, die mehr Experimente in gleicher Zeit ermöglicht. Hardware-Feasibility macht Training auf verfügbarer Hardware möglich. Energieeffizienz durch reduzierte Trainingszeit führt zu geringerem Energieverbrauch. Stabilität gewährleistet zuverlässigere Training-Runs in Constrained Environments [SOURCE-MISSING]. Die konkrete Anwendung, Konfiguration und Trainings-Logs werden in Abschnitt~\ref{sec:6.2.1} dargestellt.
        
        
        \subsubsection{Prompt Engineering}
        \label{sec:2.2.2}
        
        % Ziel: 1,5-2 Seiten
        
        % EINLEITUNG - PROMPT ENGINEERING ALS KOMPLEMENTÄRE METHODE:
        % - Fine-Tuning (Abschnitt~\ref{sec:2.2.1}): Permanente Modellanpassung durch Parameteränderung
        % - Prompt Engineering: Verhaltenssteuerung ohne Gewichtsänderungen
        % - Synergieeffekt: Fine-Tuning für domänenspezifische Fähigkeiten, Prompts für aufgabenspezifische Kontrolle
        % - Relevanz für diese Arbeit: Stilrichtlinien, Formatvorgaben, Konsistenz
        
        % VON ZERO-SHOT ZU FEW-SHOT:
        % - Zero-Shot Prompting: Nur Instruktion, keine Beispiele
        %   * Definition: Modell erhält Aufgabenbeschreibung ohne Demonstrationen
        %   * Stärke: Funktioniert bei großen, instruction-tuned Modellen (GPT-4, Claude)
        %   * Limitation: Base Models (wie LeoLM-7B) haben schwache Zero-Shot-Performance
        %   * Nicht ausreichend für komplexe Transformationsaufgaben
        %   * Beispiel: "Schreibe diese Verkehrsanweisung verständlich um." → Unpräzise Ergebnisse
        %
        % - Few-Shot Learning: Aufgabe + Beispiele im Prompt \cite{wei2021finetuned,berryman2024prompt}
        %   * Definition: In-Context Learning durch Demonstration gewünschten Verhaltens
        %   * Mechanismus: Modell erkennt Muster aus Input-Output-Paaren
        %   * Anzahl Beispiele: Typisch 1-10 Shots (Context-Window-limitiert) \cite{berryman2024prompt}
        %   * Empirische Erkenntnisse: Dramatische Leistungssteigerung vs. Zero-Shot \cite{wei2021finetuned}
        %   * Performance skaliert mit Beispielanzahl (bis zu Plateau-Effekt) \cite{berryman2024prompt}
        %
        % - Few-Shot vs. Fine-Tuning \cite{berryman2024prompt}:
        %   * Few-Shot: Keine permanenten Änderungen, flexibel anpassbar
        %   * Fine-Tuning: Modellgewichte ändern sich, domänenspezifische Expertise
        %   * Trade-off: Few-Shot benötigt längere Prompts (Kontext-Token-Kosten)
        %   * Kombination optimal: Fine-Tuning für Grundfähigkeiten + Few-Shot für Nuancen
        
        % FEW-SHOT PROMPT DESIGN - PRAKTISCHE ÜBERLEGUNGEN:
        %
        % 1. Beispielauswahl (Example Selection) \cite{berryman2024prompt}:
        %    - Diversität: Abdeckung verschiedener Störungstypen (Bauarbeiten, technische Störungen, Umleitungen)
        %    - Repräsentativität: Häufigste Fälle zuerst, Edge Cases später
        %    - Qualität über Quantität: 3 perfekte Beispiele > 10 durchschnittliche \cite{berryman2024prompt}
        %    - Balancierung: Gleiche Verteilung wie im Datensatz (siehe Abschnitt~\ref{sec:2.2.3})
        %
        % 2. Beispielreihenfolge (Example Ordering) \cite{berryman2024prompt}:
        %    - Recency Bias: Letzte Beispiele haben stärksten Einfluss
        %    - Strategie: Komplexität aufsteigend (einfach → schwierig)
        %    - Ähnlichkeitsbasierte Sortierung: Relevanteste Beispiele zuerst
        %
        % 3. Formatierung und Struktur \cite{berryman2024prompt}:
        %    - Klare Trennung zwischen Input und Output (z.B. "Input:", "Output:")
        %    - Konsistente Formatierung über alle Beispiele hinweg
        %    - Delimiter zwischen Beispielen (z.B. "---", "\n\n")
        %    - Template-Konsistenz: Gleiche Struktur wie bei Inferenz
        
        % INSTRUCTION PROMPTING - SYSTEMANWEISUNGEN:
        %
        % - Definition: Explizite Aufgabenbeschreibung vor Beispielen
        % - Komponenten effektiver Instructions \cite{berryman2024prompt,ouyang2022training}:
        %   * Rollenspezifikation: "Du bist ein Experte für Fahrgastinformation im ÖPNV"
        %   * Aufgabendefinition: Präzise Beschreibung der Transformation
        %   * Constraints: Was NICHT getan werden darf (keine Halluzinationen, keine Informationsweglassung)
        %   * Stilrichtlinien: Tonalität, Formalität, Zielgruppe
        %   * Ausgabeformat: Erwartete Struktur des generierten Texts
        %
        % - Instruction-Länge und Spezifität \cite{berryman2024prompt}:
        %   * Trade-off: Detailliert vs. flexibel
        %   * Zu vage: "Schreibe verständlich" → Interpretation variiert
        %   * Zu spezifisch: "Nutze genau 2 Sätze mit maximal 15 Wörtern" → Unnatürlich
        %   * Optimal: Prinzipien statt starre Regeln ("Bevorzuge aktive Formulierungen")
        %
        % - Negativbeispiele und Abgrenzungen \cite{berryman2024prompt}:
        %   * "Schreibe NICHT im Telegrammstil" (effektiver als nur positive Vorgaben)
        %   * "Erfinde KEINE Details, die nicht in der Eingabe stehen" (Anti-Halluzination)
        %   * Empirisch: Kombination aus positiven und negativen Constraints am effektivsten
        
        % CHAIN-OF-THOUGHT PROMPTING - KOMPLEXE REASONING:
        %
        % - Grundkonzept \cite{berryman2024prompt,bonstra2024prompt}:
        %   * Standard-Prompting: Direkte Input → Output-Zuordnung
        %   * Chain-of-Thought: Zwischenschritte explizit machen
        %   * Mechanismus: Modell zeigt Denkprozess ("Lass uns Schritt für Schritt denken") \cite{bonstra2024prompt}
        %   * Besonders effektiv bei Multi-Hop-Reasoning und komplexen Transformationen \cite{berryman2024prompt}
        %
        % - Anwendung auf Verkehrsanweisungs-Transformation:
        %   * Schritt 1: Identifiziere Kernaussage (welche Linie, welche Störung)
        %   * Schritt 2: Extrahiere Entitäten (Liniennummern, Haltestellen, Zeitangaben)
        %   * Schritt 3: Bestimme Fahrzeugtyp aus Knowledge Base
        %   * Schritt 4: Formuliere verständlichen Satz mit allen Informationen
        %   * Vorteil: Reduziert Fehler durch strukturierten Prozess
        %
        % - Zero-Shot-CoT vs. Few-Shot-CoT \cite{bonstra2024prompt}:
        %   * Zero-Shot-CoT: "Lass uns Schritt für Schritt denken" als Trigger
        %   * Few-Shot-CoT: Beispiele mit expliziten Reasoning-Schritten
        %   * Few-Shot-CoT überlegen bei domänenspezifischen Aufgaben
        %   * Relevant für diese Arbeit: Strukturierte Transformation mit Reasoning
        %
        % - Limitierungen \cite{berryman2024prompt}:
        %   * Erhöhter Token-Verbrauch durch Zwischenschritte
        %   * Nicht notwendig bei einfachen Transformationen
        %   * Produktionsumgebung: Post-Processing entfernt Reasoning (nur Endergebnis ausgeben)
        
        % PROMPT TEMPLATE DESIGN FÜR AUTOMATISIERUNG:
        %
        % - Motivation: Manuelle Prompts für einzelne Anfragen vs. Templates für Batch-Verarbeitung \cite{berryman2024prompt}
        % - Template-Struktur \cite{berryman2024prompt}:
        %   * System Prompt: Statisch, enthält Rollenspezifikation und Grundregeln
        %   * Few-Shot Examples: Statisch oder dynamisch ausgewählt
        %   * User Prompt: Variabel, enthält konkrete Verkehrsanweisung
        %   * Optional: Knowledge Context (Linien-Fahrzeug-Zuordnungen als JSON)
        %
        % - Variablen-Substitution \cite{berryman2024prompt}:
        %   * Platzhalter für dynamische Inhalte: {INPUT_INSTRUCTION}, {KNOWLEDGE_BASE}
        %   * Formatierung: JSON-Escaping, Whitespace-Normalisierung
        %   * Validierung: Prüfung auf vollständige Substitution vor Inferenz
        %
        % - Template-Typen für verschiedene Störungskategorien:
        %   * Typ A: Bauarbeiten (erfordert Alternativrouten-Information)
        %   * Typ B: Technische Störungen (erfordert Fahrzeugtyp-Präzision)
        %   * Typ C: Umleitungen (erfordert geografische Klarheit)
        %   * Adaptive Template-Auswahl basierend auf Input-Klassifizierung
        
        % PROMPT OPTIMIZATION - ITERATIVE VERBESSERUNG:
        %
        % - Systematische Optimierung \cite{berryman2024prompt}:
        %   * Baseline erstellen: Einfacher Prompt als Ausgangspunkt
        %   * Fehleranalyse: Welche Arten von Fehlern treten auf?
        %   * Gezielte Constraints hinzufügen: Adressierung spezifischer Fehlerklassen
        %   * A/B-Testing: Verschiedene Prompt-Varianten vergleichen
        %   * Iterative Refinement: Schrittweise Verbesserung
        %
        % - Automatische Prompt Optimization (kurz erwähnen) \cite{berryman2024prompt}:
        %   * APE (Automatic Prompt Engineering)
        %   * LLMs generieren und evaluieren Prompt-Varianten
        %   * Nicht in dieser Arbeit verwendet (zu ressourcenintensiv)
        %   * Manuelle Optimierung praktikabler bei kleinem Anwendungsbereich
        %
        % - Metrik-gesteuerte Optimierung \cite{berryman2024prompt}:
        %   * Faktentreue: Prüfung gegen strukturierte Eingabedaten
        %   * Stilkonformität: Einhaltung der Richtlinien (siehe Abschnitt~\ref{sec:3.2})
        %   * Konsistenz: Gleiche Eingaben → identische Ausgaben (deterministische Temperatur)
        %   * Effizienz: Token-Länge minimieren bei gleichbleibender Qualität
        
        % ADVANCED PROMPTING TECHNIQUES (KURZ):
        %
        % - Self-Consistency \cite{bonstra2024prompt}:
        %   * Mehrfache Generierung mit Sampling, dann Mehrheitsentscheidung
        %   * Reduziert Halluzinationen durch Konsensbildung
        %   * Trade-off: Mehrfache Inferenz-Kosten
        %   * Anwendbar bei kritischen Transformationen
        %
        % - Least-to-Most Prompting \cite{berryman2024prompt}:
        %   * Zerlegt komplexe Aufgaben in einfachere Teilschritte
        %   * Jeder Schritt baut auf vorherigem auf
        %   * Ähnlich zu Chain-of-Thought, aber strukturierter
        %
        % - ReAct (Reasoning + Acting) \cite{berryman2024prompt}:
        %   * Kombination aus Reasoning und Tool-Use
        %   * Modell entscheidet, wann externe Informationen benötigt werden
        %   * Nicht direkt relevant für diese Arbeit (keine externen Tools)
        
        % PROMPTING BEI BASE MODELS VS. INSTRUCTION-TUNED MODELS:
        %
        % - Base Models (wie LeoLM-7B Base) \cite{berryman2024prompt}:
        %   * Schwache Instruktionsbefolgung ohne Fine-Tuning
        %   * Tendenz zur Textkontinuation statt Aufgabenlösung
        %   * Erfordern präzisere, strukturiertere Prompts
        %   * Few-Shot Examples essentiell (nicht optional)
        %   * Completion-Framing: "Text: ... Umschreibung:" (statt "Schreibe um:")
        %
        % - Instruction-Tuned Models (wie LeoLM-7B-Instruct) \cite{berryman2024prompt,ouyang2022training}:
        %   * Bessere Zero-Shot-Performance
        %   * Verstehen imperative Formulierungen ("Transformiere", "Schreibe")
        %   * Flexiblere Prompt-Formate akzeptabel
        %   * Few-Shot verbessert weiter, aber weniger kritisch
        %
        % - Implikation für diese Arbeit \cite{berryman2024prompt}:
        %   * Base Model gewählt (siehe Kapitel~\ref{chap:4})
        %   * Prompt Engineering umso wichtiger bei Base Models
        %   * Fine-Tuning kann Prompt-Abhängigkeit reduzieren, aber nicht eliminieren
        %   * Post-Fine-Tuning Prompts: Kürzer und direkter möglich
        
        % PROMPT ENGINEERING IM KONTEXT DIESER ARBEIT:
        %
        % - Einsatz in verschiedenen Phasen:
        %   1. Pre-Fine-Tuning: Evaluation verschiedener Base Models (Kapitel~\ref{chap:4})
        %   2. Training: Few-Shot Examples als Teil des Trainingsformats (Kapitel~\ref{chap:5})
        %   3. Post-Fine-Tuning: Inference-Zeit-Prompts für Konsistenz (Kapitel~\ref{chap:6})
        %
        % - Spezifische Anwendungen:
        %   * System Prompt: LVB-Stilrichtlinien, Zielgruppendefinition (Fahrgäste)
        %   * Knowledge Integration: Linien-Fahrzeug-Zuordnungen als strukturierter Kontext
        %   * Constraint Specification: Keine Halluzinationen, vollständige Informationswiedergabe
        %   * Format Control: Einzelner Fließtext-Satz als Output
        %
        % - Synergieeffekt mit Fine-Tuning:
        %   * Fine-Tuning lernt domänenspezifisches Vokabular und Muster
        %   * Prompts steuern aufgabenspezifische Nuancen und Edge Cases
        %   * Kombination robuster als jede Methode einzeln
        %   * Reduziert Overfitting-Risiko (Abschnitt~\ref{sec:2.2.4}) durch flexible Steuerung
        
        % ÜBERLEITUNG ZU ABSCHNITT 2.2.3:
        % - Prompt Engineering: Verhaltenssteuerung zur Inferenzzeit
        % - Nächster Schritt: Datenqualität und -struktur für Fine-Tuning
        % - Zusammenspiel: Prompts definieren Aufgabe, Daten trainieren Fähigkeiten
        % - Von "Wie kommuniziere ich die Aufgabe?" zu "Wie bereite ich Trainingsdaten auf?"
        
        
        \subsubsection{Datensätze für Fine-Tuning}
        \label{sec:2.2.3}
        
        % Ziel: 2-2,5 Seiten
        
        % EINLEITUNG
        % - Qualität und Zusammenstellung von Trainingsdaten als kritischer Erfolgsfaktor
        % - Besondere Herausforderungen bei kleinen domänenspezifischen Datensätzen
        % - Transfer Learning reduziert Datenbedarf, aber ausreichende Qualität bleibt essentiell
        
        % 1. STRUKTUR VON TRAININGSDATEN
        % - Input-Output-Paare für Supervised Fine-Tuning
        % - Format-Anforderungen (JSON, JSONL, spezifische Modell-Formate)
        % - Konsistenz in Struktur und Formatierung
        % - Annotationsrichtlinien: Klare, eindeutige Richtlinien zur Qualitätssicherung
        % - Inter-Annotator Agreement als Qualitätsmetrik
        %
        % Prompt-Template-basiertes Format-Design \cite{berryman2024prompt}:
        % - Training-Daten sollten Inference-Format widerspiegeln
        % - Instruction + Few-Shot Examples + Task als Trainingsstruktur
        % - Konsistente Delimiter und Formatierung zwischen Training und Inferenz
        % - Vorteil: Modell lernt nicht nur Inhalt, sondern auch Format-Erwartungen
        % - Verweis auf Prompt Engineering-Prinzipien (siehe Abschnitt~\ref{sec:2.2.2})
        
        % 2. DATA AUGMENTATION STRATEGIEN
        % - Definition und Motivation: Vergrößerung des Datensatzes ohne zusätzliche manuelle Annotation
        % - Ziele: Generalisierung verbessern, Overfitting reduzieren, Robustheit erhöhen
        %
        % Methoden der Datenaugmentierung \cite{feng2021survey}:
        % a) RULE-BASED AUGMENTATION:
        %    - Synonym Replacement: Austausch von Wörtern durch Synonyme
        %    - Random Insertion: Einfügen zusätzlicher Wörter
        %    - Random Swap: Vertauschen von Wortpositionen
        %    - Random Deletion: Entfernen einzelner Wörter
        %    - EDA (Easy Data Augmentation) \cite{wei2019eda}: Kombination obiger Techniken
        %    - Effektiv bei kleinen Datensätzen (50-500 Beispiele)
        %
        % b) MODEL-BASED AUGMENTATION:
        %    - Back-Translation: Übersetzung in andere Sprache und zurück
        %    - Paraphrasing mit vortrainierten Modellen
        %    - Contextual Word Embeddings für Ersetzungen
        %    - Template-basierte Generierung \cite{kumar2020data}
        %    - Höhere Qualität, aber rechenintensiver
        %
        % c) SYNTHETIC DATA GENERATION:
        %    - Generierung komplett neuer Beispiele durch LLMs
        %    - Constraint-basierte Generation mit validierten Entitäten \cite{kumar2020data}
        %    - Kontrolle über Diversität und Abdeckung
        %    - Risiko: Halluzinationen und unrealistische Beispiele
        %    - Best Practice: Kombination aus realen und synthetischen Daten (typisch 20-30% synthetisch)
        %
        % Empirische Erkenntnisse:
        % - EDA zeigt 50% der Daten können gleiche Accuracy erreichen wie 100% ohne Augmentierung \cite{wei2019eda}
        % - Effekt verstärkt sich bei kleineren Datensätzen
        % - Qualität wichtiger als Quantität bei synthetischen Daten
        
        % 3. BALANCED DATASETS UND CLASS DISTRIBUTION
        % - Problematik unbalancierter Datensätze:
        %   * Modell tendiert zu häufigen Klassen
        %   * Schlechtere Performance auf seltenen, aber wichtigen Fällen
        %   * Bias in Vorhersagen
        % - Strategien für Balance \cite{feng2021survey}:
        %   * Oversampling unterrepräsentierter Kategorien
        %   * Undersampling überrepräsentierter Kategorien (mit Vorsicht)
        %   * Synthetic Minority Over-sampling (SMOTE-ähnliche Ansätze)
        %   * Class-weighted Loss Functions
        % - Empfohlene Verhältnisse:
        %   * Ideale Balance: Gleiche Anzahl pro Kategorie
        %   * Praxis: Max. 1:3 Verhältnis zwischen seltenster und häufigster Kategorie akzeptabel
        %   * Bei stärkerer Imbalance: Gezielte Augmentierung erforderlich
        
        % 4. UMGANG MIT KLEINEN DATENSÄTZEN
        % - Definition "klein": Typischerweise < 1000 Beispiele für Fine-Tuning
        % - Few-Shot Learning als Alternative \cite{wei2021finetuned}
        % - Data Efficiency Techniques:
        %   * Aggressive Augmentierung (aber Qualität wahren)
        %   * Parameter-effiziente Methoden (LoRA) reduzieren Overfitting-Risiko
        %   * Niedrigere Learning Rates
        %   * Early Stopping basierend auf Validation Loss
        % - Mixed Task Training:
        %   * Kombination domänenspezifischer und allgemeiner Daten
        %   * Verhältnis typisch 85-90% spezifisch, 10-15% allgemein \cite{raffel2020exploring}
        %   * Verhindert Catastrophic Forgetting (siehe Abschnitt~\ref{sec:2.2.4})
        %   * Erhält Generalisierungsfähigkeit
        
        % 5. TRAIN-VALIDATION-TEST-SPLIT
        % - Standard-Aufteilung: 70-80% Training, 10-15% Validation, 10-15% Test
        % - Bei kleinen Datensätzen: 80-10-10 oder Cross-Validation
        % - Wichtig: Stratified Split bei kategorischen Daten (erhält Balance)
        % - Temporale Splits bei zeitabhängigen Daten
        % - Vermeidung von Data Leakage zwischen Splits
        
        % 6. QUALITÄTSSICHERUNG
        % - Automatisierte Validierung (siehe Abschnitt~\ref{sec:5.2.2} für praktische Umsetzung):
        %   * Strukturprüfung (Format, Pflichtfelder)
        %   * Konsistenzprüfung (Entitäten, Fakten)
        %   * Duplikatserkennung
        % - Manuelle Stichprobenprüfung
        % - Iterative Verbesserung basierend auf Modellfehlern
        
        % ÜBERGANG zu Kapitel~\ref{chap:5}:
        % - Diese theoretischen Grundlagen bilden Basis für praktische Datensatzerstellung
        % - Konkrete Anwendung dieser Prinzipien in Abschnitt~\ref{sec:5.2} beschrieben
        % - Anpassung an spezifische Anforderungen der Verkehrsanweisungstransformation
        
        
        \subsubsection{Herausforderungen beim Fine-Tuning}
        \label{sec:2.2.4}
        
        % Ziel: 1,5-2 Seiten
        
        % EINLEITUNG:
        % - Fine-Tuning vortrainierter Modelle bringt spezifische Herausforderungen
        % - Spannungsfeld: Spezialisierung auf neue Aufgabe vs. Erhalt allgemeiner Fähigkeiten
        % - Besonders kritisch bei kleinen domänenspezifischen Datensätzen
        % - Drei zentrale Problemfelder: Overfitting, Catastrophic Forgetting, Konvergenz
        
        % ============================================================================
        % 1. OVERFITTING - ÜBERANPASSUNG AN TRAININGSDATEN
        % ============================================================================
        
        % DEFINITION UND SYMPTOME:
        % - Modell lernt Trainingsdaten auswendig statt Muster zu generalisieren
        % - Charakteristisch: Hohe Training Accuracy, niedrige Validation/Test Accuracy
        % - Divergenz zwischen Training und Validation Loss
        % - Besonders ausgeprägt bei kleinen Datensätzen (< 1000 Beispiele)
        % - LoRA reduziert Overfitting-Risiko gegenüber Full Fine-Tuning, eliminiert es aber nicht
        %
        % URSACHEN:
        % - Modellkomplexität übersteigt Informationsgehalt der Trainingsdaten
        % - Zu hohe Anzahl trainierbarer Parameter relativ zur Datensatzgröße
        % - Zu viele Trainings-Epochen: Modell memoriert statt zu lernen
        % - Unbalancierte Datensätze: Bias zu überrepräsentierten Kategorien
        % - Zu hohe Learning Rate: Instabile, zu schnelle Anpassung an Trainingsdaten
        %
        % Bei LoRA-Fine-Tuning spezifisch:
        % - Höherer Rank (r) → mehr trainierbare Parameter → höheres Overfitting-Risiko
        % - Mehr Target Modules → größere Angriffsfläche für Überanpassung
        % - Trade-off: Expressivität vs. Generalisierung
        %
        % VERMEIDUNGSSTRATEGIEN:
        %
        % a) Regularisierung [SOURCE-MISSING: Goodfellow et al., 2016]:
        %    - L2-Regularisierung (Weight Decay): Bestraft große Gewichtswerte
        %      * Mathematisch: Zusätzlicher Term λ||w||² zur Loss-Funktion
        %      * Fördert kleinere, gleichmäßiger verteilte Gewichte
        %    - L1-Regularisierung: Fördert Sparsity (weniger relevant für LoRA)
        %    - Dropout [SOURCE-MISSING: Srivastava et al., 2014]:
        %      * Zufälliges Deaktivieren von Neuronen während Training
        %      * Bei LoRA: Nur für lange Trainings-Episoden geeignet (siehe Abschnitt~\ref{sec:2.2.1})
        %      * Verhindert Co-Adaptationen zwischen Neuronen
        %
        % b) Early Stopping [SOURCE-MISSING: Prechelt, 1998]:
        %    - Kontinuierliches Monitoring von Validation Loss während Training
        %    - Training stoppen, wenn Validation Loss nicht mehr sinkt oder ansteigt
        %    - Typische Patience-Strategie: 3-5 Epochen ohne Verbesserung
        %    - Verhindert Überanpassung in späten Trainingsphasen
        %    - Erfordert separates Validation Set (nicht Teil der Trainingsdaten)
        %
        % c) Datenaugmentierung (siehe Abschnitt~\ref{sec:2.2.3}):
        %    - Künstliche Vergrößerung des effektiven Datensatzes
        %    - Reduziert Overfitting durch erhöhte Variabilität
        %    - Methoden: EDA, Back-Translation, Synthetic Data Generation
        %    - Erhöht Robustheit gegenüber Formulierungsvariationen
        %
        % d) Cross-Validation:
        %    - K-Fold Cross-Validation bei sehr kleinen Datensätzen
        %    - Robustere Schätzung der Generalisierungsfähigkeit
        %    - Rechenintensiv, aber aussagekräftiger bei limitierten Daten
        %
        % e) PEFT-spezifische Strategien:
        %    - Niedrigerer LoRA-Rank: Reduziert trainierbare Parameter
        %    - Selektive Target Modules: Nur kritische Layer anpassen
        %    - Trade-off: Geringere Expressivität vs. bessere Generalisierung
        %
        % UNDERFITTING VS. OVERFITTING - BALANCIERUNG:
        % - Underfitting: Modell zu einfach, erfasst Muster nicht (zu niedriger Rank, zu wenig Training)
        % - Overfitting: Modell zu komplex, memoriert Daten (zu hoher Rank, zu viel Training)
        % - Optimaler Arbeitspunkt liegt zwischen beiden Extremen
        % - Empirische Bestimmung durch Validation-Set-Performance notwendig
        % - Bias-Variance Trade-off [SOURCE-MISSING: Geman et al., 1992]
        
        % ============================================================================
        % 2. CATASTROPHIC FORGETTING - VERLUST VORTRAINIERTER FÄHIGKEITEN
        % ============================================================================
        
        % DEFINITION:
        % - Phänomen: Modell verliert während Fine-Tuning Fähigkeiten aus Pretraining-Phase
        % - Spezialisierung auf neue Aufgabe geht zu Lasten allgemeiner Kompetenzen
        % - Graduelle oder abrupte Verschlechterung auf Pretraining-Tasks
        % - Erstmals dokumentiert in neuronalen Netzen [SOURCE-MISSING: McCloskey & Cohen, 1989]
        % - Bei LoRA: Weniger ausgeprägt als bei Full Fine-Tuning, aber vorhanden [SOURCE-MISSING: Luo et al., 2025; Yang et al., 2024]
        %
        % MANIFESTATIONEN:
        % - Wissensverlust: Fakten aus Pretraining-Korpora nicht mehr abrufbar
        % - Sprachkompetenz-Degradation: Verschlechterung grammatikalischer Fähigkeiten
        % - Task Interference: Neue Aufgabe überschreibt Wissen über alte Aufgaben
        % - Safety Alignment Degradation [SOURCE-MISSING: Qi et al., 2023; Hsu et al., 2024]:
        %   * Fine-Tuning kann Safety Guardrails schwächen
        %   * Betrifft alle PEFT-Methoden (LoRA, Adapter, Prefix Tuning)
        %   * Selbst bei benign Training Data möglich (nicht nur adversarial)
        %
        % THEORETISCHE URSACHEN:
        % - Weight Interference: Neue Gewichtsanpassungen überschreiben alte Repräsentationen
        % - Gradient Descent Bias: Optimierung favorisiert aktuelle Task über vorherige
        % - Representation Overlap: Shared Weights müssen beide Tasks gleichzeitig kodieren
        % - Distribution Shift: Trainingsverteilung unterscheidet sich stark von Pretraining-Daten
        %
        % LoRA-spezifische Faktoren:
        % - Zero-Initialisierung der B-Matrix kann Konvergenz verlangsamen [SOURCE-MISSING: Luo et al., 2025]
        % - Low-Rank Updates können wichtige Pretraining-Repräsentationen nicht vollständig bewahren
        % - Trotz eingefrorener Basisgewichte: Adapter-Outputs können Aktivierungen verzerren
        %
        % GEGENMAÄSSNAHMEN:
        %
        % a) Niedrigere Learning Rates [SOURCE-MISSING: Howard & Ruder, 2018]:
        %    - Typisch: 1e-4 bis 5e-5 für LoRA (vs. 1e-3 für Full Fine-Tuning)
        %    - Sanftere Anpassung erhält mehr Pretraining-Wissen
        %    - Trade-off: Langsamere Konvergenz
        %
        % b) LoRA als sanftere Alternative zu Full Fine-Tuning:
        %    - Originalgewichte bleiben eingefroren (siehe Abschnitt~\ref{sec:2.2.1})
        %    - Nur Low-Rank-Adapter werden trainiert
        %    - Reduziert Catastrophic Forgetting im Vergleich zu Full Fine-Tuning
        %    - Aber: Nicht vollständig eliminiert [SOURCE-MISSING: Luo et al., 2025]
        %
        % c) Mixed Task Training:
        %    - Kombination domänenspezifischer und allgemeiner Daten
        %    - Typisches Verhältnis: 85-90% spezifisch, 10-15% allgemein \cite{raffel2020exploring}
        %    - Erhält Generalisierungsfähigkeit während Spezialisierung
        %    - Beispiel: LVB-Verkehrsanweisungen + allgemeine deutsche Texte
        %
        % d) Regularisierungstechniken:
        %    - Elastic Weight Consolidation (EWC) [SOURCE-MISSING: Kirkpatrick et al., 2017]
        %    - Knowledge Distillation vom Pretraining-Modell
        %    - Bei LoRA: Weniger kritisch aufgrund gefrorener Basisgewichte
        %
        % e) Early Stopping basierend auf Generalisierungsmetriken:
        %    - Monitoring nicht nur Task-Accuracy, sondern auch allgemeine Sprachfähigkeiten
        %    - Perplexity auf Out-of-Domain-Daten als Indikator
        %    - Training stoppen bei signifikantem Anstieg
        
        % ============================================================================
        % 3. KONVERGENZ-HERAUSFORDERUNGEN BEI LORA
        % ============================================================================
        
        % LANGSAME KONVERGENZ:
        % - LoRA konvergiert signifikant langsamer als Full Fine-Tuning [SOURCE-MISSING: Wang et al., 2024]
        % - Empirisch dokumentiert: 5-6x mehr Iterationen und FLOPs für gleiche Performance
        % - Paradoxe Situation: Pro-Iteration effizienter, aber Gesamt-Trainingskosten höher
        % - Kann zu schlechterer finaler Test-Performance führen
        % - Nicht nur Geschwindigkeitsproblem, sondern fundamentale Optimierungsschwierigkeit
        %
        % ARCHITEKTONISCHE URSACHEN:
        %
        % a) Zero-Initialisierung der B-Matrix [SOURCE-MISSING: Wang et al., 2024]:
        %    - Bei Training-Start: ΔW = BA = 0 (keine initiale Störung)
        %    - Langsame Trainingsdynamik zwischen A und B in frühen Epochen
        %    - "Kurzsichtige" Inter-Layer-Interaktionen
        %    - Verzögert Entwicklung komplexer Feature-Transformationen
        %
        % b) Low-Rank Constraint [SOURCE-MISSING: Shen et al., 2025; Xia et al., 2024]:
        %    - Gewichtsupdates auf niedrigdimensionalen Unterraum beschränkt
        %    - Kann komplexe Adaptionen nicht vollständig abbilden
        %    - Limitiert Expressivität der Weight Updates
        %    - Performance-Gap zu Full Fine-Tuning bei komplexen Datensätzen [SOURCE-MISSING: Wang et al., 2025]
        %
        % c) Imbalancierte Weight Updates [SOURCE-MISSING: Zhang et al., 2025; Yen et al., 2024]:
        %    - Low-Rank-Faktorisierung ist nicht eindeutig (non-unique)
        %    - Verschiedene A-B-Kombinationen ergeben gleiches ΔW
        %    - Führt zu inkonsistenten und imbalancierten Updates
        %    - Suboptimale Konvergenzpfade im Optimierungsraum
        %
        % RANK-AUSWAHL-DILEMMA - "LOW-RANK BOTTLENECK":
        %
        % - Fundamentales Trade-off [SOURCE-MISSING: Dong et al., 2025; Biderman et al., 2024]:
        %   * Niedriger Rank (r=4-8): 
        %     - Hohe Parameter-Effizienz
        %     - Schnelles Training pro Iteration
        %     - ABER: Signifikanter Performance-Gap zu Full Fine-Tuning
        %     - Expressivität zu gering für komplexe Aufgaben
        %   
        %   * Hoher Rank (r=64-128):
        %     - Bessere Performance, nähert sich Full Fine-Tuning an
        %     - Höhere Expressivität der Weight Updates
        %     - ABER: Parameter-Kosten wachsen linear mit Rank
        %     - Schwächt fundamentalen Vorteil der Parameter-Effizienz
        %
        % - Empirische Beobachtungen:
        %   * Performance verbessert sich monoton mit steigendem Rank [SOURCE-MISSING: Dong et al., 2025]
        %   * Aber: Diminishing Returns ab bestimmtem Rank (task-abhängig)
        %   * Optimaler Rank variiert nach Modellgröße, Task-Komplexität, Datensatzgröße
        %   * Keine universelle Heuristik: Empirische Bestimmung erforderlich [SOURCE-MISSING: Rajabzadeh et al., 2024]
        %
        % - "Low-Rank Bottleneck":
        %   * Narrowing Performance-Gap erfordert Rank-Erhöhung
        %   * Bei sehr hohem Rank: Annäherung an Full Fine-Tuning-Kosten
        %   * Fundamentale Limitation der Low-Rank-Annahme
        %   * Theoretische Grenze der PEFT-Effizienz
        %
        % PERFORMANCE-GAP ZU FULL FINE-TUNING:
        % - Konsistent dokumentiert über verschiedene Benchmarks [SOURCE-MISSING: Tastan et al., 2025; Biderman et al., 2024]
        % - Besonders ausgeprägt bei:
        %   * Komplexen Datensätzen mit diversen Sub-Domänen
        %   * Tasks mit hoher semantischer Variabilität
        %   * Kleinen Modellen (< 7B Parameter) [SOURCE-MISSING: Wang et al., 2025]
        % - Gap verringert sich mit:
        %   * Größeren Basismodellen
        %   * Höherem LoRA-Rank
        %   * Längeren Trainings-Episoden
        %
        % OVERFITTING BEI LORA-TRAINING:
        % - Widersprüchliche Phänomene [SOURCE-MISSING: Mao et al., 2024]:
        %   * Höherer Rank bringt nicht zwingend bessere Performance
        %   * Kann zu Overfitting führen, besonders bei kleinen Datensätzen
        %   * Non-monotones Verhalten: Optimum liegt oft bei mittlerem Rank
        % - Initialization Bottleneck [SOURCE-MISSING: Xue, 2025]:
        %   * Zero-Initialisierung limitiert Aktivierung der Originalgewichte
        %   * Kann optimale Performance-Pfade blockieren
        
        % ============================================================================
        % ZUSAMMENFASSUNG & INTERAKTIONEN
        % ============================================================================
        
        % WECHSELWIRKUNGEN ZWISCHEN HERAUSFORDERUNGEN:
        % - Overfitting und Catastrophic Forgetting:
        %   * Beide profitieren von Regularisierung und Early Stopping
        %   * Gegenläufig: Overfitting will Spezialisierung, CF will Generalisierung
        %   * Balance erforderlich: Weder zu spezialisiert noch zu generisch
        %
        % - Konvergenz und Overfitting:
        %   * Langsame Konvergenz kann Overfitting verzögern (implizite Regularisierung)
        %   * Zu viele Epochen: Langsame Konvergenz führt trotzdem zu Overfitting
        %   * Early Stopping muss beide Faktoren berücksichtigen
        %
        % - Learning Rate als zentraler Hebel:
        %   * Zu hoch: Schnelle Konvergenz, aber Overfitting und CF-Risiko
        %   * Zu niedrig: Langsame Konvergenz, paradoxerweise auch Overfitting möglich
        %   * Optimal: Task- und datensatzabhängig, empirisch zu bestimmen
        %
        % THEORETISCHE PERSPEKTIVE:
        % - Fine-Tuning als Optimierungsproblem mit multiplen Constraints
        % - Keine universelle Lösung: Trade-offs zwischen verschiedenen Zielen
        % - PEFT-Methoden wie LoRA: Verschieben Trade-offs, eliminieren sie nicht
        % - Empirische Validierung unerlässlich für praktische Anwendungen


    \subsection{Ressourceneffizienz und Modelloptimierung}
    \label{sec:2.3}
    
    % Ziel: 4-5 Seiten
    
        \subsubsection{Problematik großer Sprachmodelle}
        \label{sec:2.3.1}
        
        % Ziel: 1,5 Seiten
        % - Energieverbrauch und CO2-Bilanz großer Modelle
        % - Überdimensionierung in der Praxis
        % - Wirtschaftliche und ökologische Implikationen
        % - Notwendigkeit ressourceneffizienter Alternativen
        
        
        \subsubsection{Quantisierung}
        \label{sec:2.3.2}
        
        % Ziel: 2,5-3 Seiten
        % AUSFÜHRLICH, da zentral für die Arbeit:
        % - Grundprinzip der Quantisierung
        % - INT8/INT4-Quantisierung
        % - Theoretische Einsparungen bei Speicher und Rechenleistung
        % - Speicherbedarf (konkrete Zahlen aus Literatur)
        % - Inferenzgeschwindigkeit
        % - Energieverbrauch (theoretische Berechnungen)
        % - Trade-off: Effizienz vs. Qualität
        % - Quantisierung vor vs. nach Fine-Tuning
        % - Verfügbare Open-Source-Tools (llama.cpp, bitsandbytes, GPTQ)
        % - Erwartete praktische Implikationen
        
        
        \subsubsection{Weitere Optimierungsansätze}
        \label{sec:2.3.3}
        
        % Ziel: 1,5-2 Seiten
        % - RAG-Systeme (Retrieval-Augmented Generation)
        %   * Konzept: Dynamischer Retrieval + Generation
        %   * Zwei-Stufen-Prozess (Retrieval aus Vektorstore, dann LLM-Generation)
        %   * Vorteile: Skalierung auf große Wissensbasen, Aktualität
        %   * Anwendungsfälle im Verkehrssektor
        %   * Hardware-Anforderungen und Komplexität
        % - Knowledge-Enhanced Prompting als vereinfachte Alternative
        %   * Statische Einbettung von Domänenwissen in System-Prompts
        %   * Vorteile: Einfachere Implementierung, geringere Hardware-Anforderungen
        %   * Limitierungen: Begrenzte Wissensbasis (Context-Window), keine Dynamik
        %   * Geeignet für überschaubare, stabile Domänen
        % - Modellkompression (Pruning, Destillation - kurz erwähnen)
        % - Vergleich verschiedener Ansätze


    \subsection{Qualitätssicherung bei KI-generierten Texten}
    \label{sec:2.4}
    
    % Ziel: 2-2,5 Seiten
    
        \subsubsection{Evaluationsmetriken für NLP}
        \label{sec:2.4.1}
        
        % Ziel: 1 Seite
        
        % AUTOMATISCHE METRIKEN:
        % - BLEU, ROUGE (kurz): N-gram Overlap-basiert
        %   * Ursprünglich für Maschinenübersetzung entwickelt
        %   * Limitiert für semantische Äquivalenz
        %   * Schnell berechenbar, aber oberflächlich
        %
        % - Semantische Ähnlichkeitsmetriken (BERTScore):
        %   * Contextualized Embeddings statt Surface-Form
        %   * Bessere Erfassung von Paraphrasen
        %   * Rechenintensiver, aber aussagekräftiger
        %
        % - Perplexity:
        %   * Misst Konfidenz des Modells
        %   * Niedriger = besseres Language Modeling
        %   * Nicht direkt für Qualität, aber für Training-Monitoring
        %
        % PROMPT-BASIERTE EVALUATION \cite{berryman2024prompt}:
        % - LLM-as-a-Judge: Nutzung größerer Modelle zur Bewertung
        %   * Prompt: "Bewerte die Qualität dieser Transformation auf Skala 1-5"
        %   * Kriterien: Faktentreue, Verständlichkeit, Stilkonformität
        %   * Vorteil: Flexibel, domänenspezifisch anpassbar
        %   * Limitation: Bias des Evaluator-Modells, Kosten
        %
        % - Comparative Evaluation \cite{berryman2024prompt}:
        %   * A/B-Vergleich zweier Outputs
        %   * "Welcher Text ist verständlicher: A oder B?"
        %   * Robuster als absolute Bewertungen
        %   * Nützlich für Prompt-Optimierung (siehe Abschnitt~\ref{sec:2.2.2})
        %
        % DOMÄNENSPEZIFISCHE METRIKEN:
        % - Faktentreue: Prüfung gegen strukturierte Eingabedaten
        %   * Entitätsextraktion: Liniennummer, Haltestellen, Zeitangaben
        %   * Vollständigkeitsprüfung: Alle Input-Informationen im Output?
        %   * Halluzination Detection: Erfundene Details? (siehe Abschnitt~\ref{sec:2.4.2})
        %
        % - Stilkonformität:
        %   * Einhaltung von Stilrichtlinien (siehe Abschnitt~\ref{sec:3.2})
        %   * Regelbasierte Checks: Passiv vs. Aktiv, Fachbegriffe vs. Allgemeinsprache
        %   * Pattern-Matching für verbotene Formulierungen
        %
        % - Konsistenz:
        %   * Gleiche Eingabe → identische Ausgabe (bei deterministischer Temperatur)
        %   * Messung: Wiederholte Generierung, Hamming-Distanz zwischen Outputs
        %   * Wichtig für Produktionsumgebung: Vorhersagbarkeit
        
        
        \subsubsection{Halluzination Detection und Validierung}
        \label{sec:2.4.2}
        
        % Ziel: 1-1,5 Seiten
        
        % PROBLEMSTELLUNG:
        % - Halluzinationen: Modell generiert faktisch inkorrekte oder erfundene Informationen
        % - Besonders kritisch bei sicherheitsrelevanten Verkehrsinformationen
        % - Herausforderung: Plausibel klingende, aber falsche Aussagen
        % - Beispiel: "Straßenbahn Linie 10" statt korrekter "Bus Linie 10"
        %
        % SELF-CONSISTENCY CHECKS \cite{bonstra2024prompt}:
        % - Grundprinzip: Mehrfache Generierung mit Sampling (siehe Abschnitt~\ref{sec:2.2.2})
        % - Mechanismus:
        %   * Gleiche Eingabe → N verschiedene Outputs generieren (typisch N=3-10)
        %   * Sampling mit Temperature > 0 für Variabilität
        %   * Mehrheitsentscheidung oder Konsensanalyse
        % - Annahme: Korrekte Antworten konvergieren, Halluzinationen divergieren
        % - Vorteil: Keine externe Wissensquelle erforderlich
        % - Limitation: Mehrfache Inferenz-Kosten (N-facher Rechenaufwand)
        % - Anwendung: Kritische Transformationen, wo Faktentreue essentiell ist
        %
        % FACT CHECKING GEGEN EINGABEDATEN:
        % - Strukturierte Validierung \cite{berryman2024prompt}:
        %   * Extraktion von Entitäten aus Input und Output
        %   * Vergleich: Sind alle Input-Entitäten im Output enthalten?
        %   * Regel: Output darf KEINE Entitäten enthalten, die nicht im Input sind
        % - Named Entity Recognition (NER) für Extraktion:
        %   * Liniennummern, Haltestellen, Zeitangaben, Fahrzeugtypen
        %   * Pattern-Matching oder NER-Modell
        % - Constraint-basierte Validierung:
        %   * Whitelist-Ansatz: Nur bekannte Liniennummern erlaubt
        %   * Knowledge Base: Linien-Fahrzeug-Zuordnungen als Ground Truth
        %   * Reject-Output bei Verletzung von Constraints
        %
        % PROMPT-BASIERTE HALLUZINATION PREVENTION \cite{berryman2024prompt}:
        % - Negative Constraints in System Prompt (siehe Abschnitt~\ref{sec:2.2.2}):
        %   * "Erfinde KEINE Details, die nicht in der Eingabe stehen"
        %   * "Nutze NUR Informationen aus dem bereitgestellten Kontext"
        %   * "Bei Unsicherheit: NICHT spekulieren, sondern Information weglassen"
        % - Knowledge Grounding:
        %   * Explizite Einbettung von Fakten im Prompt
        %   * Beispiel: "Linie 10 ist ein Bus, Linie 11 ist eine Straßenbahn"
        %   * Reduziert Halluzinationen durch direkte Verfügbarkeit von Wissen
        % - Chain-of-Thought für Faktentreue:
        %   * Zwischenschritt: "Welche Informationen sind in der Eingabe enthalten?"
        %   * Explizite Verifikation vor Generierung
        %   * Erhöht Token-Kosten, aber verbessert Zuverlässigkeit
        %
        % AUTOMATISIERTE VS. MANUELLE EVALUATION:
        % - Automatisiert (skalierbar, aber limitiert):
        %   * Entitätsvergleich: Schnell, deterministisch
        %   * Pattern-Matching: Regelbasiert, wartungsintensiv
        %   * Self-Consistency: Rechenintensiv, keine externe Referenz
        % - Manuell (präzise, aber teuer):
        %   * Human Evaluation: Gold Standard, aber nicht skalierbar
        %   * Stichprobenbasiert: Regelmäßige Quality Checks
        %   * Iterative Verbesserung: Fehleranalyse → Prompt-Anpassung
        % - Hybrid-Ansatz (optimal für Praxis):
        %   * Automatisierte Pre-Filter: Offensichtliche Fehler abfangen
        %   * Manuelle Review: Grenzfälle und Stichproben
        %   * Feedback-Loop: Erkannte Fehler → Prompt-Optimierung
        %
        % PRAKTISCHE ANSÄTZE FÜR RESSOURCENLIMITIERTE UMGEBUNGEN:
        % - Deterministische Temperatur (T=0) reduziert Halluzinationen \cite{berryman2024prompt}
        %   * Trade-off: Weniger kreativ, aber konsistenter
        %   * Geeignet für faktische Transformationen
        % - Kleinere Validierungsmodelle:
        %   * Leichtgewichtige NER-Modelle für Entitätsextraktion
        %   * Regelbasierte Checks statt LLM-as-a-Judge
        % - Cached Knowledge Integration:
        %   * Statische Wissensbasis im Prompt (kein RAG-Overhead)
        %   * Siehe Abschnitt~\ref{sec:2.3.3} für Knowledge Integration ohne RAG
        %
        % ÜBERGANG ZU KAPITEL 7 (EVALUATION):
        % - Theoretische Konzepte hier: Methodik und Prinzipien
        % - Praktische Anwendung in Kapitel~\ref{chap:7}: Konkrete Metriken und Ergebnisse
        % - Kombination automatisierter und manueller Validierung in Implementierung


