\section{Implementierung} \label{chap:6}

% Ziel: 14-16 Seiten

    \subsection{Technische Umsetzung}
    \label{sec:6.1}
    
    % Ziel: 2-3 Seiten
    % - Entwicklungsumgebung und Hardware
    % - Frameworks und Bibliotheken (Transformers, PEFT, etc.)
    % - Software-Architektur (nur Überblick, Details in Anhang)


    \subsection{Fine-Tuning des Modells}
    \label{sec:6.2}
    
    % Ziel: 8-10 Seiten
    % KERNKAPITEL - hier passiert die Hauptarbeit
    
        \subsubsection{LoRA-basiertes Fine-Tuning}
        \label{sec:6.2.1}
        
        % Ziel: 4-5 Seiten
        % - Konfiguration der LoRA-Parameter
        %   * Rank, Alpha, Target Modules
        %   * Begründung der Parameterwahl
        % - Trainingsprozess
        %   * Hyperparameter (Learning Rate, Batch Size, Epochs)
        %   * Trainingsverhalten und Konvergenz
        %   * Overfitting-Vermeidung
        %     - KRITISCH: Datensatz-Shuffling zwischen Epochen
        %       * Selbst bei kleinen Datensätzen (100-500 Beispiele) essentiell
        %       * Verhindert sequentielles Auswendiglernen gleichartiger Muster
        %       * Ohne Shuffling: Schnelles Overfitting bereits nach wenigen 100 Beispielen
        %       * Theoretische Grundlage: Siehe Abschnitt~\ref{sec:2.2.4} (Overfitting-Vermeidung)
        %       * Praktische Umsetzung: Randomisierung vor jeder Epoche mittels PyTorch DataLoader
        %     - Learning-Rate-Paradoxon (empirische Beobachtung):
        %       * Initial vermutete konservative Learning Rate (5e-5) erwies sich als kontraproduktiv
        %       * Zu langsames Lernen führte zu direktem Memorization-Verhalten
        %       * Moderate Learning Rate (2e-4) ermöglichte bessere Generalisierung
        %       * Bestätigt theoretische Diskussion in Abschnitt~\ref{sec:2.2.4}
        % - Iterative Optimierung
        %   * Anpassungen basierend auf Validierungsergebnissen
        %   * Verschiedene Experimente
        %
        % - UNSLOTH-OPTIMIERTES TRAINING:
        %   * Einsatz von Unsloth-Framework (siehe theoretische Grundlagen Abschnitt~\ref{sec:2.2.1})
        %   * Begründung: Hardware-Limitierungen (7B-Modell auf Consumer-GPU)
        %   * TODO: TRAININGS-LOGS AUSWERTEN!
        %     - Training-Zeit (Vergleich mit/ohne Unsloth falls vorhanden)
        %     - VRAM-Verbrauch während des Trainings
        %     - Iterations-Geschwindigkeit (Samples/Sekunde)
        %     - Loss-Kurven und Konvergenzverhalten
        %     - Praktische Vorteile in der iterativen Entwicklung
        
        
        \subsubsection{Prompt Engineering und Automatisierung}
        \label{sec:6.2.2}
        
        % Ziel: 3-4 Seiten
        % - Design der Prompt-Templates
        %   * System-Prompts
        %   * Instruction-Format
        %   * Few-Shot-Beispiele (wenn verwendet)
        % - Knowledge-Enhanced System Prompts (statt RAG)
        %   * Integration domänenspezifischer Informationen
        %   * Statische Einbettung von Zuordnungen (Liniennummern zu Fahrzeugtypen)
        %   * Strukturierte Faktendatenbank im Prompt
        %   * Begründung: Hardware-Limitierungen, ausreichend für überschaubare Domäne
        %   * Abgrenzung zu vollständigem RAG-System (siehe Abschnitt~\ref{sec:2.3.3})
        % - Automatisierungslogik
        %   * Preprocessing-Pipeline
        %   * Template-Filling
        %   * Format-Erkennung
        % - Iteration und Verfeinerung
        %   * A/B-Tests verschiedener Prompts
        %   * Optimierung für Konsistenz
        
        
        \subsubsection{Quantisierung}
        \label{sec:6.2.3}
        
        % Ziel: 1 Seite
        % - Anwendung der Quantisierung auf das fine-getunte Modell
        % - INT8 und INT4 Varianten
        % - Technische Umsetzung mit Open-Source-Tools
        % - Gemessene Inferenzgeschwindigkeit (falls messbar)


    \subsection{Qualitätssicherung und Validierung}
    \label{sec:6.3}
    
    % Ziel: 3-4 Seiten
    % - Implementierung automatisierter Checks
    % - Halluzination Detection
    % - Semantische Konsistenzprüfung
    % - Feedback-Loop für Verbesserungen

