% ============================================================================
% GRUNDLAGEN: TRANSFORMER & NLP
% ============================================================================

@article{vaswani2017attention,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017},
  publisher = {Curran Associates, Inc.}
}

@article{devlin2018bert,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {arXiv preprint arXiv:1810.04805},
  year      = {2018}
}

@article{radford2018improving,
  author    = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  title     = {Improving Language Understanding by Generative Pre-Training},
  year      = {2018},
  publisher = {OpenAI}
}

@article{radford2019language,
  author    = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language Models are Unsupervised Multitask Learners},
  year      = {2019},
  publisher = {OpenAI}
}

@article{raffel2020exploring,
  author    = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal   = {Journal of Machine Learning Research},
  volume    = {21},
  number    = {140},
  pages     = {1--67},
  year      = {2020}
}

% ============================================================================
% TRANSFER LEARNING
% ============================================================================

@article{pan2010survey,
  author    = {Sinno Jialin Pan and Qiang Yang},
  title     = {A Survey on Transfer Learning},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  volume    = {22},
  number    = {10},
  pages     = {1345--1359},
  year      = {2010},
  publisher = {IEEE}
}

@inproceedings{howard2018universal,
  author    = {Jeremy Howard and Sebastian Ruder},
  title     = {Universal Language Model Fine-tuning for Text Classification},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {328--339},
  year      = {2018},
  publisher = {Association for Computational Linguistics}
}

% ============================================================================
% MISTRAL ARCHITECTURE
% ============================================================================

@article{jiang2023mistral,
  author    = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L{\'e}lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  title     = {Mistral 7B},
  journal   = {arXiv preprint arXiv:2310.06825},
  year      = {2023}
}

% ============================================================================
% LEOLM / DEUTSCHSPRACHIGE MODELLE
% ============================================================================

@misc{leolm2023,
  author       = {{HessianAI}},
  title        = {{LeoLM}: Linguistically Enhanced Open Language Model for German},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/LeoLM}},
  note         = {Accessed: 2025-11-12}
}

% ============================================================================
% INSTRUCTION TUNING
% ============================================================================

@article{wei2021finetuned,
  author    = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title     = {Finetuned Language Models Are Zero-Shot Learners},
  journal   = {arXiv preprint arXiv:2109.01652},
  year      = {2021}
}

@article{ouyang2022training,
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  journal   = {arXiv preprint arXiv:2203.02155},
  year      = {2022}
}

% ============================================================================
% DATA AUGMENTATION & SYNTHETIC DATA
% ============================================================================

@inproceedings{wei2019eda,
  author    = {Jason Wei and Kai Zou},
  title     = {{EDA}: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages     = {6382--6388},
  year      = {2019},
  publisher = {Association for Computational Linguistics}
}

@article{feng2021survey,
  author    = {Steven Y. Feng and Varun Gangal and Jason Wei and Sarath Chandar and Soroush Vosoughi and Teruko Mitamura and Eduard Hovy},
  title     = {A Survey of Data Augmentation Approaches for {NLP}},
  booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
  pages     = {968--988},
  year      = {2021},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{kumar2020data,
  author    = {Varun Kumar and Ashutosh Choudhary and Eunah Cho},
  title     = {Data Augmentation using Pre-trained Transformer Models},
  booktitle = {Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems},
  pages     = {18--26},
  year      = {2020},
  publisher = {Association for Computational Linguistics}
}

% ============================================================================
% UNSLOTH & TRAINING OPTIMIZATION
% ============================================================================

@article{dao2022,
  author    = {Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher R{\'e}},
  title     = {{FlashAttention}: Fast and Memory-Efficient Exact Attention with {IO}-Awareness},
  journal   = {arXiv preprint arXiv:2205.14135},
  year      = {2022},
  url       = {https://api.semanticscholar.org/CorpusID:249151871}
}

@article{dettmers2023,
  author    = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  title     = {{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  journal   = {arXiv preprint arXiv:2305.14314},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:258841328}
}

% ============================================================================
% PEFT METHODS: LoRA, ADAPTER, PREFIX/PROMPT TUNING
% ============================================================================

@article{smith2023,
  author    = {James Smith and Yen-Chang Hsu and Lingyu Zhang and Ting Hua and Zsolt Kira and Yilin Shen and Hongxia Jin},
  title     = {Continual Diffusion: Continual Customization of Text-to-Image Diffusion with {C-LoRA}},
  journal   = {arXiv preprint arXiv:2304.06027},
  year      = {2023},
  volume    = {abs/2304.06027},
  url       = {https://api.semanticscholar.org/CorpusID:258078844}
}

@article{hua2023,
  author    = {Wenhui Hua and Brian Williams and Davood Shamsi},
  title     = {{LACoS-BLOOM}: Low-rank Adaptation with Contrastive objective on 8 bits {Siamese-BLOOM}},
  journal   = {arXiv preprint arXiv:2305.06404},
  year      = {2023},
  volume    = {abs/2305.06404},
  url       = {https://api.semanticscholar.org/CorpusID:258615538}
}

@article{liu2025up,
  author    = {Yating Liu and Yaowei Li and Xiangyuan Lan and Wenming Yang and Zimo Liu and Qingmin Liao},
  title     = {{UP-Person}: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval},
  journal   = {arXiv preprint arXiv:2504.10084},
  year      = {2025},
  volume    = {abs/2504.10084},
  url       = {https://api.semanticscholar.org/CorpusID:277781585}
}

@article{houlsby2019,
  author    = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  title     = {Parameter-Efficient Transfer Learning for {NLP}},
  journal   = {arXiv preprint arXiv:1902.00751},
  year      = {2019},
  volume    = {abs/1902.00751},
  url       = {https://api.semanticscholar.org/CorpusID:59599816}
}

@article{gao2023,
  author    = {Kaiyuan Gao and Su He and Zhenyu He and Jiacheng Lin and Qizhi Pei and Jie Shao and Wei Zhang},
  title     = {Examining User-Friendly and Open-Sourced Large {GPT} Models: A Survey on Language, Multimodal, and Scientific {GPT} Models},
  journal   = {arXiv preprint arXiv:2308.14149},
  year      = {2023},
  volume    = {abs/2308.14149},
  url       = {https://api.semanticscholar.org/CorpusID:261243909}
}

@article{li2021prefix,
  author    = {Xiang Lisa Li and Percy Liang},
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  journal   = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year      = {2021},
  pages     = {4582--4597},
  url       = {https://api.semanticscholar.org/CorpusID:230433941}
}

@inproceedings{lester2021,
  author    = {Brian Lester and Rami Al-Rfou and Noah Constant},
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:233296808}
}

@article{he2025rasa,
  author    = {Zhiwei He and Zhaopeng Tu and Xing Wang and Xingyu Chen and Zhijie Wang and Jiahao Xu and Tian Liang and Wenxiang Jiao and Zhuosheng Zhang and Rui Wang},
  title     = {{RaSA}: Rank-Sharing Low-Rank Adaptation},
  journal   = {arXiv preprint arXiv:2503.12576},
  year      = {2025},
  volume    = {abs/2503.12576},
  url       = {https://api.semanticscholar.org/CorpusID:277066136}
}

@article{hu2021lora,
  author    = {J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2106.09685},
  year      = {2021},
  volume    = {abs/2106.09685},
  url       = {https://api.semanticscholar.org/CorpusID:235458009}
}

@article{adegoke2024lora,
  author    = {Adegoke A. Israel and Daniel A. Izenyi and Lwasinam L. Dilli},
  title     = {Efficiently Fine-tuning Large Language Model: {LoRA} Approach},
  journal   = {Zenodo},
  year      = {2024},
  month     = {May},
  doi       = {10.5281/zenodo.11312792},
  url       = {https://www.researchgate.net/publication/380881011}
}

@article{wang2025floe,
  author    = {Xinyi Wang and Lirong Gao and Haobo Wang and Yiming Zhang and Junbo Zhao},
  title     = {{FLoE}: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts},
  journal   = {arXiv preprint arXiv:2506.00495},
  year      = {2025},
  volume    = {abs/2506.00495},
  url       = {https://api.semanticscholar.org/CorpusID:279074867}
}

@article{meng2024pissa,
  author    = {Fanxu Meng and Zhaohui Wang and Muhan Zhang},
  title     = {{PiSSA}: Principal Singular Values and Singular Vectors Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2404.02948},
  year      = {2024},
  volume    = {abs/2404.02948},
  url       = {https://api.semanticscholar.org/CorpusID:268889493}
}

@inproceedings{hasan2025,
  author    = {Navid Bin Hasan and Md. Ashraful Islam and Junaed Younus Khan and Sanjida Senjik and Anindya Iqbal},
  title     = {Automatic High-Level Test Case Generation using Large Language Models},
  booktitle = {2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)},
  pages     = {674--685},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277271660}
}

@article{lu2025,
  author    = {Shao-Chien Lu and Chen-Chen Yeh and Hui-Lin Cho and Yu-Cheng Lin and Rung-Bin Lin},
  title     = {Subitizing-Inspired Large Language Models for Floorplanning},
  journal   = {arXiv preprint arXiv:2504.12076},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277824583}
}

@article{pingua2025,
  author    = {Bhagyajit Pingua and Adyakanta Sahoo and Meenakshi Kandpal and Deepak Murmu and Jyotirmayee Rautaray and Rabindra Kumar Barik and Manob Saikia},
  title     = {Medical {LLMs}: Fine-Tuning vs. Retrieval-Augmented Generation},
  journal   = {Bioengineering},
  volume    = {12},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:279689800}
}

@article{zhong2025,
  author    = {Yi Zhong and Hongchao Liu and Di Zhao},
  title     = {{AutoAssert} 1: A {LoRA} Fine-Tuned {LLM} Model for Efficient Automated Assertion Generation},
  journal   = {arXiv preprint arXiv:2508.07371},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280566161}
}

@inproceedings{lea2025,
  author    = {Darrin Lea and James Ghawaly and Golden G. Richard and Aisha I. Ali-Gombe and Andrew Case},
  title     = {{REx86}: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:282384347}
}

@article{mohammadi2025,
  author    = {Fatemeh Mohammadi and Tommaso Romano and Samira Maghool and Paolo Ceravolo},
  title     = {Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data},
  journal   = {arXiv preprint arXiv:2503.24062},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277468293}
}

@inproceedings{tahir2024,
  author    = {Talha Tahir},
  title     = {Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression: A Feasibility Study},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:274436753}
}

@article{phan2025,
  author    = {Hoang Hai Phan and Nguyen Duc Minh Vu and Nam Dang Phuong},
  title     = {{VNJPTranslate}: A comprehensive pipeline for Vietnamese-Japanese translation},
  journal   = {arXiv preprint arXiv:2504.00339},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277467802}
}

@inproceedings{bandara2025,
  author    = {Eranga Bandara and Ross Gore and Atmaram Yarlagadda and Anita H Clayton and Preston Samuel and Christopher Rhea and Sachin Shetty},
  title     = {Standardization of Psychiatric Diagnoses -- Role of Fine-tuned {LLM} Consortium and {OpenAI}-gpt-oss Reasoning {LLM} Enabled Decision Support System},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:282574013}
}

@article{upadhyay2025,
  author    = {Ojasw Upadhyay and Abishek Saravanakumar and Ayman Ismail},
  title     = {{SynLexLM}: Scaling Legal {LLMs} with Synthetic Data and Curriculum Learning},
  journal   = {arXiv preprint arXiv:2504.18762},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278165289}
}

@article{rao2025,
  author    = {Balaji Rao and William Eiers and Carlo Lipizzi},
  title     = {Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification},
  journal   = {arXiv preprint arXiv:2504.17017},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278032938}
}

@article{pourcel2025,
  author    = {Julien Pourcel and C{\'e}dric Colas and Pierre-Yves Oudeyer},
  title     = {Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on {ARC-AGI}},
  journal   = {arXiv preprint arXiv:2507.14172},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280270875}
}

@article{jimenez2025,
  author    = {Javier Jim{\'e}nez-Rom{\'a}n and Florina Almenares-Mendoza and Alfonso S{\'a}nchez-Maci{\'a}n},
  title     = {Design and Development of an Intelligent {LLM}-based {LDAP} Honeypot},
  journal   = {arXiv preprint arXiv:2509.16682},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:281421124}
}

@article{huang2025,
  author    = {Ziyao Huang and Weiwei Wu and Kui Wu and Jianping Wang and Wei-Bin Lee},
  title     = {{CALM}: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design},
  journal   = {arXiv preprint arXiv:2505.12285},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278740272}
}

@article{machlovi2025,
  author    = {Naseem Machlovi and Maryam Saleki and Innocent Boakye Ababio and Ruhul Amin},
  title     = {Towards Safer {AI} Moderation: Evaluating {LLM} Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach},
  journal   = {arXiv preprint arXiv:2508.07063},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280565722}
}

@article{mansha2025,
  author    = {Imran Mansha},
  title     = {Resource-Efficient Fine-Tuning of {LLaMA}-3.2-3B for Medical Chain-of-Thought Reasoning},
  journal   = {arXiv preprint arXiv:2510.05003},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:281843723}
}

% ============================================================================
% PROMPT ENGINEERING
% ============================================================================

@book{bonstra2024prompt,
  author    = {Lee Bonstra},
  title     = {Prompt Engineering: The Art and Science of Effective Communication with {AI}},
  year      = {2024},
  publisher = {Manning Publications},
  note      = {ISBN: b29ca55ebb259afbb960585f3427fdba}
}

@book{berryman2024prompt,
  author    = {John Berryman and Albert Ziegler},
  title     = {Prompt Engineering for {LLMs}: The Art and Science of Building Large Language Models},
  year      = {2024},
  publisher = {O'Reilly Media, Incorporated},
  isbn      = {9781098156152}
}