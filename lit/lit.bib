% ============================================================================
% GRUNDLAGEN: TRANSFORMER & NLP
% ============================================================================

@article{vaswani2017attention,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017},
  publisher = {Curran Associates, Inc.}
}

@article{devlin2018bert,
  author    = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {arXiv preprint arXiv:1810.04805},
  year      = {2018}
}

@article{radford2018improving,
  author    = {Alec Radford and Karthik Narasimhan and Tim Salimans and Ilya Sutskever},
  title     = {Improving Language Understanding by Generative Pre-Training},
  year      = {2018},
  publisher = {OpenAI}
}

@article{radford2019language,
  author    = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  title     = {Language Models are Unsupervised Multitask Learners},
  year      = {2019},
  publisher = {OpenAI}
}

@article{raffel2020exploring,
  author    = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal   = {Journal of Machine Learning Research},
  volume    = {21},
  number    = {140},
  pages     = {1--67},
  year      = {2020}
}

% ============================================================================
% TRANSFER LEARNING
% ============================================================================

@article{pan2010survey,
  author    = {Sinno Jialin Pan and Qiang Yang},
  title     = {A Survey on Transfer Learning},
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  volume    = {22},
  number    = {10},
  pages     = {1345--1359},
  year      = {2010},
  publisher = {IEEE}
}

@inproceedings{howard2018universal,
  author    = {Jeremy Howard and Sebastian Ruder},
  title     = {Universal Language Model Fine-tuning for Text Classification},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages     = {328--339},
  year      = {2018},
  publisher = {Association for Computational Linguistics}
}

% ============================================================================
% MISTRAL ARCHITECTURE
% ============================================================================

@article{jiang2023mistral,
  author    = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L{\'e}lio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  title     = {Mistral 7B},
  journal   = {arXiv preprint arXiv:2310.06825},
  year      = {2023}
}

% ============================================================================
% LEOLM / DEUTSCHSPRACHIGE MODELLE
% ============================================================================

@misc{leolm2023,
  author       = {{HessianAI}},
  title        = {{LeoLM}: Linguistically Enhanced Open Language Model for German},
  year         = {2023},
  howpublished = {\url{https://huggingface.co/LeoLM}},
  note         = {Accessed: 2025-11-12}
}

% ============================================================================
% INSTRUCTION TUNING
% ============================================================================

@article{wei2021finetuned,
  author    = {Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  title     = {Finetuned Language Models Are Zero-Shot Learners},
  journal   = {arXiv preprint arXiv:2109.01652},
  year      = {2021}
}

@article{ouyang2022training,
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  journal   = {arXiv preprint arXiv:2203.02155},
  year      = {2022}
}