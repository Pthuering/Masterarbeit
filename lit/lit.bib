% ============================================================================
% GRUNDLAGEN: TRANSFORMER & NLP
% ============================================================================

@article{vaswani2017attention,
  author    = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {30},
  year      = {2017},
  publisher = {Curran Associates, Inc.}

}

% ============================================================================

@article{smith2023,
  author    = {James Smith and Yen-Chang Hsu and Lingyu Zhang and Ting Hua and Zsolt Kira and Yilin Shen and Hongxia Jin},
  title     = {Continual Diffusion: Continual Customization of Text-to-Image Diffusion with {C-LoRA}},
  journal   = {arXiv preprint arXiv:2304.06027},
  year      = {2023},
  volume    = {abs/2304.06027},
  url       = {https://api.semanticscholar.org/CorpusID:258078844}
}

@article{hua2023,
  author    = {Wenhui Hua and Brian Williams and Davood Shamsi},
  title     = {{LACoS-BLOOM}: Low-rank Adaptation with Contrastive objective on 8 bits {Siamese-BLOOM}},
  journal   = {arXiv preprint arXiv:2305.06404},
  year      = {2023},
  volume    = {abs/2305.06404},
  url       = {https://api.semanticscholar.org/CorpusID:258615538}
}

@article{liu2025up,
  author    = {Yating Liu and Yaowei Li and Xiangyuan Lan and Wenming Yang and Zimo Liu and Qingmin Liao},
  title     = {{UP-Person}: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval},
  journal   = {arXiv preprint arXiv:2504.10084},
  year      = {2025},
  volume    = {abs/2504.10084},
  url       = {https://api.semanticscholar.org/CorpusID:277781585}
}

@article{houlsby2019,
  author    = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
  title     = {Parameter-Efficient Transfer Learning for {NLP}},
  journal   = {arXiv preprint arXiv:1902.00751},
  year      = {2019},
  volume    = {abs/1902.00751},
  url       = {https://api.semanticscholar.org/CorpusID:59599816}
}

@article{gao2023examining,
  author    = {Kaiyuan Gao and Su He and Zhenyu He and Jiacheng Lin and Qizhi Pei and Jie Shao and Wei Zhang},
  title     = {Examining User-Friendly and Open-Sourced Large {GPT} Models: A Survey on Language, Multimodal, and Scientific {GPT} Models},
  journal   = {arXiv preprint arXiv:2308.14149},
  year      = {2023},
  volume    = {abs/2308.14149},
  url       = {https://api.semanticscholar.org/CorpusID:261243909}
}

@article{li2021prefix,
  author    = {Xiang Lisa Li and Percy Liang},
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  journal   = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year      = {2021},
  pages     = {4582--4597},
  url       = {https://api.semanticscholar.org/CorpusID:230433941}
}

@inproceedings{lester2021,
  author    = {Brian Lester and Rami Al-Rfou and Noah Constant},
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  booktitle = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:233296808}
}

@article{he2025rasa,
  author    = {Zhiwei He and Zhaopeng Tu and Xing Wang and Xingyu Chen and Zhijie Wang and Jiahao Xu and Tian Liang and Wenxiang Jiao and Zhuosheng Zhang and Rui Wang},
  title     = {{RaSA}: Rank-Sharing Low-Rank Adaptation},
  journal   = {arXiv preprint arXiv:2503.12576},
  year      = {2025},
  volume    = {abs/2503.12576},
  url       = {https://api.semanticscholar.org/CorpusID:277066136}
}

@article{hu2021lora,
  author    = {J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  title     = {{LoRA}: Low-Rank Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2106.09685},
  year      = {2021},
  volume    = {abs/2106.09685},
  url       = {https://api.semanticscholar.org/CorpusID:235458009}
}

@article{adegoke2024lora,
  author    = {Adegoke A. Israel and Daniel A. Izenyi and Lwasinam L. Dilli},
  title     = {Efficiently Fine-tuning Large Language Model: {LoRA} Approach},
  journal   = {Zenodo},
  year      = {2024},
  doi       = {10.5281/zenodo.11312792},
  url       = {https://www.researchgate.net/publication/380881011}
}

@article{wang2025floe,
  author    = {Xinyi Wang and Lirong Gao and Haobo Wang and Yiming Zhang and Junbo Zhao},
  title     = {{FLoE}: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts},
  journal   = {arXiv preprint arXiv:2506.00495},
  year      = {2025},
  volume    = {abs/2506.00495},
  url       = {https://api.semanticscholar.org/CorpusID:279074867}
}

@article{meng2024pissa,
  author    = {Fanxu Meng and Zhaohui Wang and Muhan Zhang},
  title     = {{PiSSA}: Principal Singular Values and Singular Vectors Adaptation of Large Language Models},
  journal   = {arXiv preprint arXiv:2404.02948},
  year      = {2024},
  volume    = {abs/2404.02948},
  url       = {https://api.semanticscholar.org/CorpusID:268889493}
}

@inproceedings{hasan2025,
  author    = {Navid Bin Hasan and Md. Ashraful Islam and Junaed Younus Khan and Sanjida Senjik and Anindya Iqbal},
  title     = {Automatic High-Level Test Case Generation using Large Language Models},
  booktitle = {2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)},
  pages     = {674--685},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277271660}
}

@article{lu2025,
  author    = {Shao-Chien Lu and Chen-Chen Yeh and Hui-Lin Cho and Yu-Cheng Lin and Rung-Bin Lin},
  title     = {Subitizing-Inspired Large Language Models for Floorplanning},
  journal   = {arXiv preprint arXiv:2504.12076},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277824583}
}

@article{pingua2025,
  author    = {Bhagyajit Pingua and Adyakanta Sahoo and Meenakshi Kandpal and Deepak Murmu and Jyotirmayee Rautaray and Rabindra Kumar Barik and Manob Saikia},
  title     = {Medical {LLMs}: Fine-Tuning vs. Retrieval-Augmented Generation},
  journal   = {Bioengineering},
  volume    = {12},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:279689800}
}

@article{zhong2025,
  author    = {Yi Zhong and Hongchao Liu and Di Zhao},
  title     = {{AutoAssert} 1: A {LoRA} Fine-Tuned {LLM} Model for Efficient Automated Assertion Generation},
  journal   = {arXiv preprint arXiv:2508.07371},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280566161}
}

@inproceedings{lea2025,
  author    = {Darrin Lea and James Ghawaly and Golden G. Richard and Aisha I. Ali-Gombe and Andrew Case},
  title     = {{REx86}: A Local Large Language Model for Assisting in x86 Assembly Reverse Engineering},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:282384347}
}

@article{mohammadi2025,
  author    = {Fatemeh Mohammadi and Tommaso Romano and Samira Maghool and Paolo Ceravolo},
  title     = {Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data},
  journal   = {arXiv preprint arXiv:2503.24062},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277468293}
}

@inproceedings{tahir2024,
  author    = {Talha Tahir},
  title     = {Fine-Tuning Open-Weight Language Models to Deliver Cognitive Behavioral Therapy for Depression: A Feasibility Study},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:274436753}
}

@article{phan2025,
  author    = {Hoang Hai Phan and Nguyen Duc Minh Vu and Nam Dang Phuong},
  title     = {{VNJPTranslate}: A comprehensive pipeline for Vietnamese-Japanese translation},
  journal   = {arXiv preprint arXiv:2504.00339},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:277467802}
}

@inproceedings{bandara2025,
  author    = {Eranga Bandara and Ross Gore and Atmaram Yarlagadda and Anita H Clayton and Preston Samuel and Christopher Rhea and Sachin Shetty},
  title     = {Standardization of Psychiatric Diagnoses -- Role of Fine-tuned {LLM} Consortium and {OpenAI}-gpt-oss Reasoning {LLM} Enabled Decision Support System},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:282574013}
}

@article{upadhyay2025,
  author    = {Ojasw Upadhyay and Abishek Saravanakumar and Ayman Ismail},
  title     = {{SynLexLM}: Scaling Legal {LLMs} with Synthetic Data and Curriculum Learning},
  journal   = {arXiv preprint arXiv:2504.18762},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278165289}
}

@article{rao2025,
  author    = {Balaji Rao and William Eiers and Carlo Lipizzi},
  title     = {Neural Theorem Proving: Generating and Structuring Proofs for Formal Verification},
  journal   = {arXiv preprint arXiv:2504.17017},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278032938}
}

@article{pourcel2025,
  author    = {Julien Pourcel and C{\'e}dric Colas and Pierre-Yves Oudeyer},
  title     = {Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on {ARC-AGI}},
  journal   = {arXiv preprint arXiv:2507.14172},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280270875}
}

@article{jimenez2025,
  author    = {Javier Jim{\'e}nez-Rom{\'a}n and Florina Almenares-Mendoza and Alfonso S{\'a}nchez-Maci{\'a}n},
  title     = {Design and Development of an Intelligent {LLM}-based {LDAP} Honeypot},
  journal   = {arXiv preprint arXiv:2509.16682},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:281421124}
}

@article{huang2025calm,
  author    = {Ziyao Huang and Weiwei Wu and Kui Wu and Jianping Wang and Wei-Bin Lee},
  title     = {{CALM}: Co-evolution of Algorithms and Language Model for Automatic Heuristic Design},
  journal   = {arXiv preprint arXiv:2505.12285},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278740272}
}

@article{machlovi2025,
  author    = {Naseem Machlovi and Maryam Saleki and Innocent Boakye Ababio and Ruhul Amin},
  title     = {Towards Safer {AI} Moderation: Evaluating {LLM} Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach},
  journal   = {arXiv preprint arXiv:2508.07063},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:280565722}
}

@article{mansha2025,
  author    = {Imran Mansha},
  title     = {Resource-Efficient Fine-Tuning of {LLaMA}-3.2-3B for Medical Chain-of-Thought Reasoning},
  journal   = {arXiv preprint arXiv:2510.05003},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:281843723}
}

% ============================================================================
% PROMPT ENGINEERING
% ============================================================================

@book{bonstra2024prompt,
  author    = {Lee Boonstra},
  title     = {Prompt Engineering},
  year      = {2024}
}

@book{berryman2024prompt,
  author    = {John Berryman and Albert Ziegler},
  title     = {Prompt Engineering for {LLMs}: The Art and Science of Building Large Language Models},
  year      = {2024},
  publisher = {O'Reilly Media, Incorporated},
  isbn      = {9781098156152}
}

@article{reynolds2021prompt,
  title     = {Prompt programming for large language models: Beyond the few-shot paradigm},
  author    = {Reynolds, Laria and McDonell, Kyle},
  journal   = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages     = {1--7},
  year      = {2021},
  note      = {Verfügbar unter: https://arxiv.org/abs/2102.07350}
}

% ============================================================================
% GENERAL LLM BOOKS & GUIDES
% ============================================================================

@book{codeEtTu2024,
  author    = {{Code, Et Tu}},
  title     = {{LLM}, Transformer, {RAG AI}: Mastering Large Language Models},
  year      = {2024}
}

@book{esposito2024,
  author    = {Francesco Esposito},
  title     = {Programming Large Language Models With Azure {OpenAI}},
  year      = {2024},
  publisher = {Microsoft Press}
}

@book{godoy2025,
  author    = {Daniel Voigt Godoy},
  title     = {A Hands-On Guide to Fine-Tuning Large Language Models},
  year      = {2025},
  publisher = {Leanpub}
}

% ============================================================================
% LLM APPLICATIONS & REVIEWS
% ============================================================================

@article{kaur2024,
  author    = {Priyanka Kaur and Gurinder Singh Kashyap and Anil Kumar and Md. Tabrez Nafis and Sukhpal Kumar and Vivek Shokeen},
  title     = {From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility},
  year      = {2024}
}

% ============================================================================
% TRANSPORTATION & TRAFFIC SAFETY
% ============================================================================

@article{karim2025,
  author    = {Md Mahmudul Karim and Yining Shi and Shengyin Zhang and Bo Wang and Mojtaba Nasri and Yuanchang Wang},
  title     = {Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review},
  year      = {2025}
}

@techreport{jonnala2024,
  author      = {Ravi Jonnala and Gina Liang and Jun Yang and Izzat Alsmadi},
  title       = {Using Large Language Models in Public Transit Systems: San Antonio as a Case Study},
  institution = {Texas A\&M University},
  year        = {2024}
}

@article{khalil2024,
  author    = {Rana Azhar Khalil and Zainab Safelnasr and Nahom Yemane and Mohammed Kedir and Ahmed Shafiqurrahman and Noman Saeed},
  title     = {Advanced Learning Technologies for Intelligent Transportation Systems: Prospects and Challenges},
  journal   = {IEEE Open Journal of Vehicular Technology},
  year      = {2024},
  doi       = {10.1109/OJVT.2024.3369691}
}

@article{zhang2020applications,
  title={Applications of deep learning in intelligent transportation systems},
  author={Zhang, Junbo and Zheng, Yu and Qi, Dekang and Li, Ruiyuan and Yi, Xiuwen},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={21},
  number={4},
  pages={1373--1384},
  year={2020},
  publisher={IEEE},
  note={Verfügbar unter: https://ieeexplore.ieee.org/document/8758916}
}

@inproceedings{silva2021natural,
  title={Natural language processing for intelligent transportation systems: A systematic literature review},
  author={Silva, Nuno and Sousa, P{\'a}vel and Gon{\c{c}}alves, Jo{\~a}o},
  booktitle={2021 16th Iberian Conference on Information Systems and Technologies (CISTI)},
  pages={1--6},
  year={2021},
  organization={IEEE},
  note={Verfügbar unter: https://ieeexplore.ieee.org/document/9476472}
}

% ============================================================================
% RESOURCE EFFICIENCY & MODEL OPTIMIZATION
% ============================================================================

@article{behera2025,
  author    = {Adarsh Prasad Behera and Jaya Prakash Varma Champati and Roberto Morabito and Sasu Tarkoma and James Richard Gross},
  title     = {Towards Efficient Multi-{LLM} Inference: Characterization and Analysis of {LLM} Routing and Hierarchical Techniques},
  journal   = {arXiv preprint arXiv:2506.06579},
  year      = {2025},
  volume    = {abs/2506.06579},
  url       = {https://api.semanticscholar.org/CorpusID:279250933}
}

@article{chen2025survey,
  author    = {Yi Chen and JiaHao Zhao and HaoHao Han},
  title     = {A Survey on Collaborative Mechanisms Between Large and Small Language Models},
  journal   = {arXiv preprint arXiv:2505.07460},
  year      = {2025},
  volume    = {abs/2505.07460},
  url       = {https://api.semanticscholar.org/CorpusID:278502445}
}

@inproceedings{chen2024adaptive,
  author    = {Jiajing Chen and Bingying Liu and Xiaoxuan Liao and Jia Gao and Hongye Zheng and Yue Li},
  title     = {Adaptive Optimization for Enhanced Efficiency in Large-Scale Language Model Training},
  booktitle = {2024 6th International Conference on Frontier Technologies of Information and Computer (ICFTIC)},
  year      = {2024},
  pages     = {1315--1319},
  url       = {https://api.semanticscholar.org/CorpusID:274581360}
}

@article{gu2024,
  author    = {Yang Gu and Hengyu You and Jian Cao and Muran Yu and Haoran Fan and Shiyou Qian},
  title     = {Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey},
  journal   = {ACM Transactions on Software Engineering and Methodology},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:274130835}
}

@article{huang2024when,
  author    = {Sen Huang and Kaixiang Yang and Sheng Qi and Rui Wang},
  title     = {When Large Language Model Meets Optimization},
  journal   = {arXiv preprint arXiv:2405.10098},
  year      = {2024},
  volume    = {abs/2405.10098},
  url       = {https://api.semanticscholar.org/CorpusID:269791108}
}

@article{kong2025,
  author    = {Jie Kong and Junxiang Zhang and Jiheng Xu and Yalong Li and Shouhua Zhang and Jiehan Zhou and Yuhai Liu and Peng Liang and Quan Zhang and Luohan Jiang},
  title     = {Opt-{GPTQ}: An Optimized {GPTQ} Combining Sparse Attention and Quantization Techniques},
  journal   = {arXiv preprint arXiv:2505.02351},
  year      = {2025},
  volume    = {abs/2505.02351},
  url       = {https://api.semanticscholar.org/CorpusID:278327010}
}

@article{lee2025,
  author    = {Yu-Ang Lee and Guan-Ting Yi and Mei-Yi Liu and Jui-Chao Lu and Guan-Bo Yang and Yun-Nung Chen},
  title     = {Compound {AI} Systems Optimization: A Survey of Methods, Challenges, and Future Directions},
  journal   = {arXiv preprint arXiv:2506.08234},
  year      = {2025},
  volume    = {abs/2506.08234},
  url       = {https://api.semanticscholar.org/CorpusID:279260634}
}

@article{liang2025,
  author    = {Mingyu Liang and Hiwot Tadese Kassa and Wenyin Fu and Brian Coutinho and Louis Feng and Christina Delimitrou},
  title     = {Lumos: Efficient Performance Modeling and Estimation for Large-scale {LLM} Training},
  journal   = {arXiv preprint arXiv:2504.09307},
  year      = {2025},
  volume    = {abs/2504.09307},
  url       = {https://api.semanticscholar.org/CorpusID:277781663}
}

@article{liu2024evolutionary,
  author    = {Wanyi Liu and Long Chen and Zhenzhou Tang},
  title     = {Large Language Model Aided Multi-objective Evolutionary Algorithm: a Low-cost Adaptive Approach},
  journal   = {arXiv preprint arXiv:2410.02301},
  year      = {2024},
  volume    = {abs/2410.02301},
  url       = {https://api.semanticscholar.org/CorpusID:273098268}
}

@article{ma2025,
  author    = {Ziyang Ma and Zuchao Li and Lefei Zhang and Gui-Song Xia and Bo Du and Liangpei Zhang and Dacheng Tao},
  title     = {Model Hemorrhage and the Robustness Limits of Large Language Models},
  journal   = {arXiv preprint arXiv:2503.23924},
  year      = {2025},
  volume    = {abs/2503.23924},
  url       = {https://api.semanticscholar.org/CorpusID:277452419}
}

@article{nie2025,
  author    = {Lunyiu Nie and Zhimin Ding and Kevin Yu and Marco Cheung and Christopher Jermaine and Swarat Chaudhuri},
  title     = {Resource-efficient Inference with Foundation Model Programs},
  journal   = {arXiv preprint arXiv:2504.07247},
  year      = {2025},
  volume    = {abs/2504.07247},
  url       = {https://api.semanticscholar.org/CorpusID:277667091}
}

@inproceedings{ouyang2024,
  author    = {Bei Ouyang and Shengyuan Ye and Liekang Zeng and Tianyi Qian and Jingyi Li and Xu Chen},
  title     = {Pluto and Charon: A Time and Memory Efficient Collaborative Edge {AI} Framework for Personal {LLMs} Fine-tuning},
  booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:271815352}
}

@article{piccialli2024,
  author    = {Francesco Piccialli and Diletta Chiaro and Pian Qi and Valerio Bellandi and Ernesto Damiani},
  title     = {Federated and edge learning for large language models},
  journal   = {Information Fusion},
  year      = {2024},
  volume    = {117},
  pages     = {102840},
  url       = {https://api.semanticscholar.org/CorpusID:274815509}
}

@article{qian2024,
  author    = {Shengsheng Qian and Zuyi Zhou and Dizhan Xue and Bing Wang and Changsheng Xu},
  title     = {From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models},
  journal   = {arXiv preprint arXiv:2409.18996},
  year      = {2024},
  volume    = {abs/2409.18996},
  url       = {https://api.semanticscholar.org/CorpusID:272987668}
}

@article{rostam2024,
  author    = {Zhyar Rzgar K. Rostam and S{\'a}ndor Sz{\'e}n{\'a}si and G{\'a}bor Kert{\'e}sz},
  title     = {Achieving Peak Performance for Large Language Models: A Systematic Review},
  journal   = {IEEE Access},
  year      = {2024},
  volume    = {12},
  pages     = {96017--96050},
  url       = {https://api.semanticscholar.org/CorpusID:271083368}
}

@article{shan2025,
  author    = {Jiaxin Shan and Varun Gupta and Le Xu and Haiyang Shi and Jingyuan Zhang and Ning Wang and Linhui Xu and Rong Kang and Tongping Liu and Yifei Zhang and Yiqing Zhu and Shuowei Jin and Gangmuk Lim and Binbin Chen and Zuzhi Chen and Xiao Liu and Xin Chen and Kante Yin and Chak-Pong Chung and Chenyu Jiang and Yichen Lu and Jianjun Chen and Caixue Lin and Wu Xiang and Rui Shi and Liguang Xie},
  title     = {{AIBrix}: Towards Scalable, Cost-Effective Large Language Model Inference Infrastructure},
  journal   = {arXiv preprint arXiv:2504.03648},
  year      = {2025},
  volume    = {abs/2504.03648},
  url       = {https://api.semanticscholar.org/CorpusID:277621320}
}

@article{shen2024,
  author    = {Xuan Shen and Pu Zhao and Yifan Gong and Zhenglun Kong and Zheng Zhan and Yushu Wu and Ming Lin and Chao Wu and Xue Lin and Yanzhi Wang},
  title     = {Search for Efficient Large Language Models},
  journal   = {arXiv preprint arXiv:2409.17372},
  year      = {2024},
  volume    = {abs/2409.17372},
  url       = {https://api.semanticscholar.org/CorpusID:272911378}
}

@inproceedings{sundaravadivel2024,
  author    = {Prabha Sundaravadivel and Preetha Roselyn and Vedachalam Narayanaswamy and Vincent I. Jeyaraj and Aishree Ramesh and Aaditya Khanal},
  title     = {Integrating image-based {LLMs} on edge-devices for underwater robotics},
  booktitle = {Defense + Commercial Sensing},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:270352081}
}

@article{team2025,
  author    = {{Ling Team}},
  title     = {Every {FLOP} Counts: Scaling a 300B Mixture-of-Experts {LING LLM} without Premium {GPUs}},
  journal   = {arXiv preprint arXiv:2503.05139},
  year      = {2025},
  volume    = {abs/2503.05139},
  url       = {https://api.semanticscholar.org/CorpusID:276885331}
}

@article{wang2025efficient,
  author    = {Ningning Wang and Xavier Hu and Pai Liu and He Zhu and Yue Hou and Heyuan Huang and Shengyu Zhang and Jian Yang and Jiaheng Liu and Ge Zhang and Changwang Zhang and Jun Wang and Yuchen Eleanor Jiang and Wangchunshu Zhou},
  title     = {Efficient Agents: Building Effective Agents While Reducing Cost},
  journal   = {arXiv preprint arXiv:2508.02694},
  year      = {2025},
  volume    = {abs/2508.02694},
  url       = {https://api.semanticscholar.org/CorpusID:280526822}
}

@article{yang2025,
  author    = {Hantao Yang and Hong Xie and Defu Lian and Enhong Chen},
  title     = {{LLM} Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective {LLM} Inference},
  journal   = {arXiv preprint arXiv:2509.15515},
  year      = {2025},
  volume    = {abs/2509.15515},
  url       = {https://api.semanticscholar.org/CorpusID:281410834}
}

@inproceedings{zhang2024edge,
  author    = {Xinyuan Zhang and Jiang Liu and Zehui Xiong and Yudong Huang and Gaochang Xie and Ran Zhang},
  title     = {Edge Intelligence Optimization for Large Language Model Inference with Batching and Quantization},
  booktitle = {2024 IEEE Wireless Communications and Networking Conference (WCNC)},
  year      = {2024},
  pages     = {1--6},
  url       = {https://api.semanticscholar.org/CorpusID:269758014}
}

@article{zhang2024zeroth,
  author    = {Yihua Zhang and Pingzhi Li and Junyuan Hong and Jiaxiang Li and Yimeng Zhang and Wenqing Zheng and Pin-Yu Chen and Jason D. Lee and Wotao Yin and Mingyi Hong and Zhangyang Wang and Sijia Liu and Tianlong Chen},
  title     = {Revisiting Zeroth-Order Optimization for Memory-Efficient {LLM} Fine-Tuning: A Benchmark},
  journal   = {arXiv preprint arXiv:2402.11592},
  year      = {2024},
  volume    = {abs/2402.11592},
  url       = {https://api.semanticscholar.org/CorpusID:267750841}
}

@article{hooker2020,
  author    = {Sara Hooker},
  title     = {The hardware lottery},
  journal   = {Communications of the ACM},
  year      = {2020},
  volume    = {64},
  pages     = {58--65},
  url       = {https://api.semanticscholar.org/CorpusID:221655745}
}

@article{reddy2025,
  author    = {Sandeep Reddy and Kabir Khan and Rohit Patil and Ananya Chakraborty and Faizan A. Khan and Swati Kulkarni and Arjun Verma and Neha Singh},
  title     = {Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints},
  journal   = {arXiv preprint arXiv:2508.10426},
  year      = {2025},
  volume    = {abs/2508.10426},
  url       = {https://api.semanticscholar.org/CorpusID:280649743}
}

@article{yu2025,
  author    = {Erlan Yu and Xuehong Chu and Wanwan Zhang and Xiangbin Meng and Yaodong Yang and Xunming Ji and Chuanjie Wu},
  title     = {Large Language Models in Medicine: Applications, Challenges, and Future Directions},
  journal   = {International Journal of Medical Sciences},
  year      = {2025},
  volume    = {22},
  pages     = {2792--2801},
  url       = {https://api.semanticscholar.org/CorpusID:279052935}
}

% ============================================================================
% ENERGY & ENVIRONMENTAL IMPACT
% ============================================================================

@article{barbierato2025,
  author    = {Enrico Barbierato and Alice Gatti and Alessandro Incremona and Andrea Pozzi and Daniele Toti},
  title     = {Breaking Away From {AI}: The Ontological and Ethical Evolution of Machine Learning},
  journal   = {IEEE Access},
  year      = {2025},
  volume    = {13},
  pages     = {55627--55647},
  url       = {https://api.semanticscholar.org/CorpusID:277164738}
}

@article{barbierato2024,
  author    = {Enrico Barbierato and Alice Gatti},
  title     = {Toward Green {AI}: A Methodological Survey of the Scientific Literature},
  journal   = {IEEE Access},
  year      = {2024},
  volume    = {12},
  pages     = {23989--24013},
  url       = {https://api.semanticscholar.org/CorpusID:267472176}
}

@inproceedings{bender2021,
  author    = {Emily M. Bender and Timnit Gebru and Angelina McMillan-Major and Shmargaret Shmitchell},
  title     = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  year      = {2021},
  url       = {https://api.semanticscholar.org/CorpusID:262580630}
}

@inproceedings{fernandez2025,
  author    = {Jared Fernandez and Clara Na and Vashisth Tiwari and Yonatan Bisk and Sasha Luccioni and Emma Strubell},
  title     = {Energy Considerations of Large Language Model Inference and Efficiency Optimizations},
  booktitle = {Annual Meeting of the Association for Computational Linguistics},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278033677}
}

@article{gholami2023,
  author    = {Sia Gholami and Marwan Omar},
  title     = {Do Generative Large Language Models need billions of parameters?},
  journal   = {arXiv preprint arXiv:2309.06589},
  year      = {2023},
  volume    = {abs/2309.06589},
  url       = {https://api.semanticscholar.org/CorpusID:261705636}
}

@inproceedings{khan2025,
  author    = {Tahniat Khan and Soroor Motie and Sedef Akinli Ko{\c{c}}ak and Shaina Raza},
  title     = {Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights},
  booktitle = {2025 IEEE Conference on Artificial Intelligence (CAI)},
  year      = {2025},
  pages     = {370--375},
  url       = {https://api.semanticscholar.org/CorpusID:277633825}
}

@inproceedings{luccioni2023,
  author    = {Sasha Luccioni and Yacine Jernite and Emma Strubell},
  title     = {Power Hungry Processing: Watts Driving the Cost of {AI} Deployment?},
  booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
  year      = {2023},
  url       = {https://api.semanticscholar.org/CorpusID:265466268}
}

@article{morrison2025,
  author    = {Jacob Daniel Morrison and Clara Na and Jared Fernandez and Tim Dettmers and Emma Strubell and Jesse Dodge},
  title     = {Holistically Evaluating the Environmental Impact of Creating Language Models},
  journal   = {arXiv preprint arXiv:2503.05804},
  year      = {2025},
  volume    = {abs/2503.05804},
  url       = {https://api.semanticscholar.org/CorpusID:276902612}
}

@article{qiao2025,
  author    = {Yu Qiao and Phuong-Nam Tran and Jisu Yoon and Loc X. Nguyen and Choong-Seon Hong},
  title     = {{DeepSeek}-Inspired Exploration of {RL}-based {LLMs} and Synergy with Wireless Networks: A Survey},
  journal   = {arXiv preprint arXiv:2503.09956},
  year      = {2025},
  volume    = {abs/2503.09956},
  url       = {https://api.semanticscholar.org/CorpusID:276961675}
}

@article{rajput2025,
  author    = {Saurabhsingh Rajput and Mootez Saad and Tushar Sharma},
  title     = {Tu(r)ning {AI} Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations},
  journal   = {arXiv preprint arXiv:2506.18289},
  year      = {2025},
  volume    = {abs/2506.18289},
  url       = {https://api.semanticscholar.org/CorpusID:280000384}
}

@article{ren2024,
  author    = {Mengchao Ren},
  title     = {Advancements and Applications of Large Language Models in Natural Language Processing: A Comprehensive Review},
  journal   = {Applied and Computational Engineering},
  year      = {2024},
  url       = {https://api.semanticscholar.org/CorpusID:274326044}
}

@inproceedings{strubell2020,
  author    = {Emma Strubell and Ananya Ganesh and Andrew McCallum},
  title     = {Energy and Policy Considerations for Modern Deep Learning Research},
  booktitle = {AAAI Conference on Artificial Intelligence},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:219182397}
}

@article{strubell2019energy,
  title     = {Energy and policy considerations for deep learning in NLP},
  author    = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal   = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages     = {3645--3650},
  year      = {2019},
  note      = {Verfügbar unter: https://aclanthology.org/P19-1355/}
}

@article{schwartz2020green,
  title     = {Green AI},
  author    = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal   = {Communications of the ACM},
  volume    = {63},
  number    = {12},
  pages     = {54--63},
  year      = {2020},
  publisher = {ACM New York, NY, USA},
  note      = {Verfügbar unter: https://dl.acm.org/doi/10.1145/3381831}
}

@inproceedings{wallace2025,
  author    = {Tom Wallace and Beatrice Ombuki-Berman and Naser Ezzati-Jivan},
  title     = {Optimization Strategies for Enhancing Resource Efficiency in Transformers \& Large Language Models},
  booktitle = {Proceedings of the 16th ACM/SPEC International Conference on Performance Engineering},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:276094830}
}

@article{wu2025,
  author    = {Hui Wu and Xiaoyang Wang and Zhong Yuan Fan},
  title     = {Addressing the sustainable {AI} trilemma: a case study on {LLM} agents and {RAG}},
  journal   = {arXiv preprint arXiv:2501.08262},
  year      = {2025},
  volume    = {abs/2501.08262},
  url       = {https://api.semanticscholar.org/CorpusID:275515730}
}

% ============================================================================
% QUANTISIERUNG
% ============================================================================

@article{weng2021,
  title     = {Neural Network Quantization for Efficient Inference: A Survey},
  author    = {Olivia Weng},
  journal   = {arXiv preprint arXiv:2112.06126},
  year      = {2021},
  volume    = {abs/2112.06126},
  url       = {https://api.semanticscholar.org/CorpusID:245124603}
}

@article{hasan2024,
  title     = {Optimizing Large Language Models through Quantization: A Comparative Analysis of {PTQ} and {QAT} Techniques},
  author    = {Jahid Hasan},
  journal   = {arXiv preprint arXiv:2411.06084},
  year      = {2024},
  volume    = {abs/2411.06084},
  url       = {https://api.semanticscholar.org/CorpusID:273962606}
}

@article{chen2024ternary,
  title     = {Efficient Ternary Weight Embedding Model: Bridging Scalability and Performance},
  author    = {Jiayi Chen and Chen Wu and Shaoqun Zhang and Nan Li and Liangjie Zhang and Qi Zhang},
  journal   = {arXiv preprint arXiv:2411.15438},
  year      = {2024},
  volume    = {abs/2411.15438},
  url       = {https://api.semanticscholar.org/CorpusID:274234842}
}

@article{bie2019,
  title     = {A Simplified Fully Quantized Transformer for End-to-end Speech Recognition},
  author    = {Alex Bie and Bharat Venkitesh and Jo{\~a}o Monteiro and Md. Akmal Haidar and Mehdi Rezagholizadeh},
  journal   = {arXiv preprint arXiv:1910.10202},
  year      = {2019},
  url       = {https://api.semanticscholar.org/CorpusID:211171600}
}

@inproceedings{jacob2017,
  title     = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author    = {Benoit Jacob and Skirmantas Kligys and Bo Chen and Menglong Zhu and Matthew Tang and Andrew G. Howard and Hartwig Adam and Dmitry Kalenichenko},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2018},
  pages     = {2704--2713},
  url       = {https://api.semanticscholar.org/CorpusID:39867659}
}

@article{capra2020,
  title     = {Hardware and Software Optimizations for Accelerating Deep Neural Networks: Survey of Current Trends, Challenges, and the Road Ahead},
  author    = {Maurizio Capra and Beatrice Bussolino and Alberto Marchisio and Guido Masera and Maurizio Martina and Muhammad Akmal Shafique},
  journal   = {IEEE Access},
  year      = {2020},
  volume    = {8},
  pages     = {225134--225180},
  url       = {https://api.semanticscholar.org/CorpusID:229339862}
}

@article{gysel2018,
  title     = {Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks},
  author    = {Philipp Gysel and Jon J. Pimentel and Mohammad Motamedi and Soheil Ghiasi},
  journal   = {IEEE Transactions on Neural Networks and Learning Systems},
  year      = {2018},
  volume    = {29},
  pages     = {5784--5789},
  url       = {https://api.semanticscholar.org/CorpusID:51610353}
}

@article{gholami2021,
  title     = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  author    = {Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
  journal   = {arXiv preprint arXiv:2103.13630},
  year      = {2021},
  volume    = {abs/2103.13630},
  url       = {https://api.semanticscholar.org/CorpusID:232352683}
}

@article{paglieri2024,
  title     = {Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern {LLMs}},
  author    = {Davide Paglieri and Saurabh Dash and Tim Rocktaschel and Jack Parker-Holder},
  journal   = {arXiv preprint arXiv:2405.20835},
  year      = {2024},
  volume    = {abs/2405.20835},
  url       = {https://api.semanticscholar.org/CorpusID:270199368}
}

@article{zhang2024mixpe,
  title     = {{MixPE}: Quantization and Hardware Co-design for Efficient {LLM} Inference},
  author    = {Yu Zhang and Mingzi Wang and Lancheng Zou and Wulong Liu and Hui-Ling Zhen and Mingxuan Yuan and Bei Yu},
  journal   = {arXiv preprint arXiv:2411.16158},
  year      = {2024},
  volume    = {abs/2411.16158},
  url       = {https://api.semanticscholar.org/CorpusID:274234214}
}

@article{liu2025quant,
  title     = {A Comprehensive Evaluation on Quantization Techniques for Large Language Models},
  author    = {Yutong Liu and Cairong Zhao and Guosheng Hu},
  journal   = {arXiv preprint arXiv:2507.17417},
  year      = {2025},
  volume    = {abs/2507.17417},
  url       = {https://api.semanticscholar.org/CorpusID:280228040}
}

@article{yao2022zeroquant,
  title     = {{ZeroQuant}: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author    = {Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  journal   = {arXiv preprint arXiv:2206.01861},
  year      = {2022},
  volume    = {abs/2206.01861},
  url       = {https://api.semanticscholar.org/CorpusID:249395624}
}

@article{dettmers2022llmint8,
  title     = {{LLM.int8()}: 8-bit Matrix Multiplication for Transformers at Scale},
  author    = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  journal   = {arXiv preprint arXiv:2208.07339},
  year      = {2022},
  volume    = {abs/2208.07339},
  url       = {https://api.semanticscholar.org/CorpusID:251564521}
}

@article{xiao2022smoothquant,
  title     = {{SmoothQuant}: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author    = {Guangxuan Xiao and Ji Lin and Mickael Seznec and Julien Demouth and Song Han},
  journal   = {arXiv preprint arXiv:2211.10438},
  year      = {2022},
  volume    = {abs/2211.10438},
  url       = {https://api.semanticscholar.org/CorpusID:253708271}
}

@article{chai2023int21,
  title     = {{INT2.1}: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation},
  author    = {Yuji Chai and John Gkountouras and Glenn G. Ko and David M. Brooks and Gu-Yeon Wei},
  journal   = {arXiv preprint arXiv:2306.08162},
  year      = {2023},
  volume    = {abs/2506.08162},
  url       = {https://api.semanticscholar.org/CorpusID:259165162}
}

@article{liu2025compression,
  title     = {A survey of model compression techniques: past, present, and future},
  author    = {Defu Liu and Yixiao Zhu and Zhe Liu and Yi Liu and Changlin Han and Jinkai Tian and Ruihao Li and Wei Yi},
  journal   = {Frontiers in Robotics and AI},
  year      = {2025},
  volume    = {12},
  url       = {https://api.semanticscholar.org/CorpusID:277197048}
}

@inproceedings{yao2020hawq,
  title     = {{HAWQV3}: Dyadic Neural Network Quantization},
  author    = {Zhewei Yao and Zhen Dong and Zhangcheng Zheng and Amir Gholami and Jiali Yu and Eric Tan and Leyuan Wang and Qijing Huang and Yida Wang and Michael W. Mahoney and Kurt Keutzer},
  booktitle = {International Conference on Machine Learning},
  year      = {2020},
  url       = {https://api.semanticscholar.org/CorpusID:227127479}
}

@article{park2024quant,
  title     = {Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control},
  author    = {Seongmin Park and Hyung-Se Kim and Wonseok Jeon and Juyoung Yang and Byeongwook Jeon and Yoonseon Oh and Jungwook Choi},
  journal   = {arXiv preprint arXiv:2412.01034},
  year      = {2024},
  volume    = {abs/2412.01034},
  url       = {https://api.semanticscholar.org/CorpusID:274437507}
}

@article{wang2025potpqt,
  title     = {{PoTPTQ}: A Two-step Power-of-Two Post-training for {LLMs}},
  author    = {Xinyu Wang and Vahid Partovi Nia and Peng Lu and Jerry Huang and Xiao-Wen Chang and Boxing Chen and Yufei Cui},
  journal   = {arXiv preprint arXiv:2507.11959},
  year      = {2025},
  volume    = {abs/2507.11959},
  url       = {https://api.semanticscholar.org/CorpusID:280266474}
}

@article{huang2022,
  title     = {Weight-Quantized {SqueezeNet} for Resource-Constrained Robot Vacuums for Indoor Obstacle Classification},
  author    = {Qixing Huang},
  journal   = {AI},
  year      = {2022},
  url       = {https://api.semanticscholar.org/CorpusID:247369531}
}

@article{kim2021,
  title     = {Performance Evaluation of {INT8} Quantized Inference on Mobile {GPUs}},
  author    = {Sumin Kim and Gu Yong Park and Youngmin Yi},
  journal   = {IEEE Access},
  year      = {2021},
  volume    = {PP},
  pages     = {1--1},
  url       = {https://api.semanticscholar.org/CorpusID:244948019}
}

@inproceedings{li2025quantization,
  title     = {Quantization Meets Reasoning: Exploring and Mitigating Degradation of Low-Bit {LLMs} in Mathematical Reasoning},
  author    = {Zhen Li and Yupeng Su and Songmiao Wang and Runming Yang and Congkai Xie and Aofan Liu and Ming Li and Jiannong Cao and Ngai Wong and Hongxia Yang},
  year      = {2025},
  url       = {https://api.semanticscholar.org/CorpusID:278741093}
}

@article{rey2025,
  title     = {Aggregating empirical evidence from data strategy studies: a case on model quantization},
  author    = {Santiago del Rey and Paulo S{\'e}rgio Medeiros Santos and Guilherme Horta Travassos and Xavier Franch and Silverio Mart{\'i}nez-Fern{\'a}ndez},
  journal   = {arXiv preprint arXiv:2505.00816},
  year      = {2025},
  volume    = {abs/2505.00816},
  url       = {https://api.semanticscholar.org/CorpusID:278311047}
}

@article{dantas2024,
  title     = {A comprehensive review of model compression techniques in machine learning},
  author    = {Pierre V. Dantas and Waldir Sabino da Silva and Lucas C. Cordeiro and Celso Barbosa Carvalho},
  journal   = {Applied Intelligence},
  year      = {2024},
  volume    = {54},
  pages     = {11804--11844},
  url       = {https://api.semanticscholar.org/CorpusID:272362488}
}

@inproceedings{dettmers2022case,
  title     = {The case for 4-bit precision: k-bit Inference Scaling Laws},
  author    = {Tim Dettmers and Luke Zettlemoyer},
  booktitle = {International Conference on Machine Learning},
  year      = {2022},
  url       = {https://api.semanticscholar.org/CorpusID:254853733}
}

@article{abushahla2025,
  title     = {Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications},
  author    = {Hamza A. Abushahla and Dara Varam and Ariel J. N. Panopio and Mohamed I. AlHajri},
  journal   = {arXiv preprint arXiv:2508.15008},
  year      = {2025},
  volume    = {abs/2508.15008},
  url       = {https://api.semanticscholar.org/CorpusID:280700073}
}

% ============================================================================
% CATASTROPHIC FORGETTING & CONTINUAL LEARNING
% ============================================================================

@article{catastrophic_forgetting_llm,
  title={Fine-Tuning LLMs: Overcoming Catastrophic Forgetting},
  author={{Legion Intel}},
  year={2024},
  url={https://www.legionintel.com/blog/navigating-the-challenges-of-fine-tuning-and-catastrophic-forgetting}
}

@article{scaling_laws_forgetting,
  title={Scaling Laws for Forgetting When Fine-Tuning Large Language Models},
  author={Kalajdzievski, Damjan and others},
  journal={arXiv preprint arXiv:2401.05605},
  year={2024},
  month=jan,
  url={https://arxiv.org/html/2401.05605v1}
}

@article{continual_fine_tuning,
  title={Continued Fine-tuning of LLMs: A Technical Deep Dive},
  author={{Together AI}},
  year={2024},
  url={https://www.together.ai/blog/continued-fine-tuning}
}

@article{understanding_catastrophic_forgetting,
  title={Understanding Catastrophic Forgetting in Language Models via Implicit Inference},
  author={Anonymous},
  journal={International Conference on Learning Representations (ICLR)},
  year={2024},
  url={https://openreview.net/forum?id=VrHiF2hsrm}
}

@article{fine_tune_instruct_model,
  title={Fine-tune an Instruct model over raw text data},
  author={Flynn, Jon},
  journal={Towards Data Science},
  year={2025},
  month=jan,
  url={https://towardsdatascience.com/fine-tune-an-instruct-model-over-raw-text-data-6db654e7e2ed}
}

@article{forgetting_analysis,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2025},
  month=jan,
  url={https://arxiv.org/abs/2308.08747}
}

@article{base_vs_instruct_finetuning,
  title={A Comparative Analysis of Instruction Fine-Tuning Large Language Models for Financial Text Classification},
  author={Unknown},
  journal={ACM Transactions on Management Information Systems},
  year={2024},
  url={https://dl.acm.org/doi/10.1145/3706119}
}

@article{deployment_optimization,
  title={Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting},
  author={Unknown},
  journal={Proceedings of EMNLP},
  year={2020},
  url={https://aclanthology.org/2020.emnlp-main.634.pdf}
}

% ============================================================================
% CROSS-LINGUAL TRANSFER LEARNING
% ============================================================================

@article{cross_lingual_pretraining,
  title={Cross-lingual Language Model Pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019},
  url={https://arxiv.org/abs/1901.07291}
}

@article{cross_lingual_transfer_learning,
  title={Cross-lingual Transfer Learning for Pre-trained Contextualized Language Models},
  author={Unknown},
  journal={OpenReview},
  year={2020},
  month=oct,
  url={https://openreview.net/forum?id=1WF-fPvY_jQ}
}

@inproceedings{zero_shot_cross_lingual,
  title={Cross-lingual language model pretraining},
  author={Conneau, Alexis and others},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  year={2019},
  url={https://dl.acm.org/doi/10.5555/3454287.3454921}
}

@article{language_similarity_transfer,
  title={Zero-shot cross-lingual transfer language selection using linguistic similarity},
  author={Eronen, Jetro and others},
  journal={Expert Systems with Applications},
  year={2023},
  month=jan,
  url={https://www.sciencedirect.com/science/article/abs/pii/S030645732200351X}
}

@misc{clp_transfer,
  title={Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning},
  author={Ostendorff, Malte and Rehm, Georg},
  year={2023},
  howpublished={GitHub Repository},
  url={https://github.com/malteos/clp-transfer},
  note={doi: 10.48550/ARXIV.2301.09626}
}

@misc{xlm_paper,
  title={PyTorch original implementation of Cross-lingual Language Model Pretraining},
  author={{Facebook Research}},
  year={2019},
  howpublished={GitHub Repository},
  url={https://github.com/facebookresearch/XLM}
}

@inproceedings{xlm_paper_neurips,
  title={Cross-lingual Language Model Pretraining},
  author={Conneau, Alexis and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={--},
  year={2019},
  url={https://proceedings.neurips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf}
}

@misc{cross_lingual_papers,
  title={Cross-Lingual Transfer},
  author={{Papers with Code}},
  year={2024},
  howpublished={Website},
  url={https://paperswithcode.com/task/cross-lingual-transfer}
}

@article{cross_lingual_subnetworks,
  title={Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing},
  author={Unknown},
  journal={Computational Linguistics},
  volume={49},
  number={3},
  pages={613--666},
  year={2023},
  month=sep,
  publisher={MIT Press},
  url={https://direct.mit.edu/coli/article/49/3/613/116157/Cross-Lingual-Transfer-with-Language-Specific}
}

@article{cross_lingual_post_training,
  title={An analysis on language transfer of pre-trained language model with cross-lingual post-training},
  author={Unknown},
  journal={ScienceDirect},
  year={2024},
  month=dec,
  url={https://www.sciencedirect.com/science/article/abs/pii/S0957417424027088}
}

% ============================================================================
% FINE-TUNING METHODOLOGIES
% ============================================================================

@article{fine_tuning_small_datasets,
  title={Fine-Tuning LLMs on Small Datasets: Methods and Techniques},
  author={{Sapien AI}},
  year={2025},
  month=jul,
  url={https://www.sapien.io/blog/strategies-for-fine-tuning-llms-on-small-datasets}
}

@article{transfer_learning_vs_fine_tuning,
  title={Transfer Learning vs Fine Tuning: When to Use Each},
  author={{Label Your Data}},
  year={2025},
  month=oct,
  url={https://labelyourdata.com/articles/llm-fine-tuning/transfer-learning-vs-fine-tuning}
}

@article{domain_adaptation_llm,
  title={Fine-Tuning LLMs: A Guide With Examples},
  author={{DataCamp}},
  year={2024},
  month=dec,
  url={https://www.datacamp.com/tutorial/fine-tuning-large-language-models}
}

@article{few_shot_learning_llm,
  title={LLM Fine-Tuning Guide for Enterprises},
  author={{AIM Multiple}},
  year={2024},
  url={https://research.aimultiple.com/llm-fine-tuning/}
}

@article{efficient_llm_training,
  title={The Ultimate Guide to LLM Fine Tuning: Best Practices \& Tools},
  author={{Lakera}},
  year={2024},
  url={https://www.lakera.ai/blog/llm-fine-tuning-guide}
}

@article{llm_deployment,
  title={LLM Fine Tuning on Limited Datasets: Effective Tips and Strategies},
  author={{Techginity}},
  year={2024},
  month=aug,
  url={https://www.techginity.com/blog/tips-and-strategies-for-fine-tuning-llms-with-limited-datasets}
}

@article{fine_tuning_guide_llm,
  title={LLM Fine Tuning: The 2025 Guide for ML Teams},
  author={{Label Your Data}},
  year={2024},
  month=sep,
  url={https://labelyourdata.com/articles/llm-fine-tuning}
}

@article{quantization_regularization,
  title={Fine-tune a SmolLM on domain-specific synthetic data from a LLM},
  author={Berenstein, David},
  journal={Hugging Face Blog},
  year={2024},
  url={https://huggingface.co/blog/davidberenstein1957/fine-tune-a-smollm-on-synthetic-data-of-llm}
}

@article{peft_quantization,
  title={The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs},
  author={Unknown},
  journal={arXiv preprint arXiv:2408.13296},
  year={2025},
  month=sep,
  url={https://arxiv.org/html/2408.13296v1}
}

@article{quantization_deployment,
  title={Fine-Tuning LLMs with Small Data: Guide},
  author={{Dialzara}},
  year={2025},
  month=mar,
  url={https://dialzara.com/blog/fine-tuning-llms-with-small-data-guide}
}

@article{model_compression_survey,
  title={Fine-tuning LLMs},
  author={{TeeTracker}},
  journal={Medium},
  year={2023},
  month=dec,
  url={https://teetracker.medium.com/fine-tuning-llms-9fe553a514d0}
}

@article{quantization_practice,
  title={Weight-Only Quantization},
  author={{Intel}},
  year={2024},
  organization={Intel Extension for PyTorch},
  url={https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/llm/int4_weight_only_quantization.html}
}

% ============================================================================
% QUANTIZATION DETAILED STUDIES
% ============================================================================

@article{quantization_guide,
  title={A Guide to Quantization in LLMs},
  author={{Symbl AI}},
  year={2024},
  month=feb,
  url={https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/}
}

@article{understanding_int4,
  title={Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases},
  author={Wu, Xiaoxia and others},
  journal={arXiv preprint arXiv:2301.12017},
  year={2023},
  month=jan,
  url={https://arxiv.org/abs/2301.12017}
}

@article{quantization_benchmarks,
  title={Benchmarking Quantized LLMs: What Works Best for Real Tasks?},
  author={{Ionio AI}},
  year={2024},
  url={https://www.ionio.ai/blog/llm-quantize-analysis}
}

@article{mistral_quantization,
  title={Optimizing LLM inference: A practical guide with vLLM, Quantization and Google GPUs},
  author={Kwak, Injae},
  journal={Medium},
  year={2025},
  month=jan,
  url={https://medium.com/@injae.kwak/part-1-optimizing-llm-inference-a-practical-guide-with-vllm-quantization-and-google-cloud-gpus-tpus-30521202d15b}
}

@article{int8_performance,
  title={Quantization and performance optimization},
  author={{Meta AI}},
  year={2024},
  url={https://www.llama.com/docs/how-to-guides/quantization/}
}

@article{quantization_optimization,
  title={Doing more with less: LLM quantization (part 2)},
  author={{Red Hat}},
  year={2025},
  month=may,
  url={https://www.redhat.com/en/blog/doing-more-less-llm-quantization-part-2}
}

@article{int4_quantization_study,
  title={Understanding INT4 Quantization for Transformer Models},
  author={Wu, Xiaoxia and others},
  journal={ResearchGate},
  year={2023},
  month=jan,
  url={https://www.researchgate.net/publication/367557918_Understanding_INT4_Quantization_for_Transformer_Models_Latency_Speedup_Composability_and_Failure_Cases}
}

@article{weight_only_quantization,
  title={Weight-Only Quantization (Prototype)},
  author={{Intel}},
  year={2024},
  organization={Intel Extension for PyTorch},
  url={https://intel.github.io/intel-extension-for-pytorch/xpu/2.3.110+xpu/tutorials/llm/int4_weight_only_quantization.html}
}

@article{int4_speedup,
  title={Understanding INT4 Quantization for Language Models},
  author={Wu, Xiaoxia and others},
  journal={arXiv preprint arXiv:2301.12017},
  year={2023},
  url={https://arxiv.org/pdf/2301.12017}
}

@article{quantization_tradeoffs,
  title={Benchmarking Quantized LLMs: What Works Best for Real Tasks},
  author={{Ionio AI}},
  year={2024},
  url={https://www.ionio.ai/blog/llm-quantize-analysis}
}

% ============================================================================
% EVALUATION METRICS
% ============================================================================

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  note={Verfügbar unter: https://aclanthology.org/P02-1040/}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004},
  note={Verfügbar unter: https://aclanthology.org/W04-1013/}
}

@article{zhang2019bertscore,
  title={BERTScore: Evaluating text generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={International Conference on Learning Representations},
  year={2020},
  note={Verfügbar unter: https://arxiv.org/abs/1904.09675}
}

% ============================================================================
% DOCUMENT ANALYSIS & TECHNICAL COMMUNICATION
% ============================================================================

@book{rosenfeld2015information,
  title={Information Architecture: For the Web and Beyond},
  author={Rosenfeld, Louis and Morville, Peter and Arango, Jorge},
  edition={4},
  year={2015},
  publisher={O'Reilly Media},
  address={Sebastopol, CA},
  isbn={978-1491911686},
  note={Standardwerk zur Informationsarchitektur und Strukturierung komplexer Informationssysteme}
}

@article{redish2000readability,
  title={Readability formulas have even more limitations than Klare discusses},
  author={Redish, Janice C},
  journal={ACM Journal of Computer Documentation},
  volume={24},
  number={3},
  pages={132--137},
  year={2000},
  publisher={ACM New York, NY, USA},
  note={Verfügbar unter: https://dl.acm.org/doi/10.1145/344599.344637}
}

@book{rude2015technical,
  title={Technical Editing},
  author={Rude, Carolyn D and Eaton, Angela},
  edition={6},
  year={2015},
  publisher={Routledge},
  address={New York, NY},
  isbn={978-0415507189},
  note={Umfassendes Werk zu technischer Dokumentation und Kommunikation in Organisationen}
}

@book{schriver1997dynamics,
  title={Dynamics in Document Design: Creating Text for Readers},
  author={Schriver, Karen A},
  year={1997},
  publisher={John Wiley \& Sons},
  address={New York, NY},
  isbn={978-0471306368},
  note={Grundlegendes Werk zu Dokumentdesign und nutzerorientierter Textgestaltung}
}

@inproceedings{petersen2014audience,
  title={Audience analysis in technical writing},
  author={Petersen, Emily January},
  booktitle={Proceedings of the 32nd ACM International Conference on Design of Communication},
  pages={1--2},
  year={2014},
  note={Verfügbar unter: https://dl.acm.org/doi/10.1145/2666216.2666239}
}

@article{mathes1985arriving,
  title={Arriving at a Definition of Technical Communication},
  author={Mathes, J C and Stevenson, Dwight W},
  journal={Journal of Technical Writing and Communication},
  volume={15},
  number={4},
  pages={323--334},
  year={1985},
  publisher={SAGE Publications},
  note={Klassische Arbeit zur Definition und Zielgruppenanalyse in der technischen Kommunikation}
}

@book{mcenery2011corpus,
  title={Corpus Linguistics: Method, Theory and Practice},
  author={McEnery, Tony and Hardie, Andrew},
  year={2011},
  publisher={Cambridge University Press},
  address={Cambridge, UK},
  isbn={978-0521547628},
  note={Standardwerk zur Korpuslinguistik und systematischen Textanalyse}
}

@article{bowker2012corpus,
  title={Corpus linguistics and technical communication},
  author={Bowker, Lynne},
  journal={The Oxford Handbook of Technical Communication},
  pages={354--369},
  year={2012},
  publisher={Oxford University Press},
  note={Anwendung korpuslinguistischer Methoden auf technische Kommunikation}
}

@book{kimble2006plain,
  title={Plain Language: A Charter for Clear Writing},
  author={Kimble, Joseph},
  year={2006},
  publisher={Carolina Academic Press},
  address={Durham, NC},
  isbn={978-1594604294},
  note={Umfassendes Werk zu Plain Language Prinzipien}
}

@article{schriver2012plain,
  title={What we know about expertise in professional communication},
  author={Schriver, Karen A},
  journal={Past, Present, and Future Contributions of Cognitive Writing Research to Cognitive Psychology},
  pages={275--312},
  year={2012},
  publisher={Psychology Press},
  note={Expertise in professioneller Kommunikation und Verständlichkeit}
}

@misc{plainlanguage2011federal,
  title={Federal Plain Language Guidelines},
  author={{Plain Language Action and Information Network}},
  year={2011},
  howpublished={U.S. General Services Administration},
  note={Verfügbar unter: https://www.plainlanguage.gov/guidelines/}
}

@book{accessible2008wcag,
  title={Web Content Accessibility Guidelines (WCAG) 2.0},
  author={{W3C}},
  year={2008},
  publisher={World Wide Web Consortium},
  note={Verfügbar unter: https://www.w3.org/TR/WCAG20/}
}

@article{petrie2007accessibility,
  title={Accessibility, usability and user experience: towards an integrated approach},
  author={Petrie, Helen and Bevan, Nigel},
  journal={The Universal Access Handbook},
  pages={1--16},
  year={2007},
  publisher={CRC Press},
  note={Integration von Barrierefreiheit und Nutzerfreundlichkeit}
}

@article{henning2003writing,
  title={Writing for Multiple Media: Writing Style in Technical Communication},
  author={Henning, Teresa and Bemer, Amanda},
  journal={Technical Communication},
  volume={50},
  number={1},
  pages={28--39},
  year={2003},
  publisher={Society for Technical Communication},
  note={Entwicklung einheitlicher Schreibstile über verschiedene Medien}
}

@book{weiss2005handbook,
  title={The Handbook of Technical Writing},
  author={Weiss, Edmond H},
  edition={9},
  year={2005},
  publisher={St. Martin's Press},
  address={New York, NY},
  isbn={978-0312474782},
  note={Standardisierung und Best Practices in der technischen Dokumentation}
}

@article{hackos2002content,
  title={Content Management for Dynamic Web Delivery},
  author={Hackos, JoAnn T},
  year={2002},
  publisher={John Wiley \& Sons},
  address={New York, NY},
  isbn={978-0471085850},
  note={Qualitätssicherung und Prozessoptimierung in der Content-Erstellung}
}

@inproceedings{baker2013every,
  title={Every Page is Page One: Topic-based Writing for Technical Communication and the Web},
  author={Baker, Mark},
  year={2013},
  publisher={XML Press},
  address={Laguna Hills, CA},
  isbn={978-1937434281},
  note={Strukturierung und Konsistenz in modularer technischer Dokumentation}
}

@article{sweller1988cognitive,
  title={Cognitive load during problem solving: Effects on learning},
  author={Sweller, John},
  journal={Cognitive Science},
  volume={12},
  number={2},
  pages={257--285},
  year={1988},
  publisher={Wiley Online Library},
}

@article{chandler1991cognitive,
  title={Cognitive load theory and the format of instruction},
  author={Chandler, Paul and Sweller, John},
  journal={Cognition and Instruction},
  volume={8},
  number={4},
  pages={293--332},
  year={1991},
  publisher={Taylor \& Francis},
  note={Anwendung der Cognitive Load Theory auf Instruktionsdesign}
}

@book{rosenfeld2015information,
  title={Information Architecture: For the Web and Beyond},
  author={Rosenfeld, Louis and Morville, Peter and Arango, Jorge},
  edition={4},
  year={2015},
  publisher={O'Reilly Media},
  address={Sebastopol, CA},
  isbn={978-1491911686},
  note={Standardwerk zur Informationsarchitektur und Strukturierung komplexer Informationssysteme}
}

@article{redish2000readability,
  title={Readability formulas have even more limitations than Klare discusses},
  author={Redish, Janice C},
  journal={ACM Journal of Computer Documentation},
  volume={24},
  number={3},
  pages={132--137},
  year={2000},
  publisher={ACM New York, NY, USA},
  note={Verfügbar unter: https://dl.acm.org/doi/10.1145/344599.344637}
}

% Für Organizational Communication und Document Design
% -----------------------------------------------------

@book{rude2015technical,
  title={Technical Editing},
  author={Rude, Carolyn D and Eaton, Angela},
  edition={6},
  year={2015},
  publisher={Routledge},
  address={New York, NY},
  isbn={978-0415507189},
  note={Umfassendes Werk zu technischer Dokumentation und Kommunikation in Organisationen}
}

@book{schriver1997dynamics,
  title={Dynamics in Document Design: Creating Text for Readers},
  author={Schriver, Karen A},
  year={1997},
  publisher={John Wiley \& Sons},
  address={New York, NY},
  isbn={978-0471306368},
  note={Grundlegendes Werk zu Dokumentdesign und nutzerorientierter Textgestaltung}
}

% Für Audience Analysis und Zielgruppenorientierung
% --------------------------------------------------

@inproceedings{petersen2014audience,
  title={Audience analysis in technical writing},
  author={Petersen, Emily January},
  booktitle={Proceedings of the 32nd ACM International Conference on Design of Communication},
  pages={1--2},
  year={2014},
  note={Verfügbar unter: https://dl.acm.org/doi/10.1145/2666216.2666239}
}

@article{mathes1985arriving,
  title={Arriving at a Definition of Technical Communication},
  author={Mathes, J C and Stevenson, Dwight W},
  journal={Journal of Technical Writing and Communication},
  volume={15},
  number={4},
  pages={323--334},
  year={1985},
  publisher={SAGE Publications},
  note={Klassische Arbeit zur Definition und Zielgruppenanalyse in der technischen Kommunikation}
}

% Für Korpusanalyse und vergleichende Dokumentenanalyse
% ------------------------------------------------------

@book{mcenery2011corpus,
  title={Corpus Linguistics: Method, Theory and Practice},
  author={McEnery, Tony and Hardie, Andrew},
  year={2011},
  publisher={Cambridge University Press},
  address={Cambridge, UK},
  isbn={978-0521547628},
  note={Standardwerk zur Korpuslinguistik und systematischen Textanalyse}
}

@article{bowker2012corpus,
  title={Corpus linguistics and technical communication},
  author={Bowker, Lynne},
  journal={The Oxford Handbook of Technical Communication},
  pages={354--369},
  year={2012},
  publisher={Oxford University Press},
  note={Anwendung korpuslinguistischer Methoden auf technische Kommunikation}
}

% Für Plain Language und verständliche Kommunikation
% ---------------------------------------------------

@book{kimble2006plain,
  title={Plain Language: A Charter for Clear Writing},
  author={Kimble, Joseph},
  year={2006},
  publisher={Carolina Academic Press},
  address={Durham, NC},
  isbn={978-1594604294},
  note={Umfassendes Werk zu Plain Language Prinzipien}
}

@article{schriver2012plain,
  title={What we know about expertise in professional communication},
  author={Schriver, Karen A},
  journal={Past, Present, and Future Contributions of Cognitive Writing Research to Cognitive Psychology},
  pages={275--312},
  year={2012},
  publisher={Psychology Press},
  note={Expertise in professioneller Kommunikation und Verständlichkeit}
}

@misc{plainlanguage2011federal,
  title={Federal Plain Language Guidelines},
  author={{Plain Language Action and Information Network}},
  year={2011},
  howpublished={U.S. General Services Administration},
  note={Verfügbar unter: https://www.plainlanguage.gov/guidelines/}
}

% Für barrierefreie und inklusive Kommunikation
% ----------------------------------------------

@book{accessible2008wcag,
  title={Web Content Accessibility Guidelines (WCAG) 2.0},
  author={{W3C}},
  year={2008},
  publisher={World Wide Web Consortium},
  note={Verfügbar unter: https://www.w3.org/TR/WCAG20/}
}

@article{petrie2007accessibility,
  title={Accessibility, usability and user experience: towards an integrated approach},
  author={Petrie, Helen and Bevan, Nigel},
  journal={The Universal Access Handbook},
  pages={1--16},
  year={2007},
  publisher={CRC Press},
  note={Integration von Barrierefreiheit und Nutzerfreundlichkeit}
}

% Für Standardisierung in der technischen Kommunikation
% ------------------------------------------------------

@article{henning2003writing,
  title={Writing for Multiple Media: Writing Style in Technical Communication},
  author={Henning, Teresa and Bemer, Amanda},
  journal={Technical Communication},
  volume={50},
  number={1},
  pages={28--39},
  year={2003},
  publisher={Society for Technical Communication},
  note={Entwicklung einheitlicher Schreibstile über verschiedene Medien}
}

@book{weiss2005handbook,
  title={The Handbook of Technical Writing},
  author={Weiss, Edmond H},
  edition={9},
  year={2005},
  publisher={St. Martin's Press},
  address={New York, NY},
  isbn={978-0312474782},
  note={Standardisierung und Best Practices in der technischen Dokumentation}
}

% Für Qualitätssicherung in organisationsübergreifenden Kontexten
% ----------------------------------------------------------------

@article{hackos2002content,
  title={Content Management for Dynamic Web Delivery},
  author={Hackos, JoAnn T},
  year={2002},
  publisher={John Wiley \& Sons},
  address={New York, NY},
  isbn={978-0471085850},
  note={Qualitätssicherung und Prozessoptimierung in der Content-Erstellung}
}

@inproceedings{baker2013every,
  title={Every Page is Page One: Topic-based Writing for Technical Communication and the Web},
  author={Baker, Mark},
  year={2013},
  publisher={XML Press},
  address={Laguna Hills, CA},
  isbn={978-1937434281},
  note={Strukturierung und Konsistenz in modularer technischer Dokumentation}
}

% Für kognitive Belastung und Informationsverarbeitung
% -----------------------------------------------------

@article{sweller1988cognitive,
  title={Cognitive load during problem solving: Effects on learning},
  author={Sweller, John},
  journal={Cognitive Science},
  volume={12},
  number={2},
  pages={257--285},
  year={1988},
  publisher={Wiley Online Library},
}

@article{chandler1991cognitive,
  title={Cognitive load theory and the format of instruction},
  author={Chandler, Paul and Sweller, John},
  journal={Cognition and Instruction},
  volume={8},
  number={4},
  pages={293--332},
  year={1991},
  publisher={Taylor \& Francis},
  note={Anwendung der Cognitive Load Theory auf Instruktionsdesign}
}

@book{rosenfeld2015information,
  title={Information Architecture: For the Web and Beyond},
  author={Rosenfeld, Louis and Morville, Peter and Arango, Jorge},
  year={2015},
  edition={4},
  publisher={O'Reilly Media}
}

@article{redish2000readability,
  title={Readability formulas have even more limitations than Klare discusses},
  author={Redish, Janice C},
  journal={ACM Journal of Computer Documentation},
  volume={24},
  number={3},
  pages={132--137},
  year={2000},
  publisher={ACM}
}

@book{schriver1997dynamics,
  title={Dynamics in Document Design},
  author={Schriver, Karen A},
  year={1997},
  publisher={Wiley}
}

@book{rude2015technical,
  title={Technical Editing},
  author={Rude, Carolyn D and Eaton, Angela},
  year={2015},
  edition={5},
  publisher={Routledge}
}

@book{baker2013every,
  title={Every Page is Page One: Topic-based Writing for Technical Communication and the Web},
  author={Baker, Mark},
  year={2013},
  publisher={XML Press}
}

@article{horn1998visual,
  title={Visual language and converging technologies in the next 10-15 years (and beyond)},
  author={Horn, Robert E},
  journal={National Science Foundation Conference on Converging Technologies},
  year={2001},
  pages={141--160}
}

@book{henning2003writing,
  title={Writing in Organizations: Purposes, Strategies, and Processes},
  author={Henning, Teresa and Bemer, Amanda},
  year={2003},
  publisher={Longman}
}

@book{hackos2002content,
  title={Content Management for Dynamic Web Delivery},
  author={Hackos, JoAnn T},
  year={2002},
  publisher={Wiley}
}

@article{petersen2014audience,
  title={Audience analysis and adaptation},
  author={Petersen, Emily January},
  journal={Connexions},
  year={2014}
}

@article{mathes1985arriving,
  title={Arriving at a definition of technical writing},
  author={Mathes, John C and Stevenson, Dwight W},
  journal={The Technical Writing Process},
  pages={4--10},
  year={1985},
  publisher={Macmillan}
}

@book{weiss2005handbook,
  title={The Handbook of Technical Writing},
  author={Alred, Gerald J and Brusaw, Charles T and Oliu, Walter E},
  year={2005},
  edition={8},
  publisher={St. Martin's Press}
}

@book{mcenery2011corpus,
  title={Corpus Linguistics: Method, Theory and Practice},
  author={McEnery, Tony and Hardie, Andrew},
  year={2011},
  publisher={Cambridge University Press}
}

@book{bowker2012corpus,
  title={Working with Specialized Language: A Practical Guide to Using Corpora},
  author={Bowker, Lynne and Pearson, Jennifer},
  year={2002},
  publisher={Routledge}
}

@article{kimble2006plain,
  title={Plain English: A Charter for Clear Writing},
  author={Kimble, Joseph},
  journal={Thomas M. Cooley Journal of Practical and Clinical Law},
  volume={9},
  pages={1--55},
  year={2006}
}

@article{schriver2012plain,
  title={Plain language in the US gains momentum: 1940--2015},
  author={Schriver, Karen A},
  journal={IEEE Transactions on Professional Communication},
  volume={60},
  number={4},
  pages={343--383},
  year={2017}
}

@manual{plainlanguage2011federal,
  title={Federal Plain Language Guidelines},
  author={{Plain Language Action and Information Network}},
  year={2011},
  organization={U.S. General Services Administration}
}

@manual{accessible2008wcag,
  title={Web Content Accessibility Guidelines (WCAG) 2.0},
  author={{W3C}},
  year={2008},
  organization={World Wide Web Consortium}
}

@article{petrie2007accessibility,
  title={The accessibility of web pages for people with disabilities},
  author={Petrie, Helen and Hamilton, Fraser and King, Neil and Pavan, Patrick},
  journal={Proceedings of Human Computer Interaction},
  year={2007}
}

@article{sweller1988cognitive,
  title={Cognitive load during problem solving: Effects on learning},
  author={Sweller, John},
  journal={Cognitive Science},
  volume={12},
  number={2},
  pages={257--285},
  year={1988}
}

@article{chandler1991cognitive,
  title={Cognitive load theory and the format of instruction},
  author={Chandler, Paul and Sweller, John},
  journal={Cognition and Instruction},
  volume={8},
  number={4},
  pages={293--332},
  year={1991}
}

@article{navigli2018natural,
  title={Natural language understanding: Instructions for (present and future) use},
  author={Navigli, Roberto},
  journal={IJCAI},
  pages={5697--5702},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{zhang2023towards,
  title={Towards autonomous driving: a survey on large language models for transportation},
  author={Zhang, Xin and Chen, Yuming and Li, Wei},
  journal={arXiv preprint arXiv:2311.14357},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2019}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{huggingface2024leolm,
  title={LeoLM: Linguistically Enhanced Open Language Model},
  author={HuggingFace},
  howpublished={\url{https://huggingface.co/LeoLM}},
  year={2024}
}

@article{sennrich2016neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2016}
}

@article{ruder2019transfer,
  title={Transfer learning in natural language processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  journal={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@article{howard2018universal,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@article{zhang2020deep,
  title={Deep learning applications in traffic flow prediction: A review},
  author={Zhang, Di and Kabuka, Mansur R},
  journal={Transportation Research Part C: Emerging Technologies},
  volume={120},
  pages={102843},
  year={2020}
}

@article{silva2021natural,
  title={Natural language processing applications in transportation systems: A systematic literature review},
  author={Silva, Carlos and Ribeiro, Bernardete},
  journal={IEEE Access},
  volume={9},
  pages={132411--132430},
  year={2021}
}

@article{gkatzia2016natural,
  title={Natural language generation for the generation of textual summaries},
  author={Gkatzia, Dimitra},
  journal={Natural Language Engineering},
  volume={22},
  number={4},
  pages={583--603},
  year={2016}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020}
}

@article{gururangan2020dont,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

@article{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={International conference on machine learning},
  pages={2790--2799},
  year={2019}
}

@article{he2022towards,
  title={Towards a unified view of parameter-efficient transfer learning},
  author={He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
  journal={arXiv preprint arXiv:2110.04366},
  year={2022}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023}
}

@article{hu2021lora,
  title={LoRA: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhang2023adaptive,
  title={Adaptive budget allocation for parameter-efficient fine-tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  journal={arXiv preprint arXiv:2303.10512},
  year={2023}
}

@article{dettmers2023qlora,
  title={QLoRA: Efficient finetuning of quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@misc{unsloth2024github,
  title={Unsloth: Efficient fine-tuning of large language models},
  author={Unsloth AI},
  howpublished={\url{https://github.com/unslothai/unsloth}},
  year={2024}
}

@article{dao2022flashattention,
  title={FlashAttention: Fast and memory-efficient exact attention with IO-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  journal={Extended abstracts of the 2021 CHI conference on human factors in computing systems},
  pages={1--7},
  year={2021}
}

@article{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  journal={International conference on machine learning},
  pages={12697--12706},
  year={2021}
}

@article{wei2022finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2022}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{sanh2022multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Raja, Arun and Dey, Manan and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2022}
}

@article{wei2019eda,
  title={EDA: Easy data augmentation techniques for boosting performance on text classification tasks},
  author={Wei, Jason and Zou, Kai},
  journal={arXiv preprint arXiv:1901.11196},
  year={2019}
}

@article{sennrich2016improving,
  title={Improving neural machine translation models with monolingual data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1511.06709},
  year={2016}
}

@article{dai2023chataug,
  title={ChatAug: Leveraging ChatGPT for text data augmentation},
  author={Dai, Haixing and Liu, Zhengliang and Liao, Wenxiong and Huang, Xiaoke and Cao, Yihan and Wu, Zihao and Zhao, Lin and Xu, Shaochen and Liu, Wei and Liu, Ninghao and others},
  journal={arXiv preprint arXiv:2302.13007},
  year={2023}
}

@article{chawla2002smote,
  title={SMOTE: synthetic minority over-sampling technique},
  author={Chawla, Nitesh V and Bowyer, Kevin W and Hall, Lawrence O and Kegelmeyer, W Philip},
  journal={Journal of artificial intelligence research},
  volume={16},
  pages={321--357},
  year={2002}
}

@article{dodge2020fine,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}

@article{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  journal={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989}
}

@article{sambasivan2021everyone,
  title={"Everyone wants to do the model work, not the data work": Data cascades in high-stakes AI},
  author={Sambasivan, Nithya and Kapania, Shivani and Highfill, Hannah and Akrong, Diana and Paritosh, Praveen and Aroyo, Lora M},
  journal={Proceedings of the 2021 CHI conference on human factors in computing systems},
  pages={1--15},
  year={2021}
}

@article{mosbach2021stability,
  title={On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines},
  author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  journal={arXiv preprint arXiv:2006.04884},
  year={2021}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014}
}

@article{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  journal={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998}
}

@article{schwartz2020green,
  title={Green AI},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020}
}

@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  journal={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

@article{rajbhandari2020zero,
  title={ZeRO: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  journal={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020}
}

@article{li2023making,
  title={Making AI less "thirsty": Uncovering and addressing the secret water footprint of AI models},
  author={Li, Pengfei and Yang, Jianyi and Islam, Mohammad A and Ren, Shaolei},
  journal={arXiv preprint arXiv:2304.03271},
  year={2023}
}

@article{zhou2019edge,
  title={Edge intelligence: Paving the last mile of artificial intelligence with edge computing},
  author={Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
  journal={Proceedings of the IEEE},
  volume={107},
  number={8},
  pages={1738--1762},
  year={2019}
}

@article{gholami2022survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={Low-Power Computer Vision},
  pages={291--326},
  year={2022}
}

@article{dettmers2022llmint8,
  title={LLM. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@article{frantar2023gptq,
  title={GPTQ: Accurate post-training quantization for generative pre-trained transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{liu2023pretraining,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023}
}

@article{dhuliawala2023chain,
  title={Chain-of-verification reduces hallucination in large language models},
  author={Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
  journal={arXiv preprint arXiv:2309.11495},
  year={2023}
}

@article{maynez2020faithfulness,
  title={On faithfulness and factuality in abstractive summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  journal={arXiv preprint arXiv:2005.00661},
  year={2020}
}

@article{holtzman2020curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2020}
}

@article{celikyilmaz2021evaluation,
  title={Evaluation of text generation: A survey},
  author={Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2006.14799},
  year={2021}
}

@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{zhang2019bertscore,
  title={BERTScore: Evaluating text generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{wang2023selfconsistency,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2023}
}

@article{arora2023pal,
  title={PAL: Program-aided language models},
  author={Arora, Simran and Narayan, Avanika and Chen, Mayee F and Orr, Laurel and Guha, Neel and Bhatia, Kush and Chami, Ines and Re, Christopher},
  journal={International Conference on Machine Learning},
  pages={1176--1195},
  year={2023}
}

% Mistral 7B
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023},
  url={https://arxiv.org/abs/2310.06825}
}

% Cross-lingual transfer learning
@article{ostendorff2023clp,
  title={Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning},
  author={Ostendorff, Malte and Rehm, Georg},
  journal={arXiv preprint arXiv:2301.09626},
  year={2023},
  url={https://arxiv.org/abs/2301.09626}
}

@article{eronen2022cross,
  title={Zero-shot cross-lingual transfer language selection using linguistic similarity},
  author={Eronen, Juuso and Ptaszynski, Michal and Smywinski-Pohl, Aleksander and Lepp{\"a}nen, Leo and Masui, Fumito},
  journal={Expert Systems with Applications},
  volume={207},
  pages={118053},
  year={2022},
  publisher={Elsevier},
  doi={10.1016/j.eswa.2022.118053}
}

% LeoLM
@misc{laion2023leolm,
  title={LeoLM: Igniting German-Language LLM Research},
  author={Pl{\"u}ster, Bj{\"o}rn and Schuhmann, Christoph and {LAION} and {HessianAI}},
  year={2023},
  howpublished={\url{https://laion.ai/blog/leo-lm/}},
  note={Accessed: 2024}
}

% Catastrophic Forgetting
@article{luo2023catastrophic,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023},
  url={https://arxiv.org/abs/2308.08747}
}

@article{mckenzie2024forgetting,
  title={Scaling Laws for Forgetting When Fine-Tuning Large Language Models},
  author={McKenzie, Ian R. and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kolt, Aaron and Huang, Ziming and others},
  journal={arXiv preprint arXiv:2401.05605},
  year={2024},
  url={https://arxiv.org/abs/2401.05605}
}

@inproceedings{li2024revisiting,
  title={Revisiting Catastrophic Forgetting in Large Language Model Tuning},
  author={Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={4297--4308},
  year={2024},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2024.findings-emnlp.249/}
}

% Quantization
@article{wu2023int4,
  title={Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases},
  author={Wu, Xiaoxia and Yao, Zhewei and He, Yuxiong},
  journal={arXiv preprint arXiv:2301.12017},
  year={2023},
  url={https://arxiv.org/abs/2301.12017}
}

@article{dettmers2022int8,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html}
}

@misc{symbl2024quantization,
  title={A Guide to Quantization in LLMs},
  author={{Symbl.ai}},
  year={2024},
  howpublished={\url{https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/}},
  note={Accessed: 2024}
}

@misc{latitude2025quantization,
  title={How Quantization Reduces LLM Latency},
  author={{Latitude}},
  year={2025},
  howpublished={\url{https://latitude-blog.ghost.io/blog/how-quantization-reduces-llm-latency/}},
  note={Accessed: 2025}
}

@misc{nvidia2025quantization,
  title={Optimizing LLMs for Performance and Accuracy with Post-Training Quantization},
  author={{NVIDIA Technical Blog}},
  year={2025},
  howpublished={\url{https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/}},
  note={Accessed: 2025}
}

% Fine-tuning with small datasets
@misc{superannotate2025finetuning,
  title={Fine-tuning Large Language Models (LLMs) in 2025},
  author={{SuperAnnotate}},
  year={2025},
  howpublished={\url{https://www.superannotate.com/blog/llm-fine-tuning}},
  note={Accessed: 2025}
}

@misc{sapien2025smalldatasets,
  title={Fine-Tuning LLMs on Small Datasets: Methods and Techniques},
  author={{Sapien}},
  year={2025},
  howpublished={\url{https://www.sapien.io/blog/strategies-for-fine-tuning-llms-on-small-datasets}},
  note={Accessed: 2025}
}

@article{redhat2024supervised,
  title={Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs},
  author={Sudalairaj, Sanath Kumar and Akiki, Salma and Fu, Sophia and others},
  journal={arXiv preprint arXiv:2412.13337},
  year={2024},
  url={https://arxiv.org/abs/2412.13337}
}

@misc{datacamp2024finetuning,
  title={Fine-Tuning LLMs: A Guide With Examples},
  author={{DataCamp}},
  year={2024},
  howpublished={\url{https://www.datacamp.com/tutorial/fine-tuning-large-language-models}},
  note={Accessed: 2024}
}

@misc{dialzara2025smalldata,
  title={Fine-Tuning LLMs with Small Data: Guide},
  author={{Dialzara}},
  year={2025},
  howpublished={\url{https://dialzara.com/blog/fine-tuning-llms-with-small-data-guide}},
  note={Accessed: 2025}
}

@article{aimultiple2023finetuning,
  title={Fine-tuning and Utilization Methods of Domain-specific LLMs},
  author={{AIMultiple Research}},
  journal={arXiv preprint arXiv:2401.02981},
  year={2023},
  url={https://arxiv.org/abs/2401.02981}
}